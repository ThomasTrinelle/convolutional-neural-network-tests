{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "#### Neural Network for Suggestive CAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TempUser\\Anaconda3\\envs\\CPU_tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "#from tensorflow.python.framework import dtypes\n",
    "\n",
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "import param\n",
    "import time\n",
    "\n",
    "#from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "#import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#import pydotplus as pydot\n",
    "#import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# KERAS IMPORTS\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import History \n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###From https://gist.github.com/ambodi/408301bc5bc07bc5afa8748513ab9477#file-dataset-py-L74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Data from  https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving raw data to npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to directory\n",
    "path_train = 'raw_data/train/'\n",
    "path_test = 'raw_data/test/'\n",
    "path_val = 'raw_data/val/'\n",
    "\n",
    "img_size= 100\n",
    "\n",
    "#get folder names\n",
    "def directory_scan(path):\n",
    "    dirlist = [ item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item)) ]\n",
    "    return dirlist\n",
    "\n",
    "#build labels and image arrays, resize image to 100*100\n",
    "def read_images_in_folder(dirlist, path):\n",
    "    images=[]\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for i in range(len(dirlist)):\n",
    "        image_stack = []\n",
    "        for img in glob.glob(path+dirlist[i]+'/*jpg'):\n",
    "            count=count+1\n",
    "            labels.append(dirlist[i])\n",
    "            IMG = cv2.imread(img)\n",
    "            #RGB to grey scale\n",
    "            #IMG_2= cv2.cvtColor( IMG, cv2.COLOR_RGB2GRAY )\n",
    "            #resize to 100*100\n",
    "            im_resize = cv2.resize(IMG, (img_size, img_size), 3)\n",
    "            images.append(im_resize)\n",
    "        #np_images = np.array(image_stack)\n",
    "        \n",
    "    return images, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15008 training images\n",
      "There are 1120 testing images\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dirlist_train = directory_scan(path_train)\n",
    "    images_train,labels_train,count_train = read_images_in_folder(dirlist_train, path_train)\n",
    "    dirlist_test = directory_scan(path_test)\n",
    "    images_test,labels_test,count_test = read_images_in_folder(dirlist_test, path_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dirlist_val = directory_scan(path_val)\n",
    "    images_val,labels_val,count_val= read_images_in_folder(dirlist_val, path_val)\n",
    "\n",
    "np.save('npy-color/images_val',images_val)\n",
    "np.save('npy-color/labels_val',labels_val)\n",
    "\n",
    "#save to file\n",
    "np.save('npy-color/images_train',images_train)\n",
    "np.save('npy-color/labels_train',labels_train)\n",
    "np.save('npy-color/images_test',images_test)\n",
    "np.save('npy-color/labels_test',labels_test)\n",
    "\n",
    "print('There are '+ str(count_train) +' training images')\n",
    "print('There are '+ str(count_test) +' testing images')\n",
    "\n",
    "nb_img_train = count_train\n",
    "nb_img_test = count_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NPY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TRAIN\n",
    "#load images\n",
    "train_images = np.load('npy-color/images_train.npy')\n",
    "\n",
    "#load labels\n",
    "train_labels = np.load('npy-color/labels_train.npy')\n",
    "\n",
    "#LOAD TEST\n",
    "#load images\n",
    "test_images = np.load('npy-color/images_test.npy')\n",
    "\n",
    "\n",
    "#load labels\n",
    "test_labels = np.load('npy-color/labels_test.npy')\n",
    "\n",
    "#LOAD VALIDATION\n",
    "#load images\n",
    "val_images = np.load('npy-color/images_val.npy')\n",
    "\n",
    "#load labels\n",
    "val_labels = np.load('npy-color/labels_val.npy')\n",
    "\n",
    "img_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xd8bbee60f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd89d04fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 300\n",
    "print(train_labels[index])\n",
    "plt.imshow(train_images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'raw_data/train/'\n",
    "path_test = 'raw_data/test/'\n",
    "path_val = 'raw_data/val/'\n",
    "img_size= 100\n",
    "\n",
    "#get folder names\n",
    "def directory_scan(path):\n",
    "    dirlist = [ item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item)) ]\n",
    "    return dirlist\n",
    "\n",
    "dirlist_train = directory_scan(path_train)\n",
    "dirlist_test = directory_scan(path_test)\n",
    "dirlist_val = directory_scan(path_val)\n",
    "\n",
    "nb_img_train = len(train_images)\n",
    "nb_img_test = len(test_images)\n",
    "nb_img_val = len(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare and reshape testing/training sets\n",
    "#img_size = 100\n",
    "#nb_img_train = 1400\n",
    "#nb_img_test = 40\n",
    "\n",
    "train_img = train_images.reshape([-1,img_size,img_size,3])\n",
    "train_img = train_img.astype('float32') / 255\n",
    "\n",
    "test_img = test_images.reshape([-1,img_size,img_size,3])\n",
    "test_img = test_img.astype('float32') / 255\n",
    "\n",
    "val_img = val_images.reshape([-1,img_size,img_size,3])\n",
    "val_img = val_img.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL ONE HOT ENCODING\n",
    "#Training\n",
    "k=0\n",
    "for i in range(len(dirlist_train)):\n",
    "    for j in range(len(train_labels)):\n",
    "        if(train_labels[j]==dirlist_train[i]):\n",
    "            train_labels[j]=k\n",
    "    k=k+1\n",
    "\n",
    "#Testing\n",
    "m=0\n",
    "for l in range(len(dirlist_test)):\n",
    "    for h in range(len(test_labels)):\n",
    "        if(test_labels[h]==dirlist_test[l]):\n",
    "            test_labels[h]=m\n",
    "    m=m+1\n",
    "    \n",
    "#Validation\n",
    "g=0\n",
    "for l in range(len(dirlist_val)):\n",
    "    for h in range(len(val_labels)):\n",
    "        if(val_labels[h]==dirlist_val[l]):\n",
    "            val_labels[h]=g\n",
    "    g=g+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab= to_categorical(train_labels)\n",
    "test_lab = to_categorical(test_labels)\n",
    "val_lab = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_accuracy(train_acc, val_acc, Title):\n",
    "    x = [i for i in range(0, len(train_acc))]\n",
    "    y= train_acc\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    x = [i for i in range(0, len(val_acc))]\n",
    "    y= val_acc\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(ymax=1)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.xlim(xmin=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(Title)\n",
    "    plt.show()\n",
    "    \n",
    "def Plot_Loss(train_loss, val_loss, Title):\n",
    "    x = [i for i in range(0, len(val_loss))]\n",
    "    y= train_loss\n",
    "    plt.plot(x,y)\n",
    "\n",
    "    x = [i for i in range(0, len(val_loss))]\n",
    "    y= val_loss\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(ymax=2)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.xlim(xmin=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(Title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Import VGG pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = applications.VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Preprocess dataset with VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "train_features = conv_base.predict(train_img)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features/', train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape feature maps to match output of VGG model with input of additional layers\n",
    "train_features_reshaped = np.reshape(train_features, (len(train_img), 3 * 3 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features/train_features', train_features_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads feature map from file\n",
    "train_features_ = np.load('train_features/train_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15008, 4608)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 1 min 29.839893341064453 sec\n"
     ]
    }
   ],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "val_features = conv_base.predict(val_img)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.reshape(val_features, (len(val_img), 3 * 3 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Additional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(400, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(400, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(400, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(300, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(200, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(len(dirlist_train), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15008 samples, validate on 1120 samples\n",
      "Epoch 1/50\n",
      "15008/15008 [==============================] - 5s 315us/step - loss: 1.9647 - acc: 0.2293 - val_loss: 1.8019 - val_acc: 0.3214\n",
      "Epoch 2/50\n",
      "15008/15008 [==============================] - 4s 273us/step - loss: 1.5425 - acc: 0.4090 - val_loss: 1.4481 - val_acc: 0.4482\n",
      "Epoch 3/50\n",
      "15008/15008 [==============================] - 4s 267us/step - loss: 1.2665 - acc: 0.5318 - val_loss: 1.2998 - val_acc: 0.4795\n",
      "Epoch 4/50\n",
      "15008/15008 [==============================] - 4s 261us/step - loss: 1.1160 - acc: 0.5995 - val_loss: 1.2081 - val_acc: 0.5545\n",
      "Epoch 5/50\n",
      "15008/15008 [==============================] - 4s 263us/step - loss: 0.9873 - acc: 0.6563 - val_loss: 1.1554 - val_acc: 0.5616\n",
      "Epoch 6/50\n",
      "15008/15008 [==============================] - 4s 272us/step - loss: 0.8967 - acc: 0.6955 - val_loss: 1.0741 - val_acc: 0.6036\n",
      "Epoch 7/50\n",
      "15008/15008 [==============================] - 4s 261us/step - loss: 0.8043 - acc: 0.7233 - val_loss: 1.1642 - val_acc: 0.5812\n",
      "Epoch 8/50\n",
      "15008/15008 [==============================] - 4s 263us/step - loss: 0.7519 - acc: 0.7483 - val_loss: 0.9736 - val_acc: 0.6375\n",
      "Epoch 9/50\n",
      "15008/15008 [==============================] - 4s 263us/step - loss: 0.6889 - acc: 0.7660 - val_loss: 0.9007 - val_acc: 0.6741\n",
      "Epoch 10/50\n",
      "15008/15008 [==============================] - 4s 289us/step - loss: 0.6475 - acc: 0.7812 - val_loss: 1.0215 - val_acc: 0.6491\n",
      "Epoch 11/50\n",
      "15008/15008 [==============================] - 4s 278us/step - loss: 0.6021 - acc: 0.8004 - val_loss: 1.0078 - val_acc: 0.6786\n",
      "Epoch 12/50\n",
      "15008/15008 [==============================] - 4s 265us/step - loss: 0.5611 - acc: 0.8185 - val_loss: 0.9196 - val_acc: 0.7134\n",
      "Epoch 13/50\n",
      "15008/15008 [==============================] - 4s 256us/step - loss: 0.5218 - acc: 0.8325 - val_loss: 0.8923 - val_acc: 0.7214\n",
      "Epoch 14/50\n",
      "15008/15008 [==============================] - 4s 293us/step - loss: 0.4900 - acc: 0.8419 - val_loss: 0.8902 - val_acc: 0.7429\n",
      "Epoch 15/50\n",
      "15008/15008 [==============================] - 4s 275us/step - loss: 0.4611 - acc: 0.8487 - val_loss: 0.9252 - val_acc: 0.7366\n",
      "Epoch 16/50\n",
      "15008/15008 [==============================] - 4s 264us/step - loss: 0.4362 - acc: 0.8609 - val_loss: 0.8588 - val_acc: 0.7536\n",
      "Epoch 17/50\n",
      "15008/15008 [==============================] - 4s 255us/step - loss: 0.4193 - acc: 0.8637 - val_loss: 0.8254 - val_acc: 0.7348\n",
      "Epoch 18/50\n",
      "15008/15008 [==============================] - 4s 279us/step - loss: 0.4012 - acc: 0.8712 - val_loss: 0.9011 - val_acc: 0.7312\n",
      "Epoch 19/50\n",
      "15008/15008 [==============================] - 4s 277us/step - loss: 0.3715 - acc: 0.8785 - val_loss: 0.8868 - val_acc: 0.7518\n",
      "Epoch 20/50\n",
      "15008/15008 [==============================] - 4s 269us/step - loss: 0.3634 - acc: 0.8812 - val_loss: 0.9327 - val_acc: 0.7393\n",
      "Epoch 21/50\n",
      "15008/15008 [==============================] - 4s 269us/step - loss: 0.3484 - acc: 0.8892 - val_loss: 0.9343 - val_acc: 0.7518\n",
      "Epoch 22/50\n",
      "15008/15008 [==============================] - 4s 277us/step - loss: 0.3421 - acc: 0.8900 - val_loss: 0.8478 - val_acc: 0.7696\n",
      "Epoch 23/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.3276 - acc: 0.8970 - val_loss: 0.9731 - val_acc: 0.7384\n",
      "Epoch 24/50\n",
      "15008/15008 [==============================] - 4s 272us/step - loss: 0.3169 - acc: 0.8998 - val_loss: 0.9513 - val_acc: 0.7696\n",
      "Epoch 25/50\n",
      "15008/15008 [==============================] - 4s 275us/step - loss: 0.3092 - acc: 0.9012 - val_loss: 1.0022 - val_acc: 0.7438\n",
      "Epoch 26/50\n",
      "15008/15008 [==============================] - 4s 278us/step - loss: 0.2951 - acc: 0.9053 - val_loss: 0.9405 - val_acc: 0.7661\n",
      "Epoch 27/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.2843 - acc: 0.9092 - val_loss: 1.0340 - val_acc: 0.7527\n",
      "Epoch 28/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.2748 - acc: 0.9132 - val_loss: 1.0588 - val_acc: 0.7607\n",
      "Epoch 29/50\n",
      "15008/15008 [==============================] - 4s 278us/step - loss: 0.2660 - acc: 0.9159 - val_loss: 0.9075 - val_acc: 0.7661\n",
      "Epoch 30/50\n",
      "15008/15008 [==============================] - 4s 277us/step - loss: 0.2600 - acc: 0.9171 - val_loss: 0.9009 - val_acc: 0.7848\n",
      "Epoch 31/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.2437 - acc: 0.9232 - val_loss: 0.9798 - val_acc: 0.7830\n",
      "Epoch 32/50\n",
      "15008/15008 [==============================] - 4s 272us/step - loss: 0.2419 - acc: 0.9240 - val_loss: 1.1319 - val_acc: 0.7679\n",
      "Epoch 33/50\n",
      "15008/15008 [==============================] - 4s 278us/step - loss: 0.2449 - acc: 0.9239 - val_loss: 0.9411 - val_acc: 0.7786\n",
      "Epoch 34/50\n",
      "15008/15008 [==============================] - 4s 280us/step - loss: 0.2261 - acc: 0.9298 - val_loss: 0.9861 - val_acc: 0.7821\n",
      "Epoch 35/50\n",
      "15008/15008 [==============================] - 4s 273us/step - loss: 0.2330 - acc: 0.9296 - val_loss: 0.9603 - val_acc: 0.7768\n",
      "Epoch 36/50\n",
      "15008/15008 [==============================] - 4s 275us/step - loss: 0.2126 - acc: 0.9349 - val_loss: 1.1396 - val_acc: 0.7607\n",
      "Epoch 37/50\n",
      "15008/15008 [==============================] - 4s 280us/step - loss: 0.2068 - acc: 0.9386 - val_loss: 0.9967 - val_acc: 0.7929\n",
      "Epoch 38/50\n",
      "15008/15008 [==============================] - 4s 274us/step - loss: 0.2026 - acc: 0.9358 - val_loss: 1.0441 - val_acc: 0.7777\n",
      "Epoch 39/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.2064 - acc: 0.9374 - val_loss: 1.0695 - val_acc: 0.7607\n",
      "Epoch 40/50\n",
      "15008/15008 [==============================] - 4s 269us/step - loss: 0.1893 - acc: 0.9414 - val_loss: 1.0377 - val_acc: 0.7875\n",
      "Epoch 41/50\n",
      "15008/15008 [==============================] - 4s 281us/step - loss: 0.1936 - acc: 0.9411 - val_loss: 1.0142 - val_acc: 0.7875\n",
      "Epoch 42/50\n",
      "15008/15008 [==============================] - 4s 269us/step - loss: 0.1845 - acc: 0.9442 - val_loss: 1.0817 - val_acc: 0.7848\n",
      "Epoch 43/50\n",
      "15008/15008 [==============================] - 4s 268us/step - loss: 0.1819 - acc: 0.9459 - val_loss: 1.0212 - val_acc: 0.7911\n",
      "Epoch 44/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.1799 - acc: 0.9446 - val_loss: 1.0757 - val_acc: 0.7812\n",
      "Epoch 45/50\n",
      "15008/15008 [==============================] - 4s 282us/step - loss: 0.1678 - acc: 0.9487 - val_loss: 1.0095 - val_acc: 0.8027\n",
      "Epoch 46/50\n",
      "15008/15008 [==============================] - 4s 272us/step - loss: 0.1694 - acc: 0.9465 - val_loss: 1.0454 - val_acc: 0.7929\n",
      "Epoch 47/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.1672 - acc: 0.9502 - val_loss: 1.0799 - val_acc: 0.7821\n",
      "Epoch 48/50\n",
      "15008/15008 [==============================] - 4s 270us/step - loss: 0.1640 - acc: 0.9490 - val_loss: 1.0071 - val_acc: 0.7902\n",
      "Epoch 49/50\n",
      "15008/15008 [==============================] - 4s 282us/step - loss: 0.1534 - acc: 0.9539 - val_loss: 1.0459 - val_acc: 0.7955\n",
      "Epoch 50/50\n",
      "15008/15008 [==============================] - 4s 276us/step - loss: 0.1541 - acc: 0.9524 - val_loss: 1.2501 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "history= History()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "training = model.fit(train_features_,\n",
    "                    train_lab,\n",
    "                    epochs=50,\n",
    "                    batch_size=250,\n",
    "                    validation_data=(val_features,val_lab), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaca015e518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaca015e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plot_accuracy(history.history['acc'], history.history['val_acc'],\"Accuracy\")\n",
    "\n",
    "Plot_Loss(history.history['loss'],history.history['val_loss'], \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_features)\n",
    "\n",
    "#one hot encoding\n",
    "for i in range(len(predictions)):\n",
    "    length = len(predictions[i])\n",
    "    index = np.argmax(predictions[i])\n",
    "    predictions[i][index] = 1\n",
    "    for j in range(length):\n",
    "        if(j != index):\n",
    "            predictions[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bed       0.83      0.71      0.77       150\n",
      "      bench       0.85      0.56      0.67        70\n",
      "    cabinet       0.81      0.78      0.79       150\n",
      "      chair       0.86      0.94      0.90       150\n",
      "      couch       0.68      0.88      0.77       150\n",
      "       lamp       0.81      0.79      0.80       150\n",
      "      plant       0.95      0.91      0.93       150\n",
      "      table       0.80      0.83      0.81       150\n",
      "\n",
      "avg / total       0.82      0.82      0.81      1120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, predictions, target_names = dirlist_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "conv_base_resnet = applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(197, 197, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 197, 197, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 99, 99, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 99, 99, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 99, 99, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 49, 49, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 49, 49, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 49, 49, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 49, 49, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 49, 49, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 49, 49, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 49, 49, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 49, 49, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 49, 49, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 49, 49, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 49, 49, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 49, 49, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 49, 49, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 49, 49, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 49, 49, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 49, 49, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 25, 25, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 25, 25, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 25, 25, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 25, 25, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 25, 25, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 25, 25, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 25, 25, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 25, 25, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 25, 25, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 25, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 25, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 25, 25, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 25, 25, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 25, 25, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 25, 25, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 25, 25, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 25, 25, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 25, 25, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 13, 13, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 13, 13, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 13, 13, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 13, 13, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 13, 13, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 13, 13, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 13, 13, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 13, 13, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 13, 13, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 13, 13, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 13, 13, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 13, 13, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 13, 13, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 13, 13, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 13, 13, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 13, 13, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 13, 13, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 13, 13, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 13, 13, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 13, 13, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 13, 13, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 13, 13, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 13, 13, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 13, 13, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 13, 13, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 13, 13, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 13, 13, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_49[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_197_197 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_img)):\n",
    "    img = cv2.resize(train_img[i], (197, 197))\n",
    "    train_img_197_197.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_197 = np.array(train_img_197_197)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 73 min 45.37593460083008 sec\n"
     ]
    }
   ],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "train_features_1 = conv_base_resnet.predict(train_img_197)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_1 = train_features_1.reshape(len(train_img),2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features_resnet/train_features_resnet', train_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads feature map from file\n",
    "train_features_resnet = np.load('train_features_resnet/train_features_resnet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15008, 1, 1, 2048)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_resnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_1 = train_features_resnet.reshape(len(train_img),2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_197_197 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_img)):\n",
    "    img = cv2.resize(val_img[i], (197, 197))\n",
    "    val_img_197_197.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_197 = np.array(val_img_197_197)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 5 min 14.945082187652588 sec\n"
     ]
    }
   ],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "val_features_resnet = conv_base_resnet.predict(val_img_197)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features_resnet/val_features_resnet', val_features_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads feature map from file\n",
    "val_features_1 = np.load('train_features_resnet/val_features_resnet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features_1 = val_features_1.reshape(len(val_img),2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = models.Sequential()\n",
    "model_resnet.add(layers.Dense(400, activation='relu', input_dim=1 * 1 * 2048))\n",
    "model_resnet.add(layers.Dropout(0.5))\n",
    "model_resnet.add(layers.Dense(400, activation='relu'))\n",
    "model_resnet.add(layers.Dropout(0.2))\n",
    "\n",
    "model_resnet.add(layers.Dense(len(dirlist_train), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15008 samples, validate on 1120 samples\n",
      "Epoch 1/5000\n",
      "15008/15008 [==============================] - 2s 138us/step - loss: 1.9735 - acc: 0.2241 - val_loss: 2.0911 - val_acc: 0.2920\n",
      "Epoch 2/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.8617 - acc: 0.2997 - val_loss: 2.1049 - val_acc: 0.1491\n",
      "Epoch 3/5000\n",
      "15008/15008 [==============================] - 2s 105us/step - loss: 1.7922 - acc: 0.3411 - val_loss: 1.9186 - val_acc: 0.2321\n",
      "Epoch 4/5000\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 1.7239 - acc: 0.3766 - val_loss: 1.9488 - val_acc: 0.2509\n",
      "Epoch 5/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.6803 - acc: 0.3959 - val_loss: 1.8276 - val_acc: 0.2937\n",
      "Epoch 6/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.6364 - acc: 0.4113 - val_loss: 1.9570 - val_acc: 0.2268\n",
      "Epoch 7/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.6149 - acc: 0.4225 - val_loss: 1.8944 - val_acc: 0.2857\n",
      "Epoch 8/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.5767 - acc: 0.4378 - val_loss: 1.8678 - val_acc: 0.2929\n",
      "Epoch 9/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.5498 - acc: 0.4444 - val_loss: 2.0533 - val_acc: 0.3313\n",
      "Epoch 10/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.5363 - acc: 0.4496 - val_loss: 1.6804 - val_acc: 0.3375\n",
      "Epoch 11/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.5064 - acc: 0.4587 - val_loss: 1.6569 - val_acc: 0.4259\n",
      "Epoch 12/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.5041 - acc: 0.4628 - val_loss: 1.7597 - val_acc: 0.3643\n",
      "Epoch 13/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.4780 - acc: 0.4711 - val_loss: 1.7594 - val_acc: 0.3464\n",
      "Epoch 14/5000\n",
      "15008/15008 [==============================] - 2s 101us/step - loss: 1.4603 - acc: 0.4773 - val_loss: 1.6806 - val_acc: 0.4491\n",
      "Epoch 15/5000\n",
      "15008/15008 [==============================] - 2s 102us/step - loss: 1.4536 - acc: 0.4771 - val_loss: 1.5884 - val_acc: 0.4152\n",
      "Epoch 16/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.4329 - acc: 0.4860 - val_loss: 1.8050 - val_acc: 0.3054\n",
      "Epoch 17/5000\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 1.4339 - acc: 0.4877 - val_loss: 1.9638 - val_acc: 0.2688\n",
      "Epoch 18/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.4271 - acc: 0.4877 - val_loss: 1.6803 - val_acc: 0.4161\n",
      "Epoch 19/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.4052 - acc: 0.4955 - val_loss: 1.5675 - val_acc: 0.4018\n",
      "Epoch 20/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.3928 - acc: 0.4955 - val_loss: 1.6790 - val_acc: 0.3946\n",
      "Epoch 21/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.3904 - acc: 0.5021 - val_loss: 2.1778 - val_acc: 0.3045\n",
      "Epoch 22/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3826 - acc: 0.5092 - val_loss: 1.6133 - val_acc: 0.3795\n",
      "Epoch 23/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3751 - acc: 0.5073 - val_loss: 1.5885 - val_acc: 0.3795\n",
      "Epoch 24/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3474 - acc: 0.5145 - val_loss: 1.6179 - val_acc: 0.4330\n",
      "Epoch 25/5000\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 1.3532 - acc: 0.5142 - val_loss: 2.0820 - val_acc: 0.3179\n",
      "Epoch 26/5000\n",
      "15008/15008 [==============================] - 2s 103us/step - loss: 1.3571 - acc: 0.5177 - val_loss: 1.5263 - val_acc: 0.4670\n",
      "Epoch 27/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.3339 - acc: 0.5187 - val_loss: 1.7683 - val_acc: 0.3920\n",
      "Epoch 28/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3320 - acc: 0.5255 - val_loss: 1.5907 - val_acc: 0.4812\n",
      "Epoch 29/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.3360 - acc: 0.5199 - val_loss: 1.6461 - val_acc: 0.4464\n",
      "Epoch 30/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3152 - acc: 0.5278 - val_loss: 1.7069 - val_acc: 0.4875\n",
      "Epoch 31/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.3132 - acc: 0.5311 - val_loss: 1.6517 - val_acc: 0.3679\n",
      "Epoch 32/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3155 - acc: 0.5320 - val_loss: 1.7947 - val_acc: 0.4107\n",
      "Epoch 33/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.3042 - acc: 0.5342 - val_loss: 1.5059 - val_acc: 0.4571\n",
      "Epoch 34/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.3062 - acc: 0.5318 - val_loss: 2.3134 - val_acc: 0.2170\n",
      "Epoch 35/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.3053 - acc: 0.5362 - val_loss: 2.1298 - val_acc: 0.2812\n",
      "Epoch 36/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.3084 - acc: 0.5309 - val_loss: 1.4558 - val_acc: 0.4670\n",
      "Epoch 37/5000\n",
      "15008/15008 [==============================] - 2s 104us/step - loss: 1.2702 - acc: 0.5379 - val_loss: 1.6032 - val_acc: 0.4080\n",
      "Epoch 38/5000\n",
      "15008/15008 [==============================] - 1s 100us/step - loss: 1.2797 - acc: 0.5412 - val_loss: 1.4406 - val_acc: 0.4955\n",
      "Epoch 39/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.2700 - acc: 0.5452 - val_loss: 1.7144 - val_acc: 0.3848\n",
      "Epoch 40/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2773 - acc: 0.5413 - val_loss: 1.6783 - val_acc: 0.3857\n",
      "Epoch 41/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2655 - acc: 0.5460 - val_loss: 1.4962 - val_acc: 0.4411\n",
      "Epoch 42/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2480 - acc: 0.5497 - val_loss: 1.4604 - val_acc: 0.4768\n",
      "Epoch 43/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 1.2437 - acc: 0.5540 - val_loss: 1.6000 - val_acc: 0.4384\n",
      "Epoch 44/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.2507 - acc: 0.5565 - val_loss: 1.7115 - val_acc: 0.4420\n",
      "Epoch 45/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2404 - acc: 0.5560 - val_loss: 1.5714 - val_acc: 0.4089\n",
      "Epoch 46/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 1.2371 - acc: 0.5612 - val_loss: 2.0528 - val_acc: 0.3937\n",
      "Epoch 47/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.2420 - acc: 0.5546 - val_loss: 2.3021 - val_acc: 0.2875\n",
      "Epoch 48/5000\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 1.2466 - acc: 0.5562 - val_loss: 1.4167 - val_acc: 0.5098\n",
      "Epoch 49/5000\n",
      "15008/15008 [==============================] - 2s 104us/step - loss: 1.2280 - acc: 0.5598 - val_loss: 1.7141 - val_acc: 0.4232\n",
      "Epoch 50/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.2196 - acc: 0.5598 - val_loss: 1.4697 - val_acc: 0.4643\n",
      "Epoch 51/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2311 - acc: 0.5584 - val_loss: 1.3928 - val_acc: 0.5339\n",
      "Epoch 52/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.2111 - acc: 0.5695 - val_loss: 1.6660 - val_acc: 0.4723\n",
      "Epoch 53/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.2065 - acc: 0.5734 - val_loss: 1.6060 - val_acc: 0.4795\n",
      "Epoch 54/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.2016 - acc: 0.5708 - val_loss: 1.8598 - val_acc: 0.3982\n",
      "Epoch 55/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 1.2146 - acc: 0.5663 - val_loss: 1.4736 - val_acc: 0.4571\n",
      "Epoch 56/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 1.1955 - acc: 0.5745 - val_loss: 1.7717 - val_acc: 0.4384\n",
      "Epoch 57/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 1.2018 - acc: 0.5731 - val_loss: 1.7922 - val_acc: 0.4187\n",
      "Epoch 58/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.1983 - acc: 0.5768 - val_loss: 1.3524 - val_acc: 0.5509\n",
      "Epoch 59/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 94us/step - loss: 1.1887 - acc: 0.5758 - val_loss: 1.8064 - val_acc: 0.4634\n",
      "Epoch 60/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 1.1890 - acc: 0.5754 - val_loss: 1.6095 - val_acc: 0.4393\n",
      "Epoch 61/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1898 - acc: 0.5752 - val_loss: 2.0592 - val_acc: 0.3375\n",
      "Epoch 62/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.2011 - acc: 0.5732 - val_loss: 2.1883 - val_acc: 0.3759\n",
      "Epoch 63/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 1.1838 - acc: 0.5780 - val_loss: 1.5035 - val_acc: 0.4339\n",
      "Epoch 64/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1854 - acc: 0.5750 - val_loss: 1.7409 - val_acc: 0.4196\n",
      "Epoch 65/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1873 - acc: 0.5723 - val_loss: 1.4975 - val_acc: 0.4491\n",
      "Epoch 66/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1625 - acc: 0.5878 - val_loss: 1.7883 - val_acc: 0.4357\n",
      "Epoch 67/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1893 - acc: 0.5784 - val_loss: 1.7374 - val_acc: 0.4768\n",
      "Epoch 68/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1625 - acc: 0.5857 - val_loss: 2.0623 - val_acc: 0.3750\n",
      "Epoch 69/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1778 - acc: 0.5805 - val_loss: 2.0750 - val_acc: 0.3652\n",
      "Epoch 70/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1758 - acc: 0.5788 - val_loss: 1.7925 - val_acc: 0.4598\n",
      "Epoch 71/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 1.1754 - acc: 0.5810 - val_loss: 2.0459 - val_acc: 0.3554\n",
      "Epoch 72/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.1736 - acc: 0.5815 - val_loss: 2.2240 - val_acc: 0.2643\n",
      "Epoch 73/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1682 - acc: 0.5853 - val_loss: 1.4728 - val_acc: 0.5152\n",
      "Epoch 74/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1457 - acc: 0.5881 - val_loss: 1.3666 - val_acc: 0.5580\n",
      "Epoch 75/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1442 - acc: 0.5928 - val_loss: 1.4039 - val_acc: 0.4929\n",
      "Epoch 76/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1457 - acc: 0.5906 - val_loss: 2.2364 - val_acc: 0.3455\n",
      "Epoch 77/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1596 - acc: 0.5886 - val_loss: 2.2512 - val_acc: 0.3063\n",
      "Epoch 78/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1601 - acc: 0.5934 - val_loss: 1.5316 - val_acc: 0.4634\n",
      "Epoch 79/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1328 - acc: 0.5954 - val_loss: 1.8003 - val_acc: 0.4000\n",
      "Epoch 80/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1427 - acc: 0.5920 - val_loss: 1.4702 - val_acc: 0.4830\n",
      "Epoch 81/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1482 - acc: 0.5912 - val_loss: 1.4786 - val_acc: 0.4321\n",
      "Epoch 82/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1235 - acc: 0.5989 - val_loss: 1.4833 - val_acc: 0.4902\n",
      "Epoch 83/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1276 - acc: 0.5952 - val_loss: 1.5698 - val_acc: 0.5009\n",
      "Epoch 84/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.1300 - acc: 0.5969 - val_loss: 2.3249 - val_acc: 0.3348\n",
      "Epoch 85/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 1.1452 - acc: 0.5994 - val_loss: 1.7453 - val_acc: 0.4375\n",
      "Epoch 86/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1131 - acc: 0.6035 - val_loss: 1.4398 - val_acc: 0.4491\n",
      "Epoch 87/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1214 - acc: 0.5936 - val_loss: 1.6138 - val_acc: 0.4714\n",
      "Epoch 88/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1223 - acc: 0.5989 - val_loss: 1.6686 - val_acc: 0.4188\n",
      "Epoch 89/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1231 - acc: 0.5977 - val_loss: 1.6451 - val_acc: 0.4902\n",
      "Epoch 90/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.1157 - acc: 0.6024 - val_loss: 1.4989 - val_acc: 0.4696\n",
      "Epoch 91/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1058 - acc: 0.6044 - val_loss: 1.6010 - val_acc: 0.4500\n",
      "Epoch 92/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1321 - acc: 0.5958 - val_loss: 1.6984 - val_acc: 0.4187\n",
      "Epoch 93/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1141 - acc: 0.6005 - val_loss: 1.4723 - val_acc: 0.4920\n",
      "Epoch 94/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1155 - acc: 0.6050 - val_loss: 1.4864 - val_acc: 0.5232\n",
      "Epoch 95/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0994 - acc: 0.6049 - val_loss: 1.5149 - val_acc: 0.4786\n",
      "Epoch 96/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 1.1004 - acc: 0.6043 - val_loss: 1.8151 - val_acc: 0.4098\n",
      "Epoch 97/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.1037 - acc: 0.6061 - val_loss: 1.5199 - val_acc: 0.4455\n",
      "Epoch 98/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0970 - acc: 0.6095 - val_loss: 1.7283 - val_acc: 0.4536\n",
      "Epoch 99/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0994 - acc: 0.6093 - val_loss: 1.4104 - val_acc: 0.5438\n",
      "Epoch 100/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0936 - acc: 0.6039 - val_loss: 1.4474 - val_acc: 0.4768\n",
      "Epoch 101/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.1018 - acc: 0.6057 - val_loss: 1.2806 - val_acc: 0.5625\n",
      "Epoch 102/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0809 - acc: 0.6119 - val_loss: 2.1078 - val_acc: 0.3723\n",
      "Epoch 103/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0946 - acc: 0.6119 - val_loss: 1.5816 - val_acc: 0.4812\n",
      "Epoch 104/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0895 - acc: 0.6105 - val_loss: 1.4895 - val_acc: 0.4866\n",
      "Epoch 105/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0840 - acc: 0.6096 - val_loss: 1.7482 - val_acc: 0.4232\n",
      "Epoch 106/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0934 - acc: 0.6157 - val_loss: 1.3733 - val_acc: 0.5330\n",
      "Epoch 107/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0731 - acc: 0.6177 - val_loss: 2.0580 - val_acc: 0.3875\n",
      "Epoch 108/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0921 - acc: 0.6129 - val_loss: 1.8348 - val_acc: 0.4357\n",
      "Epoch 109/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.0870 - acc: 0.6105 - val_loss: 1.8479 - val_acc: 0.4196\n",
      "Epoch 110/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 1.0808 - acc: 0.6137 - val_loss: 1.6134 - val_acc: 0.4304\n",
      "Epoch 111/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0875 - acc: 0.6113 - val_loss: 1.4924 - val_acc: 0.4518\n",
      "Epoch 112/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0657 - acc: 0.6185 - val_loss: 1.9730 - val_acc: 0.3848\n",
      "Epoch 113/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0844 - acc: 0.6131 - val_loss: 1.4583 - val_acc: 0.4920\n",
      "Epoch 114/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0603 - acc: 0.6219 - val_loss: 1.6904 - val_acc: 0.4339\n",
      "Epoch 115/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0809 - acc: 0.6133 - val_loss: 1.9209 - val_acc: 0.4589\n",
      "Epoch 116/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0719 - acc: 0.6239 - val_loss: 1.4260 - val_acc: 0.5089\n",
      "Epoch 117/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0771 - acc: 0.6144 - val_loss: 1.8537 - val_acc: 0.4438\n",
      "Epoch 118/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 1.0650 - acc: 0.6187 - val_loss: 1.8347 - val_acc: 0.4955\n",
      "Epoch 119/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0669 - acc: 0.6215 - val_loss: 1.4429 - val_acc: 0.5179\n",
      "Epoch 120/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 1.0580 - acc: 0.6226 - val_loss: 1.6130 - val_acc: 0.4848\n",
      "Epoch 121/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.0803 - acc: 0.6195 - val_loss: 1.8705 - val_acc: 0.3339\n",
      "Epoch 122/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 1.0677 - acc: 0.6194 - val_loss: 1.3920 - val_acc: 0.5286\n",
      "Epoch 123/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0547 - acc: 0.6220 - val_loss: 1.8589 - val_acc: 0.4241\n",
      "Epoch 124/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0597 - acc: 0.6272 - val_loss: 1.9838 - val_acc: 0.4107\n",
      "Epoch 125/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0653 - acc: 0.6217 - val_loss: 1.7745 - val_acc: 0.4795\n",
      "Epoch 126/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0460 - acc: 0.6282 - val_loss: 1.7161 - val_acc: 0.4688\n",
      "Epoch 127/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0494 - acc: 0.6257 - val_loss: 2.2671 - val_acc: 0.3768\n",
      "Epoch 128/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0693 - acc: 0.6275 - val_loss: 1.5724 - val_acc: 0.4857\n",
      "Epoch 129/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0439 - acc: 0.6301 - val_loss: 1.2403 - val_acc: 0.5920\n",
      "Epoch 130/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0525 - acc: 0.6228 - val_loss: 1.4754 - val_acc: 0.4000\n",
      "Epoch 131/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0616 - acc: 0.6191 - val_loss: 1.3456 - val_acc: 0.4750\n",
      "Epoch 132/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0392 - acc: 0.6294 - val_loss: 1.3541 - val_acc: 0.5277\n",
      "Epoch 133/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0426 - acc: 0.6279 - val_loss: 1.5556 - val_acc: 0.5438\n",
      "Epoch 134/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.0385 - acc: 0.6329 - val_loss: 1.9347 - val_acc: 0.3732\n",
      "Epoch 135/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 1.0394 - acc: 0.6339 - val_loss: 2.0439 - val_acc: 0.3830\n",
      "Epoch 136/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0492 - acc: 0.6292 - val_loss: 1.8424 - val_acc: 0.4670\n",
      "Epoch 137/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0513 - acc: 0.6285 - val_loss: 1.3952 - val_acc: 0.5089\n",
      "Epoch 138/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0284 - acc: 0.6354 - val_loss: 1.3963 - val_acc: 0.4857\n",
      "Epoch 139/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0234 - acc: 0.6322 - val_loss: 2.2487 - val_acc: 0.4357\n",
      "Epoch 140/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0721 - acc: 0.6240 - val_loss: 1.6611 - val_acc: 0.4223\n",
      "Epoch 141/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0365 - acc: 0.6319 - val_loss: 1.8923 - val_acc: 0.4562\n",
      "Epoch 142/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0316 - acc: 0.6337 - val_loss: 1.7154 - val_acc: 0.4187\n",
      "Epoch 143/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0318 - acc: 0.6325 - val_loss: 1.6867 - val_acc: 0.4723\n",
      "Epoch 144/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0281 - acc: 0.6327 - val_loss: 1.3605 - val_acc: 0.4955\n",
      "Epoch 145/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 1.0239 - acc: 0.6355 - val_loss: 1.7479 - val_acc: 0.4661\n",
      "Epoch 146/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 1.0281 - acc: 0.6325 - val_loss: 1.8714 - val_acc: 0.4116\n",
      "Epoch 147/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 1.0242 - acc: 0.6376 - val_loss: 1.1875 - val_acc: 0.5893\n",
      "Epoch 148/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 1.0325 - acc: 0.6343 - val_loss: 1.8527 - val_acc: 0.4125\n",
      "Epoch 149/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0350 - acc: 0.6351 - val_loss: 2.3959 - val_acc: 0.3545\n",
      "Epoch 150/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0292 - acc: 0.6336 - val_loss: 1.7032 - val_acc: 0.4277\n",
      "Epoch 151/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0300 - acc: 0.6297 - val_loss: 1.8424 - val_acc: 0.3920\n",
      "Epoch 152/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0301 - acc: 0.6379 - val_loss: 1.7860 - val_acc: 0.4446\n",
      "Epoch 153/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0104 - acc: 0.6426 - val_loss: 1.6362 - val_acc: 0.4911\n",
      "Epoch 154/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0135 - acc: 0.6399 - val_loss: 1.6745 - val_acc: 0.4580\n",
      "Epoch 155/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0243 - acc: 0.6357 - val_loss: 1.3480 - val_acc: 0.5741\n",
      "Epoch 156/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9990 - acc: 0.6463 - val_loss: 1.4331 - val_acc: 0.4795\n",
      "Epoch 157/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0168 - acc: 0.6376 - val_loss: 1.7846 - val_acc: 0.4634\n",
      "Epoch 158/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 1.0113 - acc: 0.6375 - val_loss: 1.5224 - val_acc: 0.5384\n",
      "Epoch 159/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 1.0129 - acc: 0.6431 - val_loss: 1.4462 - val_acc: 0.5027\n",
      "Epoch 160/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0130 - acc: 0.6383 - val_loss: 1.1735 - val_acc: 0.5964\n",
      "Epoch 161/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0172 - acc: 0.6391 - val_loss: 1.3583 - val_acc: 0.5304\n",
      "Epoch 162/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9981 - acc: 0.6411 - val_loss: 1.3543 - val_acc: 0.5464\n",
      "Epoch 163/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0016 - acc: 0.6395 - val_loss: 1.8237 - val_acc: 0.4437\n",
      "Epoch 164/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0021 - acc: 0.6447 - val_loss: 1.2931 - val_acc: 0.5473\n",
      "Epoch 165/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9869 - acc: 0.6475 - val_loss: 2.0188 - val_acc: 0.3938\n",
      "Epoch 166/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 1.0084 - acc: 0.6451 - val_loss: 1.5717 - val_acc: 0.4875\n",
      "Epoch 167/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9963 - acc: 0.6421 - val_loss: 1.5440 - val_acc: 0.4938\n",
      "Epoch 168/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.9912 - acc: 0.6443 - val_loss: 1.2774 - val_acc: 0.5768\n",
      "Epoch 169/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9853 - acc: 0.6507 - val_loss: 1.4722 - val_acc: 0.5170\n",
      "Epoch 170/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 1.0045 - acc: 0.6418 - val_loss: 1.4607 - val_acc: 0.5107\n",
      "Epoch 171/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9928 - acc: 0.6451 - val_loss: 1.4340 - val_acc: 0.4964\n",
      "Epoch 172/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9814 - acc: 0.6521 - val_loss: 1.9263 - val_acc: 0.4438\n",
      "Epoch 173/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 1.0082 - acc: 0.6425 - val_loss: 1.5977 - val_acc: 0.4857\n",
      "Epoch 174/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9920 - acc: 0.6473 - val_loss: 1.7075 - val_acc: 0.4643\n",
      "Epoch 175/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9973 - acc: 0.6466 - val_loss: 1.3024 - val_acc: 0.5571\n",
      "Epoch 176/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9842 - acc: 0.6474 - val_loss: 1.7923 - val_acc: 0.4634\n",
      "Epoch 177/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9980 - acc: 0.6442 - val_loss: 1.5702 - val_acc: 0.5232\n",
      "Epoch 178/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9801 - acc: 0.6496 - val_loss: 1.7205 - val_acc: 0.4741\n",
      "Epoch 179/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9949 - acc: 0.6475 - val_loss: 1.4275 - val_acc: 0.5027\n",
      "Epoch 180/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9742 - acc: 0.6501 - val_loss: 1.6725 - val_acc: 0.4875\n",
      "Epoch 181/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9974 - acc: 0.6424 - val_loss: 1.2750 - val_acc: 0.5420\n",
      "Epoch 182/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9740 - acc: 0.6513 - val_loss: 2.4961 - val_acc: 0.3027\n",
      "Epoch 183/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 1.0051 - acc: 0.6421 - val_loss: 1.5641 - val_acc: 0.4687\n",
      "Epoch 184/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.9774 - acc: 0.6523 - val_loss: 1.3133 - val_acc: 0.5598\n",
      "Epoch 185/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9531 - acc: 0.6573 - val_loss: 1.3736 - val_acc: 0.5170\n",
      "Epoch 186/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9837 - acc: 0.6509 - val_loss: 2.1407 - val_acc: 0.3732\n",
      "Epoch 187/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9873 - acc: 0.6520 - val_loss: 1.8725 - val_acc: 0.4384\n",
      "Epoch 188/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9836 - acc: 0.6559 - val_loss: 1.5079 - val_acc: 0.4938\n",
      "Epoch 189/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9857 - acc: 0.6490 - val_loss: 1.3176 - val_acc: 0.5071\n",
      "Epoch 190/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9724 - acc: 0.6535 - val_loss: 1.2793 - val_acc: 0.5661\n",
      "Epoch 191/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9822 - acc: 0.6492 - val_loss: 1.3405 - val_acc: 0.5134\n",
      "Epoch 192/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9769 - acc: 0.6531 - val_loss: 1.7026 - val_acc: 0.4661\n",
      "Epoch 193/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9695 - acc: 0.6553 - val_loss: 1.6859 - val_acc: 0.5107\n",
      "Epoch 194/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9772 - acc: 0.6541 - val_loss: 1.5283 - val_acc: 0.5009\n",
      "Epoch 195/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9799 - acc: 0.6535 - val_loss: 1.5351 - val_acc: 0.4509\n",
      "Epoch 196/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9742 - acc: 0.6526 - val_loss: 1.7086 - val_acc: 0.4491\n",
      "Epoch 197/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9826 - acc: 0.6506 - val_loss: 1.5296 - val_acc: 0.5304\n",
      "Epoch 198/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9791 - acc: 0.6543 - val_loss: 1.5520 - val_acc: 0.4598\n",
      "Epoch 199/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9790 - acc: 0.6525 - val_loss: 1.6478 - val_acc: 0.5143\n",
      "Epoch 200/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9720 - acc: 0.6557 - val_loss: 1.4181 - val_acc: 0.4929\n",
      "Epoch 201/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9671 - acc: 0.6555 - val_loss: 2.0760 - val_acc: 0.4179\n",
      "Epoch 202/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9764 - acc: 0.6565 - val_loss: 1.3916 - val_acc: 0.5420\n",
      "Epoch 203/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9577 - acc: 0.6592 - val_loss: 1.6726 - val_acc: 0.4375\n",
      "Epoch 204/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9708 - acc: 0.6562 - val_loss: 1.3491 - val_acc: 0.5482\n",
      "Epoch 205/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9528 - acc: 0.6641 - val_loss: 1.7386 - val_acc: 0.4482\n",
      "Epoch 206/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9616 - acc: 0.6591 - val_loss: 1.2559 - val_acc: 0.5348\n",
      "Epoch 207/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9562 - acc: 0.6626 - val_loss: 1.5093 - val_acc: 0.4812\n",
      "Epoch 208/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9653 - acc: 0.6554 - val_loss: 1.3412 - val_acc: 0.4893\n",
      "Epoch 209/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.9572 - acc: 0.6594 - val_loss: 1.8186 - val_acc: 0.4125\n",
      "Epoch 210/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9516 - acc: 0.6593 - val_loss: 1.4328 - val_acc: 0.5063\n",
      "Epoch 211/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9497 - acc: 0.6616 - val_loss: 1.4435 - val_acc: 0.4813\n",
      "Epoch 212/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9702 - acc: 0.6572 - val_loss: 1.6610 - val_acc: 0.4813\n",
      "Epoch 213/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9424 - acc: 0.6655 - val_loss: 1.6397 - val_acc: 0.5063\n",
      "Epoch 214/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9603 - acc: 0.6619 - val_loss: 1.3029 - val_acc: 0.5688\n",
      "Epoch 215/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9455 - acc: 0.6605 - val_loss: 1.2989 - val_acc: 0.5652\n",
      "Epoch 216/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9457 - acc: 0.6653 - val_loss: 1.3792 - val_acc: 0.5018\n",
      "Epoch 217/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9340 - acc: 0.6713 - val_loss: 1.5452 - val_acc: 0.4964\n",
      "Epoch 218/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9443 - acc: 0.6654 - val_loss: 1.3149 - val_acc: 0.5884\n",
      "Epoch 219/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9480 - acc: 0.6618 - val_loss: 1.2252 - val_acc: 0.5732\n",
      "Epoch 220/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.9436 - acc: 0.6668 - val_loss: 1.2641 - val_acc: 0.5500\n",
      "Epoch 221/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.9357 - acc: 0.6633 - val_loss: 1.3788 - val_acc: 0.5571\n",
      "Epoch 222/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9258 - acc: 0.6711 - val_loss: 1.5160 - val_acc: 0.4714\n",
      "Epoch 223/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9365 - acc: 0.6658 - val_loss: 1.3569 - val_acc: 0.5277\n",
      "Epoch 224/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9535 - acc: 0.6602 - val_loss: 1.2561 - val_acc: 0.5670\n",
      "Epoch 225/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9422 - acc: 0.6649 - val_loss: 1.2269 - val_acc: 0.5366\n",
      "Epoch 226/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9377 - acc: 0.6630 - val_loss: 1.3047 - val_acc: 0.5545\n",
      "Epoch 227/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9228 - acc: 0.6721 - val_loss: 1.7930 - val_acc: 0.4795\n",
      "Epoch 228/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9317 - acc: 0.6710 - val_loss: 1.5052 - val_acc: 0.5446\n",
      "Epoch 229/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9232 - acc: 0.6685 - val_loss: 1.3457 - val_acc: 0.5348\n",
      "Epoch 230/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9273 - acc: 0.6674 - val_loss: 1.1644 - val_acc: 0.6089\n",
      "Epoch 231/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9380 - acc: 0.6657 - val_loss: 1.5283 - val_acc: 0.5205\n",
      "Epoch 232/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9385 - acc: 0.6651 - val_loss: 2.3582 - val_acc: 0.3214\n",
      "Epoch 233/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9575 - acc: 0.6628 - val_loss: 1.4846 - val_acc: 0.5241\n",
      "Epoch 234/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9257 - acc: 0.6702 - val_loss: 1.4460 - val_acc: 0.5268\n",
      "Epoch 235/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9423 - acc: 0.6700 - val_loss: 1.1760 - val_acc: 0.5777\n",
      "Epoch 236/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9377 - acc: 0.6634 - val_loss: 1.4894 - val_acc: 0.4768\n",
      "Epoch 237/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9388 - acc: 0.6656 - val_loss: 1.3986 - val_acc: 0.5536\n",
      "Epoch 238/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9353 - acc: 0.6712 - val_loss: 1.1409 - val_acc: 0.6187\n",
      "Epoch 239/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9318 - acc: 0.6662 - val_loss: 1.8687 - val_acc: 0.4491\n",
      "Epoch 240/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9427 - acc: 0.6630 - val_loss: 1.6173 - val_acc: 0.4446\n",
      "Epoch 241/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9389 - acc: 0.6653 - val_loss: 1.4811 - val_acc: 0.5161\n",
      "Epoch 242/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9211 - acc: 0.6738 - val_loss: 1.8250 - val_acc: 0.4589\n",
      "Epoch 243/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9502 - acc: 0.6647 - val_loss: 1.5885 - val_acc: 0.5125\n",
      "Epoch 244/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9241 - acc: 0.6724 - val_loss: 1.2946 - val_acc: 0.5420\n",
      "Epoch 245/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.9192 - acc: 0.6729 - val_loss: 1.2328 - val_acc: 0.5384\n",
      "Epoch 246/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.9042 - acc: 0.6772 - val_loss: 1.4480 - val_acc: 0.5420\n",
      "Epoch 247/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9300 - acc: 0.6690 - val_loss: 1.2139 - val_acc: 0.5920\n",
      "Epoch 248/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9143 - acc: 0.6712 - val_loss: 2.1023 - val_acc: 0.3688\n",
      "Epoch 249/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9260 - acc: 0.6747 - val_loss: 1.5175 - val_acc: 0.5063\n",
      "Epoch 250/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9211 - acc: 0.6716 - val_loss: 1.1851 - val_acc: 0.5759\n",
      "Epoch 251/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9056 - acc: 0.6755 - val_loss: 1.3215 - val_acc: 0.5482\n",
      "Epoch 252/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9086 - acc: 0.6746 - val_loss: 1.6923 - val_acc: 0.4679\n",
      "Epoch 253/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9240 - acc: 0.6746 - val_loss: 1.6062 - val_acc: 0.5027\n",
      "Epoch 254/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9131 - acc: 0.6730 - val_loss: 1.2890 - val_acc: 0.5571\n",
      "Epoch 255/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9312 - acc: 0.6674 - val_loss: 1.3628 - val_acc: 0.5679\n",
      "Epoch 256/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8957 - acc: 0.6800 - val_loss: 1.3378 - val_acc: 0.5152\n",
      "Epoch 257/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.9237 - acc: 0.6753 - val_loss: 1.3056 - val_acc: 0.5741\n",
      "Epoch 258/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9006 - acc: 0.6800 - val_loss: 1.5335 - val_acc: 0.4991\n",
      "Epoch 259/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9152 - acc: 0.6749 - val_loss: 1.7231 - val_acc: 0.4500\n",
      "Epoch 260/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.9220 - acc: 0.6719 - val_loss: 1.6347 - val_acc: 0.4875\n",
      "Epoch 261/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9086 - acc: 0.6776 - val_loss: 1.3742 - val_acc: 0.5330\n",
      "Epoch 262/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9075 - acc: 0.6791 - val_loss: 1.3506 - val_acc: 0.5286\n",
      "Epoch 263/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9086 - acc: 0.6736 - val_loss: 1.7299 - val_acc: 0.4696\n",
      "Epoch 264/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9012 - acc: 0.6768 - val_loss: 1.6160 - val_acc: 0.5259\n",
      "Epoch 265/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9094 - acc: 0.6791 - val_loss: 1.3824 - val_acc: 0.5473\n",
      "Epoch 266/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9022 - acc: 0.6837 - val_loss: 1.7740 - val_acc: 0.5170\n",
      "Epoch 267/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9096 - acc: 0.6746 - val_loss: 1.6763 - val_acc: 0.4902\n",
      "Epoch 268/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8884 - acc: 0.6860 - val_loss: 1.2723 - val_acc: 0.5732\n",
      "Epoch 269/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9105 - acc: 0.6754 - val_loss: 1.4997 - val_acc: 0.5098\n",
      "Epoch 270/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.9007 - acc: 0.6829 - val_loss: 1.6999 - val_acc: 0.4616\n",
      "Epoch 271/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.9131 - acc: 0.6778 - val_loss: 1.4132 - val_acc: 0.5848\n",
      "Epoch 272/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9043 - acc: 0.6774 - val_loss: 1.3963 - val_acc: 0.5500\n",
      "Epoch 273/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8841 - acc: 0.6820 - val_loss: 1.5847 - val_acc: 0.5125\n",
      "Epoch 274/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8848 - acc: 0.6873 - val_loss: 1.6829 - val_acc: 0.5152\n",
      "Epoch 275/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8907 - acc: 0.6896 - val_loss: 2.1045 - val_acc: 0.4116\n",
      "Epoch 276/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.9133 - acc: 0.6814 - val_loss: 2.1284 - val_acc: 0.4232\n",
      "Epoch 277/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.9066 - acc: 0.6840 - val_loss: 1.3463 - val_acc: 0.5625\n",
      "Epoch 278/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8915 - acc: 0.6787 - val_loss: 1.3972 - val_acc: 0.5670\n",
      "Epoch 279/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8868 - acc: 0.6838 - val_loss: 1.3708 - val_acc: 0.5688\n",
      "Epoch 280/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8967 - acc: 0.6778 - val_loss: 1.3712 - val_acc: 0.5455\n",
      "Epoch 281/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8838 - acc: 0.6840 - val_loss: 1.3152 - val_acc: 0.5473\n",
      "Epoch 282/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8861 - acc: 0.6806 - val_loss: 1.4675 - val_acc: 0.5339\n",
      "Epoch 283/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8862 - acc: 0.6800 - val_loss: 1.5680 - val_acc: 0.5036\n",
      "Epoch 284/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8843 - acc: 0.6858 - val_loss: 1.4562 - val_acc: 0.5518\n",
      "Epoch 285/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8813 - acc: 0.6865 - val_loss: 1.3551 - val_acc: 0.5339\n",
      "Epoch 286/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.9073 - acc: 0.6808 - val_loss: 1.4726 - val_acc: 0.5196\n",
      "Epoch 287/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8884 - acc: 0.6853 - val_loss: 1.4707 - val_acc: 0.4911\n",
      "Epoch 288/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8766 - acc: 0.6846 - val_loss: 1.4522 - val_acc: 0.5804\n",
      "Epoch 289/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8871 - acc: 0.6874 - val_loss: 1.3689 - val_acc: 0.5643\n",
      "Epoch 290/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8849 - acc: 0.6827 - val_loss: 1.3464 - val_acc: 0.5411\n",
      "Epoch 291/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8776 - acc: 0.6852 - val_loss: 1.5604 - val_acc: 0.5437\n",
      "Epoch 292/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8900 - acc: 0.6812 - val_loss: 1.4306 - val_acc: 0.4839\n",
      "Epoch 293/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8830 - acc: 0.6834 - val_loss: 1.6563 - val_acc: 0.4313\n",
      "Epoch 294/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.8856 - acc: 0.6846 - val_loss: 1.4399 - val_acc: 0.5357\n",
      "Epoch 295/5000\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.8766 - acc: 0.6883 - val_loss: 1.3102 - val_acc: 0.5732\n",
      "Epoch 296/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8703 - acc: 0.6912 - val_loss: 1.6433 - val_acc: 0.4813\n",
      "Epoch 297/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8855 - acc: 0.6852 - val_loss: 1.3676 - val_acc: 0.5295\n",
      "Epoch 298/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8814 - acc: 0.6838 - val_loss: 1.3599 - val_acc: 0.5509\n",
      "Epoch 299/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8807 - acc: 0.6856 - val_loss: 2.0253 - val_acc: 0.4036\n",
      "Epoch 300/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8917 - acc: 0.6901 - val_loss: 1.1839 - val_acc: 0.5777\n",
      "Epoch 301/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8709 - acc: 0.6905 - val_loss: 1.2717 - val_acc: 0.6054\n",
      "Epoch 302/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8652 - acc: 0.6910 - val_loss: 1.2733 - val_acc: 0.5527\n",
      "Epoch 303/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8711 - acc: 0.6932 - val_loss: 1.3221 - val_acc: 0.5170\n",
      "Epoch 304/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8805 - acc: 0.6828 - val_loss: 1.6309 - val_acc: 0.4741\n",
      "Epoch 305/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8818 - acc: 0.6862 - val_loss: 1.2636 - val_acc: 0.5437\n",
      "Epoch 306/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8598 - acc: 0.6895 - val_loss: 1.5281 - val_acc: 0.5339\n",
      "Epoch 307/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.8638 - acc: 0.6923 - val_loss: 1.8183 - val_acc: 0.4750\n",
      "Epoch 308/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8769 - acc: 0.6894 - val_loss: 1.5367 - val_acc: 0.5205\n",
      "Epoch 309/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8667 - acc: 0.6922 - val_loss: 1.6235 - val_acc: 0.4830\n",
      "Epoch 310/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8757 - acc: 0.6907 - val_loss: 1.5284 - val_acc: 0.4929\n",
      "Epoch 311/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8727 - acc: 0.6892 - val_loss: 1.5595 - val_acc: 0.5196\n",
      "Epoch 312/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8780 - acc: 0.6902 - val_loss: 1.2009 - val_acc: 0.6054\n",
      "Epoch 313/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8550 - acc: 0.6948 - val_loss: 1.5607 - val_acc: 0.4955\n",
      "Epoch 314/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8571 - acc: 0.7009 - val_loss: 1.4756 - val_acc: 0.5187\n",
      "Epoch 315/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8786 - acc: 0.6871 - val_loss: 1.2387 - val_acc: 0.5857\n",
      "Epoch 316/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8636 - acc: 0.6898 - val_loss: 1.4941 - val_acc: 0.5286\n",
      "Epoch 317/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8677 - acc: 0.6914 - val_loss: 1.7257 - val_acc: 0.4670\n",
      "Epoch 318/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8694 - acc: 0.6893 - val_loss: 1.6146 - val_acc: 0.4812\n",
      "Epoch 319/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.8675 - acc: 0.6876 - val_loss: 1.3505 - val_acc: 0.5643\n",
      "Epoch 320/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.8804 - acc: 0.6885 - val_loss: 1.4512 - val_acc: 0.5536\n",
      "Epoch 321/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8615 - acc: 0.6888 - val_loss: 1.2914 - val_acc: 0.5571\n",
      "Epoch 322/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8563 - acc: 0.6922 - val_loss: 1.2968 - val_acc: 0.6071\n",
      "Epoch 323/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8538 - acc: 0.6963 - val_loss: 1.5654 - val_acc: 0.4759\n",
      "Epoch 324/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8612 - acc: 0.6910 - val_loss: 1.3034 - val_acc: 0.5420\n",
      "Epoch 325/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8555 - acc: 0.6910 - val_loss: 1.4298 - val_acc: 0.5437\n",
      "Epoch 326/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8598 - acc: 0.6908 - val_loss: 1.3463 - val_acc: 0.5536\n",
      "Epoch 327/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8525 - acc: 0.6951 - val_loss: 1.2015 - val_acc: 0.6080\n",
      "Epoch 328/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8517 - acc: 0.6970 - val_loss: 1.3809 - val_acc: 0.5187\n",
      "Epoch 329/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8650 - acc: 0.6884 - val_loss: 1.5861 - val_acc: 0.5000\n",
      "Epoch 330/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8562 - acc: 0.6958 - val_loss: 1.4281 - val_acc: 0.5455\n",
      "Epoch 331/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8564 - acc: 0.7003 - val_loss: 1.4822 - val_acc: 0.5491\n",
      "Epoch 332/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 0.8560 - acc: 0.6980 - val_loss: 1.3634 - val_acc: 0.5562\n",
      "Epoch 333/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8403 - acc: 0.6981 - val_loss: 1.4033 - val_acc: 0.5429\n",
      "Epoch 334/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8622 - acc: 0.6893 - val_loss: 1.1891 - val_acc: 0.6125\n",
      "Epoch 335/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8507 - acc: 0.6969 - val_loss: 1.8046 - val_acc: 0.4929\n",
      "Epoch 336/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8655 - acc: 0.6956 - val_loss: 1.3771 - val_acc: 0.5384\n",
      "Epoch 337/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8567 - acc: 0.6927 - val_loss: 1.7602 - val_acc: 0.5062\n",
      "Epoch 338/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8415 - acc: 0.7014 - val_loss: 1.2178 - val_acc: 0.6063\n",
      "Epoch 339/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.8594 - acc: 0.6890 - val_loss: 2.0008 - val_acc: 0.4312\n",
      "Epoch 340/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8535 - acc: 0.6970 - val_loss: 1.7921 - val_acc: 0.4679\n",
      "Epoch 341/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8673 - acc: 0.6934 - val_loss: 1.3631 - val_acc: 0.5750\n",
      "Epoch 342/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.8441 - acc: 0.6966 - val_loss: 1.4086 - val_acc: 0.5161\n",
      "Epoch 343/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.8452 - acc: 0.6947 - val_loss: 1.1997 - val_acc: 0.5929\n",
      "Epoch 344/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8404 - acc: 0.6986 - val_loss: 1.4279 - val_acc: 0.5661\n",
      "Epoch 345/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8573 - acc: 0.6938 - val_loss: 1.2587 - val_acc: 0.6223\n",
      "Epoch 346/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8374 - acc: 0.7018 - val_loss: 1.6012 - val_acc: 0.4732\n",
      "Epoch 347/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8493 - acc: 0.6998 - val_loss: 1.3221 - val_acc: 0.5527\n",
      "Epoch 348/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8344 - acc: 0.7010 - val_loss: 1.1494 - val_acc: 0.5750\n",
      "Epoch 349/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8371 - acc: 0.7029 - val_loss: 1.3476 - val_acc: 0.5571\n",
      "Epoch 350/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8364 - acc: 0.7058 - val_loss: 1.5433 - val_acc: 0.5375\n",
      "Epoch 351/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8457 - acc: 0.7040 - val_loss: 1.1323 - val_acc: 0.6063\n",
      "Epoch 352/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8334 - acc: 0.7010 - val_loss: 1.4829 - val_acc: 0.5116\n",
      "Epoch 353/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8382 - acc: 0.6980 - val_loss: 1.4124 - val_acc: 0.5464\n",
      "Epoch 354/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8392 - acc: 0.7008 - val_loss: 1.4378 - val_acc: 0.5134\n",
      "Epoch 355/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8512 - acc: 0.6958 - val_loss: 1.8038 - val_acc: 0.4500\n",
      "Epoch 356/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8336 - acc: 0.7055 - val_loss: 1.5852 - val_acc: 0.4982\n",
      "Epoch 357/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8590 - acc: 0.6957 - val_loss: 1.3155 - val_acc: 0.5830\n",
      "Epoch 358/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8360 - acc: 0.7026 - val_loss: 1.4963 - val_acc: 0.5205\n",
      "Epoch 359/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8399 - acc: 0.7010 - val_loss: 1.5182 - val_acc: 0.5286\n",
      "Epoch 360/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8403 - acc: 0.6960 - val_loss: 1.9550 - val_acc: 0.4562\n",
      "Epoch 361/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8557 - acc: 0.7001 - val_loss: 1.3931 - val_acc: 0.5589\n",
      "Epoch 362/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8411 - acc: 0.7016 - val_loss: 1.9317 - val_acc: 0.5259\n",
      "Epoch 363/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8382 - acc: 0.7079 - val_loss: 1.3478 - val_acc: 0.5580\n",
      "Epoch 364/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8343 - acc: 0.7023 - val_loss: 1.3416 - val_acc: 0.5250\n",
      "Epoch 365/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8343 - acc: 0.7043 - val_loss: 1.4010 - val_acc: 0.5670\n",
      "Epoch 366/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8345 - acc: 0.6988 - val_loss: 1.2811 - val_acc: 0.5795\n",
      "Epoch 367/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8221 - acc: 0.7064 - val_loss: 1.7520 - val_acc: 0.4375\n",
      "Epoch 368/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.8430 - acc: 0.7020 - val_loss: 1.1662 - val_acc: 0.6152\n",
      "Epoch 369/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.8213 - acc: 0.7072 - val_loss: 1.3239 - val_acc: 0.5634\n",
      "Epoch 370/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8310 - acc: 0.7053 - val_loss: 1.6060 - val_acc: 0.5268\n",
      "Epoch 371/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8341 - acc: 0.7126 - val_loss: 1.1757 - val_acc: 0.5509\n",
      "Epoch 372/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8337 - acc: 0.7010 - val_loss: 1.4409 - val_acc: 0.5509\n",
      "Epoch 373/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8477 - acc: 0.6987 - val_loss: 1.3616 - val_acc: 0.5464\n",
      "Epoch 374/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8247 - acc: 0.7022 - val_loss: 1.9604 - val_acc: 0.4634\n",
      "Epoch 375/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8356 - acc: 0.7072 - val_loss: 1.4481 - val_acc: 0.5696\n",
      "Epoch 376/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8210 - acc: 0.7107 - val_loss: 1.4864 - val_acc: 0.5446\n",
      "Epoch 377/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8315 - acc: 0.7028 - val_loss: 1.2281 - val_acc: 0.5821\n",
      "Epoch 378/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8217 - acc: 0.7073 - val_loss: 1.1873 - val_acc: 0.6080\n",
      "Epoch 379/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8204 - acc: 0.7092 - val_loss: 1.3811 - val_acc: 0.5607\n",
      "Epoch 380/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8161 - acc: 0.7084 - val_loss: 1.2666 - val_acc: 0.5759\n",
      "Epoch 381/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8303 - acc: 0.7051 - val_loss: 1.4582 - val_acc: 0.5339\n",
      "Epoch 382/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8142 - acc: 0.7134 - val_loss: 1.1901 - val_acc: 0.6232\n",
      "Epoch 383/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8245 - acc: 0.7058 - val_loss: 1.5281 - val_acc: 0.5232\n",
      "Epoch 384/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8249 - acc: 0.7080 - val_loss: 1.5017 - val_acc: 0.5241\n",
      "Epoch 385/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8111 - acc: 0.7106 - val_loss: 1.2499 - val_acc: 0.5768\n",
      "Epoch 386/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8151 - acc: 0.7101 - val_loss: 1.3937 - val_acc: 0.5357\n",
      "Epoch 387/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8262 - acc: 0.7058 - val_loss: 1.5400 - val_acc: 0.4786\n",
      "Epoch 388/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8322 - acc: 0.7022 - val_loss: 1.3414 - val_acc: 0.5563\n",
      "Epoch 389/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.8116 - acc: 0.7100 - val_loss: 1.4967 - val_acc: 0.4946\n",
      "Epoch 390/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.8261 - acc: 0.7032 - val_loss: 1.3574 - val_acc: 0.5295\n",
      "Epoch 391/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8106 - acc: 0.7084 - val_loss: 1.4993 - val_acc: 0.5705\n",
      "Epoch 392/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8166 - acc: 0.7113 - val_loss: 1.5289 - val_acc: 0.5420\n",
      "Epoch 393/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.8212 - acc: 0.7083 - val_loss: 1.8258 - val_acc: 0.4830\n",
      "Epoch 394/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.8277 - acc: 0.7087 - val_loss: 1.6669 - val_acc: 0.4643\n",
      "Epoch 395/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8289 - acc: 0.7054 - val_loss: 1.4166 - val_acc: 0.5411\n",
      "Epoch 396/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8131 - acc: 0.7100 - val_loss: 1.5184 - val_acc: 0.5152\n",
      "Epoch 397/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8387 - acc: 0.7031 - val_loss: 1.3596 - val_acc: 0.5741\n",
      "Epoch 398/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8179 - acc: 0.7076 - val_loss: 1.3749 - val_acc: 0.5063\n",
      "Epoch 399/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.8132 - acc: 0.7100 - val_loss: 1.2117 - val_acc: 0.5518\n",
      "Epoch 400/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8210 - acc: 0.7074 - val_loss: 1.3803 - val_acc: 0.5973\n",
      "Epoch 401/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8028 - acc: 0.7136 - val_loss: 1.6129 - val_acc: 0.5420\n",
      "Epoch 402/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8206 - acc: 0.7087 - val_loss: 1.1834 - val_acc: 0.6161\n",
      "Epoch 403/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8023 - acc: 0.7126 - val_loss: 1.4403 - val_acc: 0.5402\n",
      "Epoch 404/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8041 - acc: 0.7138 - val_loss: 1.2487 - val_acc: 0.5964\n",
      "Epoch 405/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8086 - acc: 0.7122 - val_loss: 1.1309 - val_acc: 0.5938\n",
      "Epoch 406/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8039 - acc: 0.7156 - val_loss: 1.2690 - val_acc: 0.5670\n",
      "Epoch 407/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8148 - acc: 0.7111 - val_loss: 1.3829 - val_acc: 0.5634\n",
      "Epoch 408/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8174 - acc: 0.7086 - val_loss: 1.1807 - val_acc: 0.6375\n",
      "Epoch 409/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7911 - acc: 0.7183 - val_loss: 1.5290 - val_acc: 0.5286\n",
      "Epoch 410/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7992 - acc: 0.7162 - val_loss: 1.3519 - val_acc: 0.5723\n",
      "Epoch 411/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8050 - acc: 0.7129 - val_loss: 1.6025 - val_acc: 0.5107\n",
      "Epoch 412/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8115 - acc: 0.7133 - val_loss: 1.7170 - val_acc: 0.4991\n",
      "Epoch 413/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8106 - acc: 0.7135 - val_loss: 1.5382 - val_acc: 0.5518\n",
      "Epoch 414/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8197 - acc: 0.7090 - val_loss: 1.5342 - val_acc: 0.5723\n",
      "Epoch 415/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7953 - acc: 0.7172 - val_loss: 1.4374 - val_acc: 0.5625\n",
      "Epoch 416/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8088 - acc: 0.7075 - val_loss: 1.2923 - val_acc: 0.5955\n",
      "Epoch 417/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.8152 - acc: 0.7086 - val_loss: 1.2868 - val_acc: 0.5580\n",
      "Epoch 418/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.8096 - acc: 0.7104 - val_loss: 1.3178 - val_acc: 0.5384\n",
      "Epoch 419/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.8091 - acc: 0.7124 - val_loss: 1.3744 - val_acc: 0.5705\n",
      "Epoch 420/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8064 - acc: 0.7149 - val_loss: 1.1395 - val_acc: 0.6214\n",
      "Epoch 421/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7922 - acc: 0.7160 - val_loss: 1.4480 - val_acc: 0.5268\n",
      "Epoch 422/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8094 - acc: 0.7095 - val_loss: 1.3378 - val_acc: 0.6125\n",
      "Epoch 423/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8189 - acc: 0.7097 - val_loss: 1.2555 - val_acc: 0.6000\n",
      "Epoch 424/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7920 - acc: 0.7189 - val_loss: 1.2209 - val_acc: 0.6027\n",
      "Epoch 425/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7866 - acc: 0.7191 - val_loss: 1.1299 - val_acc: 0.6375\n",
      "Epoch 426/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7983 - acc: 0.7088 - val_loss: 1.3040 - val_acc: 0.5313\n",
      "Epoch 427/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8016 - acc: 0.7165 - val_loss: 1.3458 - val_acc: 0.5786\n",
      "Epoch 428/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8055 - acc: 0.7127 - val_loss: 1.2553 - val_acc: 0.5563\n",
      "Epoch 429/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7851 - acc: 0.7184 - val_loss: 1.5194 - val_acc: 0.5321\n",
      "Epoch 430/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.8150 - acc: 0.7127 - val_loss: 1.5123 - val_acc: 0.4911\n",
      "Epoch 431/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7953 - acc: 0.7170 - val_loss: 1.3127 - val_acc: 0.5652\n",
      "Epoch 432/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7963 - acc: 0.7158 - val_loss: 1.3280 - val_acc: 0.5518\n",
      "Epoch 433/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.8077 - acc: 0.7167 - val_loss: 1.8963 - val_acc: 0.4625\n",
      "Epoch 434/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7977 - acc: 0.7159 - val_loss: 1.3705 - val_acc: 0.5714\n",
      "Epoch 435/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7904 - acc: 0.7170 - val_loss: 1.8022 - val_acc: 0.4732\n",
      "Epoch 436/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8190 - acc: 0.7132 - val_loss: 1.3927 - val_acc: 0.5839\n",
      "Epoch 437/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7864 - acc: 0.7211 - val_loss: 1.7072 - val_acc: 0.4911\n",
      "Epoch 438/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8075 - acc: 0.7146 - val_loss: 1.9606 - val_acc: 0.4598\n",
      "Epoch 439/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.8001 - acc: 0.7186 - val_loss: 1.1936 - val_acc: 0.5795\n",
      "Epoch 440/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7878 - acc: 0.7211 - val_loss: 1.2527 - val_acc: 0.6062\n",
      "Epoch 441/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7899 - acc: 0.7136 - val_loss: 1.2267 - val_acc: 0.5893\n",
      "Epoch 442/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.7913 - acc: 0.7175 - val_loss: 1.1777 - val_acc: 0.5964\n",
      "Epoch 443/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.7847 - acc: 0.7168 - val_loss: 1.2030 - val_acc: 0.5732\n",
      "Epoch 444/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7981 - acc: 0.7175 - val_loss: 1.2984 - val_acc: 0.5696\n",
      "Epoch 445/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7938 - acc: 0.7140 - val_loss: 1.2001 - val_acc: 0.6179\n",
      "Epoch 446/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7815 - acc: 0.7209 - val_loss: 1.1885 - val_acc: 0.5741\n",
      "Epoch 447/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7832 - acc: 0.7217 - val_loss: 1.2931 - val_acc: 0.5750\n",
      "Epoch 448/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7787 - acc: 0.7187 - val_loss: 1.8463 - val_acc: 0.4714\n",
      "Epoch 449/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7861 - acc: 0.7203 - val_loss: 1.3564 - val_acc: 0.5696\n",
      "Epoch 450/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7973 - acc: 0.7146 - val_loss: 1.4587 - val_acc: 0.5857\n",
      "Epoch 451/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7896 - acc: 0.7148 - val_loss: 1.2665 - val_acc: 0.5714\n",
      "Epoch 452/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7711 - acc: 0.7249 - val_loss: 1.2942 - val_acc: 0.5920\n",
      "Epoch 453/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7844 - acc: 0.7187 - val_loss: 1.2899 - val_acc: 0.6107\n",
      "Epoch 454/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.7690 - acc: 0.7261 - val_loss: 1.1461 - val_acc: 0.6179\n",
      "Epoch 455/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.7814 - acc: 0.7209 - val_loss: 1.6549 - val_acc: 0.5429\n",
      "Epoch 456/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7982 - acc: 0.7233 - val_loss: 1.2780 - val_acc: 0.6330\n",
      "Epoch 457/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7771 - acc: 0.7218 - val_loss: 1.3064 - val_acc: 0.5607\n",
      "Epoch 458/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7638 - acc: 0.7297 - val_loss: 1.2017 - val_acc: 0.6205\n",
      "Epoch 459/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7880 - acc: 0.7197 - val_loss: 1.3600 - val_acc: 0.6062\n",
      "Epoch 460/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7816 - acc: 0.7233 - val_loss: 1.1897 - val_acc: 0.5750\n",
      "Epoch 461/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7734 - acc: 0.7209 - val_loss: 1.2167 - val_acc: 0.5750\n",
      "Epoch 462/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7932 - acc: 0.7139 - val_loss: 1.3017 - val_acc: 0.5884\n",
      "Epoch 463/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7797 - acc: 0.7252 - val_loss: 1.9332 - val_acc: 0.4750\n",
      "Epoch 464/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7920 - acc: 0.7178 - val_loss: 1.1341 - val_acc: 0.6268\n",
      "Epoch 465/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7824 - acc: 0.7186 - val_loss: 1.3507 - val_acc: 0.6214\n",
      "Epoch 466/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7655 - acc: 0.7263 - val_loss: 1.6956 - val_acc: 0.5161\n",
      "Epoch 467/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7812 - acc: 0.7282 - val_loss: 1.3685 - val_acc: 0.5866\n",
      "Epoch 468/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7736 - acc: 0.7246 - val_loss: 1.4021 - val_acc: 0.5518\n",
      "Epoch 469/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7823 - acc: 0.7236 - val_loss: 1.3178 - val_acc: 0.5750\n",
      "Epoch 470/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7822 - acc: 0.7225 - val_loss: 1.5462 - val_acc: 0.5375\n",
      "Epoch 471/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7778 - acc: 0.7223 - val_loss: 1.4470 - val_acc: 0.5250\n",
      "Epoch 472/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7762 - acc: 0.7244 - val_loss: 1.3355 - val_acc: 0.5536\n",
      "Epoch 473/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7754 - acc: 0.7215 - val_loss: 1.3068 - val_acc: 0.5777\n",
      "Epoch 474/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7755 - acc: 0.7193 - val_loss: 1.1240 - val_acc: 0.6420\n",
      "Epoch 475/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7863 - acc: 0.7213 - val_loss: 1.5564 - val_acc: 0.5187\n",
      "Epoch 476/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7764 - acc: 0.7241 - val_loss: 1.3976 - val_acc: 0.6152\n",
      "Epoch 477/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7612 - acc: 0.7295 - val_loss: 1.3848 - val_acc: 0.5964\n",
      "Epoch 478/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7728 - acc: 0.7298 - val_loss: 1.1419 - val_acc: 0.6062\n",
      "Epoch 479/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.7682 - acc: 0.7238 - val_loss: 1.2561 - val_acc: 0.6250\n",
      "Epoch 480/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.7758 - acc: 0.7233 - val_loss: 1.0938 - val_acc: 0.5973\n",
      "Epoch 481/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7637 - acc: 0.7241 - val_loss: 1.2699 - val_acc: 0.5616\n",
      "Epoch 482/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7619 - acc: 0.7287 - val_loss: 1.3756 - val_acc: 0.5670\n",
      "Epoch 483/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7795 - acc: 0.7250 - val_loss: 1.7535 - val_acc: 0.4714\n",
      "Epoch 484/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7990 - acc: 0.7172 - val_loss: 1.5806 - val_acc: 0.5634\n",
      "Epoch 485/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7741 - acc: 0.7223 - val_loss: 1.4903 - val_acc: 0.5768\n",
      "Epoch 486/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7734 - acc: 0.7266 - val_loss: 1.5740 - val_acc: 0.5750\n",
      "Epoch 487/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7709 - acc: 0.7253 - val_loss: 1.1700 - val_acc: 0.6098\n",
      "Epoch 488/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7655 - acc: 0.7258 - val_loss: 2.0129 - val_acc: 0.4384\n",
      "Epoch 489/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7808 - acc: 0.7250 - val_loss: 1.2399 - val_acc: 0.5964\n",
      "Epoch 490/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7525 - acc: 0.7329 - val_loss: 1.2581 - val_acc: 0.5821\n",
      "Epoch 491/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7669 - acc: 0.7275 - val_loss: 1.1132 - val_acc: 0.6304\n",
      "Epoch 492/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.7614 - acc: 0.7264 - val_loss: 1.8124 - val_acc: 0.4830\n",
      "Epoch 493/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7746 - acc: 0.7277 - val_loss: 1.5118 - val_acc: 0.5518\n",
      "Epoch 494/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7590 - acc: 0.7313 - val_loss: 1.1197 - val_acc: 0.6196\n",
      "Epoch 495/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7594 - acc: 0.7317 - val_loss: 1.7546 - val_acc: 0.5054\n",
      "Epoch 496/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7791 - acc: 0.7221 - val_loss: 1.3399 - val_acc: 0.5571\n",
      "Epoch 497/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7654 - acc: 0.7265 - val_loss: 1.5106 - val_acc: 0.5330\n",
      "Epoch 498/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7787 - acc: 0.7243 - val_loss: 1.4723 - val_acc: 0.5696\n",
      "Epoch 499/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7716 - acc: 0.7226 - val_loss: 1.3352 - val_acc: 0.5777\n",
      "Epoch 500/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7552 - acc: 0.7285 - val_loss: 1.2490 - val_acc: 0.5902\n",
      "Epoch 501/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7516 - acc: 0.7311 - val_loss: 1.3560 - val_acc: 0.5750\n",
      "Epoch 502/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7547 - acc: 0.7321 - val_loss: 1.1275 - val_acc: 0.6580\n",
      "Epoch 503/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7468 - acc: 0.7328 - val_loss: 1.3575 - val_acc: 0.5679\n",
      "Epoch 504/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7579 - acc: 0.7306 - val_loss: 1.1867 - val_acc: 0.6045\n",
      "Epoch 505/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7867 - acc: 0.7224 - val_loss: 1.3868 - val_acc: 0.5795\n",
      "Epoch 506/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.7534 - acc: 0.7261 - val_loss: 1.3062 - val_acc: 0.5929\n",
      "Epoch 507/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7703 - acc: 0.7275 - val_loss: 1.3570 - val_acc: 0.6000\n",
      "Epoch 508/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7659 - acc: 0.7247 - val_loss: 1.8729 - val_acc: 0.5009\n",
      "Epoch 509/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7566 - acc: 0.7322 - val_loss: 1.3590 - val_acc: 0.5768\n",
      "Epoch 510/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7472 - acc: 0.7347 - val_loss: 1.2426 - val_acc: 0.5830\n",
      "Epoch 511/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7554 - acc: 0.7276 - val_loss: 1.5206 - val_acc: 0.5500\n",
      "Epoch 512/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7776 - acc: 0.7244 - val_loss: 1.1766 - val_acc: 0.6179\n",
      "Epoch 513/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7432 - acc: 0.7324 - val_loss: 1.2237 - val_acc: 0.5848\n",
      "Epoch 514/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7547 - acc: 0.7326 - val_loss: 1.2377 - val_acc: 0.5732\n",
      "Epoch 515/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7687 - acc: 0.7243 - val_loss: 1.2773 - val_acc: 0.6045\n",
      "Epoch 516/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7525 - acc: 0.7291 - val_loss: 1.5144 - val_acc: 0.5304\n",
      "Epoch 517/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.7609 - acc: 0.7275 - val_loss: 1.3065 - val_acc: 0.5991\n",
      "Epoch 518/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.7598 - acc: 0.7265 - val_loss: 1.6060 - val_acc: 0.4884\n",
      "Epoch 519/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7572 - acc: 0.7317 - val_loss: 1.2558 - val_acc: 0.5625\n",
      "Epoch 520/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7622 - acc: 0.7223 - val_loss: 1.2149 - val_acc: 0.5741\n",
      "Epoch 521/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7418 - acc: 0.7350 - val_loss: 1.5508 - val_acc: 0.5321\n",
      "Epoch 522/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7582 - acc: 0.7323 - val_loss: 1.1243 - val_acc: 0.6482\n",
      "Epoch 523/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7585 - acc: 0.7283 - val_loss: 1.2974 - val_acc: 0.5884\n",
      "Epoch 524/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7493 - acc: 0.7342 - val_loss: 1.3941 - val_acc: 0.5857\n",
      "Epoch 525/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7529 - acc: 0.7309 - val_loss: 1.5632 - val_acc: 0.5089\n",
      "Epoch 526/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7577 - acc: 0.7320 - val_loss: 1.9953 - val_acc: 0.4964\n",
      "Epoch 527/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7804 - acc: 0.7268 - val_loss: 1.3120 - val_acc: 0.5821\n",
      "Epoch 528/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7355 - acc: 0.7371 - val_loss: 1.1424 - val_acc: 0.6321\n",
      "Epoch 529/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7346 - acc: 0.7375 - val_loss: 1.2265 - val_acc: 0.5973\n",
      "Epoch 530/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.7500 - acc: 0.7321 - val_loss: 1.3343 - val_acc: 0.6009\n",
      "Epoch 531/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7556 - acc: 0.7331 - val_loss: 1.4132 - val_acc: 0.5857\n",
      "Epoch 532/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7505 - acc: 0.7353 - val_loss: 1.0357 - val_acc: 0.6580\n",
      "Epoch 533/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7393 - acc: 0.7363 - val_loss: 1.2535 - val_acc: 0.6277\n",
      "Epoch 534/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7380 - acc: 0.7347 - val_loss: 1.1664 - val_acc: 0.5786\n",
      "Epoch 535/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7498 - acc: 0.7279 - val_loss: 1.8874 - val_acc: 0.4580\n",
      "Epoch 536/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7848 - acc: 0.7293 - val_loss: 1.5422 - val_acc: 0.5071\n",
      "Epoch 537/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7359 - acc: 0.7355 - val_loss: 1.4294 - val_acc: 0.5554\n",
      "Epoch 538/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7482 - acc: 0.7335 - val_loss: 1.3234 - val_acc: 0.5795\n",
      "Epoch 539/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7417 - acc: 0.7352 - val_loss: 1.1593 - val_acc: 0.6161\n",
      "Epoch 540/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7451 - acc: 0.7309 - val_loss: 1.2646 - val_acc: 0.5768\n",
      "Epoch 541/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7293 - acc: 0.7354 - val_loss: 1.5230 - val_acc: 0.5134\n",
      "Epoch 542/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7450 - acc: 0.7340 - val_loss: 1.2136 - val_acc: 0.6009\n",
      "Epoch 543/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7427 - acc: 0.7340 - val_loss: 1.4146 - val_acc: 0.5634\n",
      "Epoch 544/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7339 - acc: 0.7366 - val_loss: 1.3824 - val_acc: 0.5562\n",
      "Epoch 545/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7381 - acc: 0.7370 - val_loss: 1.2295 - val_acc: 0.6375\n",
      "Epoch 546/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7370 - acc: 0.7381 - val_loss: 1.2380 - val_acc: 0.6098\n",
      "Epoch 547/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7237 - acc: 0.7393 - val_loss: 1.3447 - val_acc: 0.5598\n",
      "Epoch 548/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7369 - acc: 0.7338 - val_loss: 1.2568 - val_acc: 0.5911\n",
      "Epoch 549/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7364 - acc: 0.7392 - val_loss: 1.1458 - val_acc: 0.6170\n",
      "Epoch 550/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7250 - acc: 0.7400 - val_loss: 1.3459 - val_acc: 0.5384\n",
      "Epoch 551/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7524 - acc: 0.7332 - val_loss: 1.0939 - val_acc: 0.6161\n",
      "Epoch 552/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7241 - acc: 0.7414 - val_loss: 1.5681 - val_acc: 0.5232\n",
      "Epoch 553/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7539 - acc: 0.7287 - val_loss: 1.3338 - val_acc: 0.5723\n",
      "Epoch 554/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7422 - acc: 0.7337 - val_loss: 1.3093 - val_acc: 0.6143\n",
      "Epoch 555/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7296 - acc: 0.7421 - val_loss: 1.6822 - val_acc: 0.5232\n",
      "Epoch 556/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7397 - acc: 0.7371 - val_loss: 1.5602 - val_acc: 0.4991\n",
      "Epoch 557/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7443 - acc: 0.7321 - val_loss: 1.4042 - val_acc: 0.5509\n",
      "Epoch 558/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7399 - acc: 0.7335 - val_loss: 1.6134 - val_acc: 0.5179\n",
      "Epoch 559/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7433 - acc: 0.7351 - val_loss: 1.6048 - val_acc: 0.4848\n",
      "Epoch 560/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7418 - acc: 0.7344 - val_loss: 1.3163 - val_acc: 0.5786\n",
      "Epoch 561/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7580 - acc: 0.7367 - val_loss: 1.4295 - val_acc: 0.5821\n",
      "Epoch 562/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7355 - acc: 0.7383 - val_loss: 1.5084 - val_acc: 0.5616\n",
      "Epoch 563/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7398 - acc: 0.7359 - val_loss: 1.3882 - val_acc: 0.5518\n",
      "Epoch 564/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7454 - acc: 0.7364 - val_loss: 1.0946 - val_acc: 0.6161\n",
      "Epoch 565/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7223 - acc: 0.7374 - val_loss: 1.0974 - val_acc: 0.6473\n",
      "Epoch 566/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7288 - acc: 0.7363 - val_loss: 1.2046 - val_acc: 0.6045\n",
      "Epoch 567/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7182 - acc: 0.7431 - val_loss: 1.2956 - val_acc: 0.6214\n",
      "Epoch 568/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7214 - acc: 0.7403 - val_loss: 1.1814 - val_acc: 0.6402\n",
      "Epoch 569/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7470 - acc: 0.7333 - val_loss: 1.4130 - val_acc: 0.5616\n",
      "Epoch 570/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7444 - acc: 0.7416 - val_loss: 1.3853 - val_acc: 0.5679\n",
      "Epoch 571/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7307 - acc: 0.7362 - val_loss: 1.4559 - val_acc: 0.4955\n",
      "Epoch 572/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7368 - acc: 0.7381 - val_loss: 1.1541 - val_acc: 0.6304\n",
      "Epoch 573/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7346 - acc: 0.7415 - val_loss: 1.2248 - val_acc: 0.6223\n",
      "Epoch 574/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7150 - acc: 0.7441 - val_loss: 1.4862 - val_acc: 0.5616\n",
      "Epoch 575/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7323 - acc: 0.7417 - val_loss: 1.4768 - val_acc: 0.5179\n",
      "Epoch 576/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7475 - acc: 0.7322 - val_loss: 1.3035 - val_acc: 0.6080\n",
      "Epoch 577/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7412 - acc: 0.7365 - val_loss: 1.5595 - val_acc: 0.5393\n",
      "Epoch 578/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7331 - acc: 0.7366 - val_loss: 1.3167 - val_acc: 0.6000\n",
      "Epoch 579/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.7361 - acc: 0.7341 - val_loss: 1.9248 - val_acc: 0.4607\n",
      "Epoch 580/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7439 - acc: 0.7328 - val_loss: 1.4264 - val_acc: 0.5688\n",
      "Epoch 581/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7328 - acc: 0.7363 - val_loss: 1.2829 - val_acc: 0.5571\n",
      "Epoch 582/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7254 - acc: 0.7400 - val_loss: 1.3110 - val_acc: 0.5705\n",
      "Epoch 583/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7407 - acc: 0.7370 - val_loss: 1.4561 - val_acc: 0.5652\n",
      "Epoch 584/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7269 - acc: 0.7425 - val_loss: 1.3013 - val_acc: 0.5723\n",
      "Epoch 585/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7261 - acc: 0.7394 - val_loss: 1.0935 - val_acc: 0.6232\n",
      "Epoch 586/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7248 - acc: 0.7399 - val_loss: 1.4006 - val_acc: 0.5679\n",
      "Epoch 587/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7156 - acc: 0.7435 - val_loss: 1.2272 - val_acc: 0.6205\n",
      "Epoch 588/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7202 - acc: 0.7403 - val_loss: 1.2978 - val_acc: 0.6125\n",
      "Epoch 589/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7340 - acc: 0.7372 - val_loss: 1.2264 - val_acc: 0.5705\n",
      "Epoch 590/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7132 - acc: 0.7463 - val_loss: 1.5195 - val_acc: 0.5259\n",
      "Epoch 591/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7327 - acc: 0.7408 - val_loss: 1.2011 - val_acc: 0.5839\n",
      "Epoch 592/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7378 - acc: 0.7344 - val_loss: 1.3552 - val_acc: 0.5938\n",
      "Epoch 593/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7322 - acc: 0.7407 - val_loss: 1.1921 - val_acc: 0.6116\n",
      "Epoch 594/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7263 - acc: 0.7395 - val_loss: 1.1758 - val_acc: 0.6312\n",
      "Epoch 595/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7326 - acc: 0.7411 - val_loss: 1.1493 - val_acc: 0.6268\n",
      "Epoch 596/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7166 - acc: 0.7437 - val_loss: 1.2969 - val_acc: 0.6277\n",
      "Epoch 597/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7106 - acc: 0.7459 - val_loss: 1.1574 - val_acc: 0.6232\n",
      "Epoch 598/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7189 - acc: 0.7427 - val_loss: 1.5928 - val_acc: 0.5455\n",
      "Epoch 599/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7412 - acc: 0.7421 - val_loss: 1.4121 - val_acc: 0.5420\n",
      "Epoch 600/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7358 - acc: 0.7383 - val_loss: 1.3176 - val_acc: 0.6000\n",
      "Epoch 601/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7344 - acc: 0.7355 - val_loss: 1.2507 - val_acc: 0.6107\n",
      "Epoch 602/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7009 - acc: 0.7503 - val_loss: 1.7142 - val_acc: 0.5054\n",
      "Epoch 603/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7331 - acc: 0.7436 - val_loss: 1.4100 - val_acc: 0.5759\n",
      "Epoch 604/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.7173 - acc: 0.7423 - val_loss: 1.2990 - val_acc: 0.5812\n",
      "Epoch 605/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.7280 - acc: 0.7421 - val_loss: 1.5367 - val_acc: 0.5214\n",
      "Epoch 606/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7227 - acc: 0.7403 - val_loss: 1.1941 - val_acc: 0.6134\n",
      "Epoch 607/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7204 - acc: 0.7446 - val_loss: 1.5366 - val_acc: 0.5616\n",
      "Epoch 608/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7200 - acc: 0.7458 - val_loss: 1.8326 - val_acc: 0.5036\n",
      "Epoch 609/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7334 - acc: 0.7422 - val_loss: 1.1249 - val_acc: 0.6723\n",
      "Epoch 610/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7122 - acc: 0.7463 - val_loss: 1.5425 - val_acc: 0.5795\n",
      "Epoch 611/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7156 - acc: 0.7417 - val_loss: 1.3096 - val_acc: 0.6179\n",
      "Epoch 612/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6964 - acc: 0.7497 - val_loss: 1.5522 - val_acc: 0.5179\n",
      "Epoch 613/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7261 - acc: 0.7441 - val_loss: 1.2418 - val_acc: 0.6455\n",
      "Epoch 614/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7036 - acc: 0.7499 - val_loss: 1.2331 - val_acc: 0.5830\n",
      "Epoch 615/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7306 - acc: 0.7395 - val_loss: 1.4383 - val_acc: 0.5304\n",
      "Epoch 616/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7049 - acc: 0.7473 - val_loss: 1.2427 - val_acc: 0.5911\n",
      "Epoch 617/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.7154 - acc: 0.7437 - val_loss: 1.4156 - val_acc: 0.6179\n",
      "Epoch 618/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7153 - acc: 0.7470 - val_loss: 1.8003 - val_acc: 0.4705\n",
      "Epoch 619/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7273 - acc: 0.7452 - val_loss: 1.1145 - val_acc: 0.6571\n",
      "Epoch 620/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7026 - acc: 0.7513 - val_loss: 1.9428 - val_acc: 0.5268\n",
      "Epoch 621/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7250 - acc: 0.7440 - val_loss: 1.3126 - val_acc: 0.5732\n",
      "Epoch 622/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7277 - acc: 0.7434 - val_loss: 1.6737 - val_acc: 0.4982\n",
      "Epoch 623/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7207 - acc: 0.7466 - val_loss: 1.3349 - val_acc: 0.5973\n",
      "Epoch 624/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7083 - acc: 0.7448 - val_loss: 1.0895 - val_acc: 0.6482\n",
      "Epoch 625/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7052 - acc: 0.7467 - val_loss: 1.3396 - val_acc: 0.5786\n",
      "Epoch 626/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7189 - acc: 0.7422 - val_loss: 1.1343 - val_acc: 0.6518\n",
      "Epoch 627/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7162 - acc: 0.7455 - val_loss: 1.1226 - val_acc: 0.6384\n",
      "Epoch 628/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7034 - acc: 0.7493 - val_loss: 1.4874 - val_acc: 0.5804\n",
      "Epoch 629/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.7054 - acc: 0.7521 - val_loss: 1.2932 - val_acc: 0.5902\n",
      "Epoch 630/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7065 - acc: 0.7465 - val_loss: 1.9401 - val_acc: 0.4696\n",
      "Epoch 631/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7311 - acc: 0.7434 - val_loss: 1.4777 - val_acc: 0.5598\n",
      "Epoch 632/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7087 - acc: 0.7419 - val_loss: 1.3942 - val_acc: 0.5786\n",
      "Epoch 633/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7031 - acc: 0.7475 - val_loss: 1.3152 - val_acc: 0.5777\n",
      "Epoch 634/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7048 - acc: 0.7499 - val_loss: 1.1869 - val_acc: 0.6402\n",
      "Epoch 635/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7106 - acc: 0.7448 - val_loss: 1.2404 - val_acc: 0.6009\n",
      "Epoch 636/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7037 - acc: 0.7471 - val_loss: 1.2291 - val_acc: 0.6214\n",
      "Epoch 637/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.7053 - acc: 0.7473 - val_loss: 1.2810 - val_acc: 0.5804\n",
      "Epoch 638/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7121 - acc: 0.7505 - val_loss: 1.2438 - val_acc: 0.6000\n",
      "Epoch 639/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7018 - acc: 0.7488 - val_loss: 1.3021 - val_acc: 0.5911\n",
      "Epoch 640/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7057 - acc: 0.7482 - val_loss: 1.6931 - val_acc: 0.5527\n",
      "Epoch 641/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7076 - acc: 0.7477 - val_loss: 1.3071 - val_acc: 0.6143\n",
      "Epoch 642/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6974 - acc: 0.7508 - val_loss: 1.3005 - val_acc: 0.5759\n",
      "Epoch 643/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.7089 - acc: 0.7479 - val_loss: 1.4503 - val_acc: 0.5812\n",
      "Epoch 644/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7054 - acc: 0.7505 - val_loss: 1.1958 - val_acc: 0.6134\n",
      "Epoch 645/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7076 - acc: 0.7439 - val_loss: 1.3935 - val_acc: 0.5688\n",
      "Epoch 646/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7030 - acc: 0.7485 - val_loss: 1.1877 - val_acc: 0.6089\n",
      "Epoch 647/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6983 - acc: 0.7487 - val_loss: 1.1635 - val_acc: 0.5866\n",
      "Epoch 648/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7225 - acc: 0.7399 - val_loss: 1.1447 - val_acc: 0.6259\n",
      "Epoch 649/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7142 - acc: 0.7439 - val_loss: 1.1372 - val_acc: 0.6509\n",
      "Epoch 650/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7002 - acc: 0.7477 - val_loss: 1.3788 - val_acc: 0.5929\n",
      "Epoch 651/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6958 - acc: 0.7493 - val_loss: 1.1615 - val_acc: 0.6295\n",
      "Epoch 652/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7022 - acc: 0.7484 - val_loss: 1.3863 - val_acc: 0.5688\n",
      "Epoch 653/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6967 - acc: 0.7537 - val_loss: 1.3906 - val_acc: 0.6125\n",
      "Epoch 654/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.6996 - acc: 0.7501 - val_loss: 1.1917 - val_acc: 0.6429\n",
      "Epoch 655/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6943 - acc: 0.7547 - val_loss: 1.1136 - val_acc: 0.6491\n",
      "Epoch 656/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7004 - acc: 0.7483 - val_loss: 1.3024 - val_acc: 0.5750\n",
      "Epoch 657/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6916 - acc: 0.7483 - val_loss: 1.2012 - val_acc: 0.6232\n",
      "Epoch 658/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6983 - acc: 0.7493 - val_loss: 1.2722 - val_acc: 0.5848\n",
      "Epoch 659/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6989 - acc: 0.7485 - val_loss: 1.5514 - val_acc: 0.5393\n",
      "Epoch 660/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7176 - acc: 0.7463 - val_loss: 1.3012 - val_acc: 0.5732\n",
      "Epoch 661/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7118 - acc: 0.7467 - val_loss: 1.2257 - val_acc: 0.5920\n",
      "Epoch 662/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6810 - acc: 0.7561 - val_loss: 1.1103 - val_acc: 0.6473\n",
      "Epoch 663/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7059 - acc: 0.7463 - val_loss: 1.5110 - val_acc: 0.5848\n",
      "Epoch 664/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6960 - acc: 0.7505 - val_loss: 1.4717 - val_acc: 0.5259\n",
      "Epoch 665/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7178 - acc: 0.7507 - val_loss: 1.2492 - val_acc: 0.5902\n",
      "Epoch 666/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6938 - acc: 0.7489 - val_loss: 1.2499 - val_acc: 0.6330\n",
      "Epoch 667/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6853 - acc: 0.7567 - val_loss: 1.6381 - val_acc: 0.5527\n",
      "Epoch 668/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.7095 - acc: 0.7494 - val_loss: 1.2188 - val_acc: 0.6304\n",
      "Epoch 669/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6900 - acc: 0.7510 - val_loss: 1.4718 - val_acc: 0.5679\n",
      "Epoch 670/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7068 - acc: 0.7515 - val_loss: 1.3476 - val_acc: 0.6152\n",
      "Epoch 671/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.7047 - acc: 0.7483 - val_loss: 1.0365 - val_acc: 0.6464\n",
      "Epoch 672/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6812 - acc: 0.7538 - val_loss: 1.0765 - val_acc: 0.6509\n",
      "Epoch 673/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6826 - acc: 0.7553 - val_loss: 1.5715 - val_acc: 0.5554\n",
      "Epoch 674/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6972 - acc: 0.7576 - val_loss: 1.1277 - val_acc: 0.6348\n",
      "Epoch 675/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6799 - acc: 0.7548 - val_loss: 1.0828 - val_acc: 0.6509\n",
      "Epoch 676/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7001 - acc: 0.7500 - val_loss: 1.2462 - val_acc: 0.6080\n",
      "Epoch 677/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6787 - acc: 0.7585 - val_loss: 1.2886 - val_acc: 0.5929\n",
      "Epoch 678/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6829 - acc: 0.7594 - val_loss: 1.6523 - val_acc: 0.5196\n",
      "Epoch 679/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.7023 - acc: 0.7485 - val_loss: 1.2018 - val_acc: 0.6393\n",
      "Epoch 680/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6924 - acc: 0.7553 - val_loss: 1.1820 - val_acc: 0.6366\n",
      "Epoch 681/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6806 - acc: 0.7576 - val_loss: 1.2896 - val_acc: 0.5804\n",
      "Epoch 682/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7018 - acc: 0.7538 - val_loss: 1.3313 - val_acc: 0.6393\n",
      "Epoch 683/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6751 - acc: 0.7587 - val_loss: 1.1724 - val_acc: 0.6071\n",
      "Epoch 684/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6935 - acc: 0.7520 - val_loss: 1.3623 - val_acc: 0.5804\n",
      "Epoch 685/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6782 - acc: 0.7568 - val_loss: 1.1715 - val_acc: 0.5884\n",
      "Epoch 686/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6860 - acc: 0.7554 - val_loss: 1.3315 - val_acc: 0.5661\n",
      "Epoch 687/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6813 - acc: 0.7594 - val_loss: 1.4019 - val_acc: 0.5777\n",
      "Epoch 688/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6877 - acc: 0.7581 - val_loss: 1.3243 - val_acc: 0.5911\n",
      "Epoch 689/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6810 - acc: 0.7555 - val_loss: 1.9563 - val_acc: 0.4991\n",
      "Epoch 690/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.7040 - acc: 0.7509 - val_loss: 1.1641 - val_acc: 0.6277\n",
      "Epoch 691/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6752 - acc: 0.7587 - val_loss: 1.2844 - val_acc: 0.6304\n",
      "Epoch 692/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.6861 - acc: 0.7545 - val_loss: 1.2815 - val_acc: 0.5991\n",
      "Epoch 693/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6825 - acc: 0.7565 - val_loss: 1.1727 - val_acc: 0.6080\n",
      "Epoch 694/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6771 - acc: 0.7547 - val_loss: 1.4758 - val_acc: 0.5545\n",
      "Epoch 695/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6822 - acc: 0.7577 - val_loss: 1.3065 - val_acc: 0.6179\n",
      "Epoch 696/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6958 - acc: 0.7546 - val_loss: 1.5915 - val_acc: 0.5429\n",
      "Epoch 697/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6825 - acc: 0.7577 - val_loss: 1.5874 - val_acc: 0.4723\n",
      "Epoch 698/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6916 - acc: 0.7494 - val_loss: 1.2347 - val_acc: 0.5875\n",
      "Epoch 699/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.7046 - acc: 0.7445 - val_loss: 1.1855 - val_acc: 0.6312\n",
      "Epoch 700/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6671 - acc: 0.7629 - val_loss: 1.3059 - val_acc: 0.5955\n",
      "Epoch 701/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6734 - acc: 0.7610 - val_loss: 1.4908 - val_acc: 0.5875\n",
      "Epoch 702/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6873 - acc: 0.7537 - val_loss: 1.2129 - val_acc: 0.6321\n",
      "Epoch 703/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6758 - acc: 0.7586 - val_loss: 1.0749 - val_acc: 0.6768\n",
      "Epoch 704/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6724 - acc: 0.7593 - val_loss: 1.4267 - val_acc: 0.5295\n",
      "Epoch 705/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6924 - acc: 0.7537 - val_loss: 1.2456 - val_acc: 0.5813\n",
      "Epoch 706/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6865 - acc: 0.7525 - val_loss: 1.3706 - val_acc: 0.5848\n",
      "Epoch 707/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6691 - acc: 0.7613 - val_loss: 1.2232 - val_acc: 0.6366\n",
      "Epoch 708/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6849 - acc: 0.7525 - val_loss: 1.2078 - val_acc: 0.6241\n",
      "Epoch 709/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6726 - acc: 0.7598 - val_loss: 1.1351 - val_acc: 0.6473\n",
      "Epoch 710/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6756 - acc: 0.7571 - val_loss: 1.3044 - val_acc: 0.6125\n",
      "Epoch 711/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6886 - acc: 0.7581 - val_loss: 1.5278 - val_acc: 0.5446\n",
      "Epoch 712/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6852 - acc: 0.7567 - val_loss: 1.1203 - val_acc: 0.6330\n",
      "Epoch 713/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6605 - acc: 0.7623 - val_loss: 1.4750 - val_acc: 0.5696\n",
      "Epoch 714/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6812 - acc: 0.7548 - val_loss: 1.2270 - val_acc: 0.5741\n",
      "Epoch 715/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6795 - acc: 0.7571 - val_loss: 1.5332 - val_acc: 0.5098\n",
      "Epoch 716/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6851 - acc: 0.7545 - val_loss: 1.7229 - val_acc: 0.4982\n",
      "Epoch 717/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6722 - acc: 0.7582 - val_loss: 1.1139 - val_acc: 0.6411\n",
      "Epoch 718/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6774 - acc: 0.7556 - val_loss: 1.3250 - val_acc: 0.5759\n",
      "Epoch 719/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6751 - acc: 0.7623 - val_loss: 1.5139 - val_acc: 0.5973\n",
      "Epoch 720/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6736 - acc: 0.7617 - val_loss: 1.1610 - val_acc: 0.6563\n",
      "Epoch 721/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6742 - acc: 0.7597 - val_loss: 1.4014 - val_acc: 0.5661\n",
      "Epoch 722/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6835 - acc: 0.7543 - val_loss: 1.3553 - val_acc: 0.5607\n",
      "Epoch 723/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6748 - acc: 0.7609 - val_loss: 1.2763 - val_acc: 0.6071\n",
      "Epoch 724/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6779 - acc: 0.7563 - val_loss: 1.3391 - val_acc: 0.5759\n",
      "Epoch 725/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6946 - acc: 0.7549 - val_loss: 1.1440 - val_acc: 0.6259\n",
      "Epoch 726/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6698 - acc: 0.7576 - val_loss: 1.4842 - val_acc: 0.5429\n",
      "Epoch 727/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6862 - acc: 0.7564 - val_loss: 1.4665 - val_acc: 0.5741\n",
      "Epoch 728/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6644 - acc: 0.7615 - val_loss: 1.7253 - val_acc: 0.5518\n",
      "Epoch 729/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6775 - acc: 0.7590 - val_loss: 1.2597 - val_acc: 0.6232\n",
      "Epoch 730/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6823 - acc: 0.7578 - val_loss: 1.2088 - val_acc: 0.6491\n",
      "Epoch 731/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6750 - acc: 0.7589 - val_loss: 1.3257 - val_acc: 0.5866\n",
      "Epoch 732/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6628 - acc: 0.7617 - val_loss: 1.2493 - val_acc: 0.6098\n",
      "Epoch 733/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6616 - acc: 0.7623 - val_loss: 1.1843 - val_acc: 0.6429\n",
      "Epoch 734/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6678 - acc: 0.7674 - val_loss: 1.0914 - val_acc: 0.6357\n",
      "Epoch 735/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6603 - acc: 0.7664 - val_loss: 1.2423 - val_acc: 0.6116\n",
      "Epoch 736/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6723 - acc: 0.7588 - val_loss: 1.1615 - val_acc: 0.6080\n",
      "Epoch 737/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6687 - acc: 0.7591 - val_loss: 1.2538 - val_acc: 0.5920\n",
      "Epoch 738/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6724 - acc: 0.7597 - val_loss: 1.1942 - val_acc: 0.6205\n",
      "Epoch 739/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6717 - acc: 0.7596 - val_loss: 1.2816 - val_acc: 0.6000\n",
      "Epoch 740/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6751 - acc: 0.7585 - val_loss: 1.2986 - val_acc: 0.6098\n",
      "Epoch 741/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6657 - acc: 0.7585 - val_loss: 1.2782 - val_acc: 0.6348\n",
      "Epoch 742/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.6599 - acc: 0.7684 - val_loss: 1.5290 - val_acc: 0.5723\n",
      "Epoch 743/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6728 - acc: 0.7613 - val_loss: 1.3801 - val_acc: 0.5973\n",
      "Epoch 744/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6711 - acc: 0.7636 - val_loss: 1.7787 - val_acc: 0.5330\n",
      "Epoch 745/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6789 - acc: 0.7613 - val_loss: 1.1743 - val_acc: 0.6670\n",
      "Epoch 746/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6557 - acc: 0.7657 - val_loss: 1.1489 - val_acc: 0.6464\n",
      "Epoch 747/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6498 - acc: 0.7664 - val_loss: 1.5397 - val_acc: 0.5598\n",
      "Epoch 748/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6934 - acc: 0.7570 - val_loss: 1.3289 - val_acc: 0.6446\n",
      "Epoch 749/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6543 - acc: 0.7687 - val_loss: 1.1964 - val_acc: 0.5955\n",
      "Epoch 750/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6657 - acc: 0.7591 - val_loss: 1.7113 - val_acc: 0.4946\n",
      "Epoch 751/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6830 - acc: 0.7546 - val_loss: 1.1935 - val_acc: 0.6330\n",
      "Epoch 752/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6570 - acc: 0.7651 - val_loss: 1.6921 - val_acc: 0.5170\n",
      "Epoch 753/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6811 - acc: 0.7601 - val_loss: 1.1465 - val_acc: 0.5839\n",
      "Epoch 754/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.6677 - acc: 0.7610 - val_loss: 1.1627 - val_acc: 0.6509\n",
      "Epoch 755/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6620 - acc: 0.7669 - val_loss: 1.1530 - val_acc: 0.6000\n",
      "Epoch 756/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6745 - acc: 0.7588 - val_loss: 1.2112 - val_acc: 0.6330\n",
      "Epoch 757/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6738 - acc: 0.7555 - val_loss: 1.2116 - val_acc: 0.5991\n",
      "Epoch 758/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6740 - acc: 0.7601 - val_loss: 1.6182 - val_acc: 0.4768\n",
      "Epoch 759/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6719 - acc: 0.7639 - val_loss: 1.6172 - val_acc: 0.5223\n",
      "Epoch 760/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6750 - acc: 0.7612 - val_loss: 1.4893 - val_acc: 0.5884\n",
      "Epoch 761/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6752 - acc: 0.7573 - val_loss: 1.5368 - val_acc: 0.5670\n",
      "Epoch 762/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6849 - acc: 0.7579 - val_loss: 1.3639 - val_acc: 0.5598\n",
      "Epoch 763/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6762 - acc: 0.7569 - val_loss: 1.1807 - val_acc: 0.6420\n",
      "Epoch 764/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6535 - acc: 0.7661 - val_loss: 1.2972 - val_acc: 0.6348\n",
      "Epoch 765/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6531 - acc: 0.7646 - val_loss: 1.2673 - val_acc: 0.6071\n",
      "Epoch 766/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6671 - acc: 0.7617 - val_loss: 1.2501 - val_acc: 0.5866\n",
      "Epoch 767/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6824 - acc: 0.7574 - val_loss: 1.0594 - val_acc: 0.6330\n",
      "Epoch 768/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6466 - acc: 0.7672 - val_loss: 1.1549 - val_acc: 0.6670\n",
      "Epoch 769/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6624 - acc: 0.7630 - val_loss: 1.4296 - val_acc: 0.6152\n",
      "Epoch 770/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6508 - acc: 0.7688 - val_loss: 1.1935 - val_acc: 0.6214\n",
      "Epoch 771/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6555 - acc: 0.7657 - val_loss: 1.6597 - val_acc: 0.5357\n",
      "Epoch 772/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6572 - acc: 0.7670 - val_loss: 1.1914 - val_acc: 0.6714\n",
      "Epoch 773/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6533 - acc: 0.7695 - val_loss: 1.2459 - val_acc: 0.6134\n",
      "Epoch 774/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6545 - acc: 0.7673 - val_loss: 1.2694 - val_acc: 0.6080\n",
      "Epoch 775/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6589 - acc: 0.7647 - val_loss: 1.1147 - val_acc: 0.6554\n",
      "Epoch 776/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6497 - acc: 0.7665 - val_loss: 1.2217 - val_acc: 0.6161\n",
      "Epoch 777/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6574 - acc: 0.7671 - val_loss: 1.3419 - val_acc: 0.5795\n",
      "Epoch 778/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6568 - acc: 0.7637 - val_loss: 1.6557 - val_acc: 0.5241\n",
      "Epoch 779/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6699 - acc: 0.7609 - val_loss: 1.2525 - val_acc: 0.6446\n",
      "Epoch 780/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6368 - acc: 0.7701 - val_loss: 1.1103 - val_acc: 0.6804\n",
      "Epoch 781/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6423 - acc: 0.7687 - val_loss: 1.5045 - val_acc: 0.5589\n",
      "Epoch 782/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6492 - acc: 0.7688 - val_loss: 1.2042 - val_acc: 0.6348\n",
      "Epoch 783/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6603 - acc: 0.7647 - val_loss: 1.2628 - val_acc: 0.6464\n",
      "Epoch 784/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6518 - acc: 0.7648 - val_loss: 1.8856 - val_acc: 0.4866\n",
      "Epoch 785/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6663 - acc: 0.7639 - val_loss: 1.2758 - val_acc: 0.6286\n",
      "Epoch 786/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6457 - acc: 0.7692 - val_loss: 1.2614 - val_acc: 0.5938\n",
      "Epoch 787/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6585 - acc: 0.7657 - val_loss: 1.1509 - val_acc: 0.6071\n",
      "Epoch 788/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6539 - acc: 0.7673 - val_loss: 1.9705 - val_acc: 0.4848\n",
      "Epoch 789/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6688 - acc: 0.7639 - val_loss: 1.3797 - val_acc: 0.6143\n",
      "Epoch 790/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6685 - acc: 0.7611 - val_loss: 1.4205 - val_acc: 0.5313\n",
      "Epoch 791/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6546 - acc: 0.7687 - val_loss: 1.1249 - val_acc: 0.6500\n",
      "Epoch 792/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6490 - acc: 0.7649 - val_loss: 1.7094 - val_acc: 0.5250\n",
      "Epoch 793/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6774 - acc: 0.7624 - val_loss: 1.2619 - val_acc: 0.6152\n",
      "Epoch 794/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6381 - acc: 0.7685 - val_loss: 1.3794 - val_acc: 0.5884\n",
      "Epoch 795/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6587 - acc: 0.7663 - val_loss: 1.2243 - val_acc: 0.6232\n",
      "Epoch 796/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6503 - acc: 0.7680 - val_loss: 1.2541 - val_acc: 0.5982\n",
      "Epoch 797/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6526 - acc: 0.7704 - val_loss: 1.3621 - val_acc: 0.5732\n",
      "Epoch 798/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6526 - acc: 0.7672 - val_loss: 1.3108 - val_acc: 0.6205\n",
      "Epoch 799/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6472 - acc: 0.7683 - val_loss: 1.1811 - val_acc: 0.6196\n",
      "Epoch 800/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6525 - acc: 0.7673 - val_loss: 1.4121 - val_acc: 0.5643\n",
      "Epoch 801/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6681 - acc: 0.7573 - val_loss: 1.4387 - val_acc: 0.5982\n",
      "Epoch 802/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6556 - acc: 0.7683 - val_loss: 1.6677 - val_acc: 0.5661\n",
      "Epoch 803/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6568 - acc: 0.7689 - val_loss: 1.2922 - val_acc: 0.5938\n",
      "Epoch 804/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.6686 - acc: 0.7618 - val_loss: 1.2286 - val_acc: 0.6250\n",
      "Epoch 805/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6448 - acc: 0.7682 - val_loss: 1.3349 - val_acc: 0.5991\n",
      "Epoch 806/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6401 - acc: 0.7727 - val_loss: 1.0731 - val_acc: 0.6509\n",
      "Epoch 807/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6361 - acc: 0.7720 - val_loss: 1.2947 - val_acc: 0.6330\n",
      "Epoch 808/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6496 - acc: 0.7733 - val_loss: 1.6238 - val_acc: 0.5286\n",
      "Epoch 809/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6567 - acc: 0.7691 - val_loss: 1.2771 - val_acc: 0.5946\n",
      "Epoch 810/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6432 - acc: 0.7713 - val_loss: 1.1565 - val_acc: 0.6473\n",
      "Epoch 811/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6530 - acc: 0.7668 - val_loss: 1.1544 - val_acc: 0.6134\n",
      "Epoch 812/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6473 - acc: 0.7655 - val_loss: 1.2971 - val_acc: 0.5973\n",
      "Epoch 813/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6369 - acc: 0.7712 - val_loss: 1.5904 - val_acc: 0.5732\n",
      "Epoch 814/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6455 - acc: 0.7738 - val_loss: 1.2547 - val_acc: 0.5929\n",
      "Epoch 815/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6411 - acc: 0.7731 - val_loss: 1.2227 - val_acc: 0.6375\n",
      "Epoch 816/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6483 - acc: 0.7699 - val_loss: 1.2928 - val_acc: 0.6107\n",
      "Epoch 817/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6599 - acc: 0.7631 - val_loss: 1.3967 - val_acc: 0.5804\n",
      "Epoch 818/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6487 - acc: 0.7662 - val_loss: 1.3216 - val_acc: 0.5723\n",
      "Epoch 819/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6320 - acc: 0.7741 - val_loss: 1.4949 - val_acc: 0.5562\n",
      "Epoch 820/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6458 - acc: 0.7750 - val_loss: 1.5138 - val_acc: 0.5384\n",
      "Epoch 821/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6645 - acc: 0.7608 - val_loss: 1.3001 - val_acc: 0.6098\n",
      "Epoch 822/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6337 - acc: 0.7703 - val_loss: 1.6726 - val_acc: 0.5330\n",
      "Epoch 823/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6539 - acc: 0.7629 - val_loss: 1.6656 - val_acc: 0.5116\n",
      "Epoch 824/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6511 - acc: 0.7659 - val_loss: 1.2481 - val_acc: 0.5937\n",
      "Epoch 825/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6655 - acc: 0.7646 - val_loss: 1.5547 - val_acc: 0.5589\n",
      "Epoch 826/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6458 - acc: 0.7705 - val_loss: 1.3332 - val_acc: 0.6134\n",
      "Epoch 827/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6352 - acc: 0.7737 - val_loss: 1.1062 - val_acc: 0.6616\n",
      "Epoch 828/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6384 - acc: 0.7695 - val_loss: 1.1857 - val_acc: 0.6321\n",
      "Epoch 829/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6522 - acc: 0.7675 - val_loss: 1.4038 - val_acc: 0.6214\n",
      "Epoch 830/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6363 - acc: 0.7705 - val_loss: 1.2436 - val_acc: 0.5911\n",
      "Epoch 831/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6471 - acc: 0.7691 - val_loss: 1.1421 - val_acc: 0.6304\n",
      "Epoch 832/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6441 - acc: 0.7684 - val_loss: 1.4042 - val_acc: 0.5768\n",
      "Epoch 833/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6423 - acc: 0.7685 - val_loss: 1.2393 - val_acc: 0.6339\n",
      "Epoch 834/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6489 - acc: 0.7692 - val_loss: 1.5373 - val_acc: 0.5259\n",
      "Epoch 835/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6279 - acc: 0.7764 - val_loss: 1.5404 - val_acc: 0.5777\n",
      "Epoch 836/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6559 - acc: 0.7693 - val_loss: 1.3883 - val_acc: 0.6232\n",
      "Epoch 837/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6417 - acc: 0.7695 - val_loss: 1.4109 - val_acc: 0.5830\n",
      "Epoch 838/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6357 - acc: 0.7764 - val_loss: 1.2525 - val_acc: 0.5714\n",
      "Epoch 839/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6370 - acc: 0.7691 - val_loss: 1.4291 - val_acc: 0.5902\n",
      "Epoch 840/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6388 - acc: 0.7711 - val_loss: 1.2272 - val_acc: 0.6321\n",
      "Epoch 841/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6538 - acc: 0.7673 - val_loss: 1.0594 - val_acc: 0.6625\n",
      "Epoch 842/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6248 - acc: 0.7766 - val_loss: 1.2739 - val_acc: 0.6107\n",
      "Epoch 843/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6526 - acc: 0.7677 - val_loss: 1.3785 - val_acc: 0.5857\n",
      "Epoch 844/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6371 - acc: 0.7709 - val_loss: 1.4762 - val_acc: 0.5688\n",
      "Epoch 845/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6392 - acc: 0.7734 - val_loss: 1.7061 - val_acc: 0.4991\n",
      "Epoch 846/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6539 - acc: 0.7709 - val_loss: 1.1400 - val_acc: 0.6455\n",
      "Epoch 847/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6282 - acc: 0.7721 - val_loss: 1.3506 - val_acc: 0.6161\n",
      "Epoch 848/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6491 - acc: 0.7673 - val_loss: 1.5093 - val_acc: 0.6063\n",
      "Epoch 849/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6372 - acc: 0.7739 - val_loss: 1.0335 - val_acc: 0.6437\n",
      "Epoch 850/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6075 - acc: 0.7838 - val_loss: 1.2279 - val_acc: 0.6205\n",
      "Epoch 851/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6587 - acc: 0.7656 - val_loss: 1.3709 - val_acc: 0.5848\n",
      "Epoch 852/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6341 - acc: 0.7733 - val_loss: 1.4758 - val_acc: 0.5723\n",
      "Epoch 853/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6446 - acc: 0.7690 - val_loss: 1.3885 - val_acc: 0.5946\n",
      "Epoch 854/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6460 - acc: 0.7735 - val_loss: 1.3656 - val_acc: 0.6143\n",
      "Epoch 855/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6397 - acc: 0.7746 - val_loss: 1.4364 - val_acc: 0.5786\n",
      "Epoch 856/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6503 - acc: 0.7681 - val_loss: 1.1709 - val_acc: 0.6259\n",
      "Epoch 857/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6380 - acc: 0.7699 - val_loss: 1.1951 - val_acc: 0.6304\n",
      "Epoch 858/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6361 - acc: 0.7695 - val_loss: 1.5530 - val_acc: 0.5902\n",
      "Epoch 859/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6344 - acc: 0.7772 - val_loss: 1.4435 - val_acc: 0.5946\n",
      "Epoch 860/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6243 - acc: 0.7755 - val_loss: 1.6841 - val_acc: 0.5304\n",
      "Epoch 861/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6487 - acc: 0.7705 - val_loss: 1.4132 - val_acc: 0.5759\n",
      "Epoch 862/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6355 - acc: 0.7730 - val_loss: 1.5526 - val_acc: 0.5366\n",
      "Epoch 863/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6372 - acc: 0.7753 - val_loss: 1.6967 - val_acc: 0.4982\n",
      "Epoch 864/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6499 - acc: 0.7707 - val_loss: 1.5356 - val_acc: 0.5634\n",
      "Epoch 865/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6342 - acc: 0.7770 - val_loss: 1.2427 - val_acc: 0.6375\n",
      "Epoch 866/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6436 - acc: 0.7741 - val_loss: 1.4162 - val_acc: 0.6036\n",
      "Epoch 867/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6589 - acc: 0.7669 - val_loss: 1.3816 - val_acc: 0.5750\n",
      "Epoch 868/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6463 - acc: 0.7703 - val_loss: 1.1314 - val_acc: 0.6357\n",
      "Epoch 869/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6433 - acc: 0.7723 - val_loss: 1.2798 - val_acc: 0.6366\n",
      "Epoch 870/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6402 - acc: 0.7695 - val_loss: 1.3001 - val_acc: 0.6188\n",
      "Epoch 871/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6307 - acc: 0.7735 - val_loss: 1.3492 - val_acc: 0.6000\n",
      "Epoch 872/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6314 - acc: 0.7767 - val_loss: 1.2554 - val_acc: 0.6312\n",
      "Epoch 873/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6378 - acc: 0.7749 - val_loss: 1.5779 - val_acc: 0.4920\n",
      "Epoch 874/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6380 - acc: 0.7759 - val_loss: 1.3772 - val_acc: 0.6348\n",
      "Epoch 875/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6393 - acc: 0.7777 - val_loss: 1.2965 - val_acc: 0.6348\n",
      "Epoch 876/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6259 - acc: 0.7771 - val_loss: 1.0799 - val_acc: 0.6643\n",
      "Epoch 877/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6321 - acc: 0.7736 - val_loss: 1.2428 - val_acc: 0.5875\n",
      "Epoch 878/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6360 - acc: 0.7736 - val_loss: 1.2378 - val_acc: 0.5991\n",
      "Epoch 879/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6207 - acc: 0.7791 - val_loss: 1.1356 - val_acc: 0.6455\n",
      "Epoch 880/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.6360 - acc: 0.7685 - val_loss: 1.2897 - val_acc: 0.6179\n",
      "Epoch 881/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6083 - acc: 0.7837 - val_loss: 1.5046 - val_acc: 0.5875\n",
      "Epoch 882/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6235 - acc: 0.7787 - val_loss: 1.1424 - val_acc: 0.6518\n",
      "Epoch 883/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6316 - acc: 0.7738 - val_loss: 1.3642 - val_acc: 0.6205\n",
      "Epoch 884/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6295 - acc: 0.7734 - val_loss: 1.3133 - val_acc: 0.5866\n",
      "Epoch 885/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6333 - acc: 0.7751 - val_loss: 1.1481 - val_acc: 0.6179\n",
      "Epoch 886/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6174 - acc: 0.7789 - val_loss: 1.3508 - val_acc: 0.6089\n",
      "Epoch 887/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6307 - acc: 0.7784 - val_loss: 1.2145 - val_acc: 0.6241\n",
      "Epoch 888/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6244 - acc: 0.7773 - val_loss: 1.3930 - val_acc: 0.5964\n",
      "Epoch 889/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6339 - acc: 0.7739 - val_loss: 1.3390 - val_acc: 0.5911\n",
      "Epoch 890/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6269 - acc: 0.7780 - val_loss: 1.1127 - val_acc: 0.6384\n",
      "Epoch 891/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6203 - acc: 0.7761 - val_loss: 1.4226 - val_acc: 0.5857\n",
      "Epoch 892/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6271 - acc: 0.7736 - val_loss: 1.7580 - val_acc: 0.5527\n",
      "Epoch 893/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6392 - acc: 0.7735 - val_loss: 1.1929 - val_acc: 0.6500\n",
      "Epoch 894/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6287 - acc: 0.7767 - val_loss: 1.2353 - val_acc: 0.6205\n",
      "Epoch 895/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6221 - acc: 0.7774 - val_loss: 1.1591 - val_acc: 0.6598\n",
      "Epoch 896/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6070 - acc: 0.7818 - val_loss: 1.2263 - val_acc: 0.6098\n",
      "Epoch 897/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6200 - acc: 0.7787 - val_loss: 1.6247 - val_acc: 0.5536\n",
      "Epoch 898/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6311 - acc: 0.7819 - val_loss: 1.1477 - val_acc: 0.6634\n",
      "Epoch 899/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6121 - acc: 0.7777 - val_loss: 1.6986 - val_acc: 0.5295\n",
      "Epoch 900/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6491 - acc: 0.7713 - val_loss: 1.4397 - val_acc: 0.5812\n",
      "Epoch 901/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6206 - acc: 0.7772 - val_loss: 1.2267 - val_acc: 0.6312\n",
      "Epoch 902/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6168 - acc: 0.7817 - val_loss: 1.1100 - val_acc: 0.6429\n",
      "Epoch 903/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6184 - acc: 0.7767 - val_loss: 1.2427 - val_acc: 0.6170\n",
      "Epoch 904/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6126 - acc: 0.7806 - val_loss: 1.1214 - val_acc: 0.6295\n",
      "Epoch 905/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6167 - acc: 0.7786 - val_loss: 1.4479 - val_acc: 0.6152\n",
      "Epoch 906/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6350 - acc: 0.7749 - val_loss: 1.0329 - val_acc: 0.6518\n",
      "Epoch 907/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6114 - acc: 0.7820 - val_loss: 1.1842 - val_acc: 0.6554\n",
      "Epoch 908/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6118 - acc: 0.7832 - val_loss: 1.5498 - val_acc: 0.5589\n",
      "Epoch 909/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6278 - acc: 0.7782 - val_loss: 1.2690 - val_acc: 0.6473\n",
      "Epoch 910/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6327 - acc: 0.7777 - val_loss: 1.2252 - val_acc: 0.6437\n",
      "Epoch 911/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6197 - acc: 0.7783 - val_loss: 1.1843 - val_acc: 0.5955\n",
      "Epoch 912/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6086 - acc: 0.7846 - val_loss: 1.4595 - val_acc: 0.5580\n",
      "Epoch 913/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6309 - acc: 0.7761 - val_loss: 1.2675 - val_acc: 0.5857\n",
      "Epoch 914/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6052 - acc: 0.7872 - val_loss: 1.7689 - val_acc: 0.5241\n",
      "Epoch 915/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6462 - acc: 0.7746 - val_loss: 1.2985 - val_acc: 0.6116\n",
      "Epoch 916/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.6077 - acc: 0.7856 - val_loss: 1.1316 - val_acc: 0.6241\n",
      "Epoch 917/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6250 - acc: 0.7763 - val_loss: 1.3065 - val_acc: 0.6134\n",
      "Epoch 918/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6112 - acc: 0.7817 - val_loss: 1.7985 - val_acc: 0.5045\n",
      "Epoch 919/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6339 - acc: 0.7801 - val_loss: 1.6160 - val_acc: 0.5268\n",
      "Epoch 920/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6246 - acc: 0.7807 - val_loss: 1.2197 - val_acc: 0.6313\n",
      "Epoch 921/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6063 - acc: 0.7836 - val_loss: 1.2734 - val_acc: 0.6277\n",
      "Epoch 922/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6118 - acc: 0.7793 - val_loss: 1.6154 - val_acc: 0.5330\n",
      "Epoch 923/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6233 - acc: 0.7763 - val_loss: 1.1630 - val_acc: 0.6196\n",
      "Epoch 924/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6230 - acc: 0.7774 - val_loss: 1.1149 - val_acc: 0.6589\n",
      "Epoch 925/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5955 - acc: 0.7881 - val_loss: 1.5781 - val_acc: 0.5723\n",
      "Epoch 926/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6173 - acc: 0.7840 - val_loss: 1.2988 - val_acc: 0.6527\n",
      "Epoch 927/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6350 - acc: 0.7746 - val_loss: 1.0664 - val_acc: 0.6536\n",
      "Epoch 928/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5912 - acc: 0.7894 - val_loss: 1.3704 - val_acc: 0.5821\n",
      "Epoch 929/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6164 - acc: 0.7782 - val_loss: 1.3089 - val_acc: 0.6205\n",
      "Epoch 930/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6125 - acc: 0.7820 - val_loss: 1.1991 - val_acc: 0.6304\n",
      "Epoch 931/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6077 - acc: 0.7857 - val_loss: 1.4995 - val_acc: 0.5625\n",
      "Epoch 932/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6206 - acc: 0.7830 - val_loss: 1.1894 - val_acc: 0.6455\n",
      "Epoch 933/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5957 - acc: 0.7854 - val_loss: 1.2146 - val_acc: 0.6125\n",
      "Epoch 934/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6055 - acc: 0.7822 - val_loss: 1.2912 - val_acc: 0.6063\n",
      "Epoch 935/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6154 - acc: 0.7813 - val_loss: 1.3952 - val_acc: 0.5857\n",
      "Epoch 936/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6264 - acc: 0.7745 - val_loss: 1.1339 - val_acc: 0.6411\n",
      "Epoch 937/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6060 - acc: 0.7831 - val_loss: 1.6394 - val_acc: 0.5125\n",
      "Epoch 938/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6306 - acc: 0.7767 - val_loss: 1.2934 - val_acc: 0.6205\n",
      "Epoch 939/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6181 - acc: 0.7805 - val_loss: 1.1328 - val_acc: 0.6536\n",
      "Epoch 940/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6148 - acc: 0.7798 - val_loss: 1.4045 - val_acc: 0.6018\n",
      "Epoch 941/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6135 - acc: 0.7836 - val_loss: 1.3766 - val_acc: 0.6214\n",
      "Epoch 942/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6038 - acc: 0.7868 - val_loss: 1.3520 - val_acc: 0.5821\n",
      "Epoch 943/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6064 - acc: 0.7843 - val_loss: 1.2274 - val_acc: 0.6179\n",
      "Epoch 944/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6204 - acc: 0.7777 - val_loss: 1.0934 - val_acc: 0.6330\n",
      "Epoch 945/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6253 - acc: 0.7736 - val_loss: 1.7248 - val_acc: 0.5330\n",
      "Epoch 946/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5963 - acc: 0.7898 - val_loss: 1.2954 - val_acc: 0.6250\n",
      "Epoch 947/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6085 - acc: 0.7848 - val_loss: 1.1634 - val_acc: 0.6161\n",
      "Epoch 948/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5963 - acc: 0.7877 - val_loss: 1.3001 - val_acc: 0.6071\n",
      "Epoch 949/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6347 - acc: 0.7750 - val_loss: 1.2061 - val_acc: 0.6161\n",
      "Epoch 950/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6137 - acc: 0.7793 - val_loss: 1.2260 - val_acc: 0.6295\n",
      "Epoch 951/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6124 - acc: 0.7804 - val_loss: 1.2742 - val_acc: 0.6179\n",
      "Epoch 952/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6060 - acc: 0.7839 - val_loss: 1.2264 - val_acc: 0.6536\n",
      "Epoch 953/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5941 - acc: 0.7876 - val_loss: 1.4321 - val_acc: 0.5884\n",
      "Epoch 954/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6036 - acc: 0.7868 - val_loss: 1.5668 - val_acc: 0.5437\n",
      "Epoch 955/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.6139 - acc: 0.7808 - val_loss: 1.2412 - val_acc: 0.6170\n",
      "Epoch 956/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5996 - acc: 0.7850 - val_loss: 1.4158 - val_acc: 0.5839\n",
      "Epoch 957/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6014 - acc: 0.7864 - val_loss: 1.4200 - val_acc: 0.5857\n",
      "Epoch 958/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6114 - acc: 0.7828 - val_loss: 1.2596 - val_acc: 0.6223\n",
      "Epoch 959/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6026 - acc: 0.7866 - val_loss: 1.2452 - val_acc: 0.6482\n",
      "Epoch 960/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6112 - acc: 0.7836 - val_loss: 1.7691 - val_acc: 0.5455\n",
      "Epoch 961/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6338 - acc: 0.7789 - val_loss: 1.1076 - val_acc: 0.6545\n",
      "Epoch 962/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5936 - acc: 0.7885 - val_loss: 1.4297 - val_acc: 0.6045\n",
      "Epoch 963/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5978 - acc: 0.7858 - val_loss: 1.2841 - val_acc: 0.6295\n",
      "Epoch 964/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5943 - acc: 0.7886 - val_loss: 1.2366 - val_acc: 0.6250\n",
      "Epoch 965/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6084 - acc: 0.7807 - val_loss: 1.1590 - val_acc: 0.6527\n",
      "Epoch 966/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5979 - acc: 0.7904 - val_loss: 1.0789 - val_acc: 0.6661\n",
      "Epoch 967/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5963 - acc: 0.7867 - val_loss: 1.3779 - val_acc: 0.6339\n",
      "Epoch 968/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6147 - acc: 0.7844 - val_loss: 1.2254 - val_acc: 0.6527\n",
      "Epoch 969/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6031 - acc: 0.7840 - val_loss: 1.1675 - val_acc: 0.6625\n",
      "Epoch 970/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5934 - acc: 0.7861 - val_loss: 1.2296 - val_acc: 0.6384\n",
      "Epoch 971/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6020 - acc: 0.7857 - val_loss: 1.0608 - val_acc: 0.6598\n",
      "Epoch 972/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5929 - acc: 0.7888 - val_loss: 1.6560 - val_acc: 0.5473\n",
      "Epoch 973/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6236 - acc: 0.7815 - val_loss: 1.1194 - val_acc: 0.6295\n",
      "Epoch 974/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5865 - acc: 0.7904 - val_loss: 1.1621 - val_acc: 0.6661\n",
      "Epoch 975/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6045 - acc: 0.7820 - val_loss: 1.5226 - val_acc: 0.5795\n",
      "Epoch 976/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6136 - acc: 0.7793 - val_loss: 1.3557 - val_acc: 0.6170\n",
      "Epoch 977/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6013 - acc: 0.7863 - val_loss: 1.4934 - val_acc: 0.6054\n",
      "Epoch 978/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6114 - acc: 0.7836 - val_loss: 1.1354 - val_acc: 0.6821\n",
      "Epoch 979/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6027 - acc: 0.7850 - val_loss: 1.4066 - val_acc: 0.5759\n",
      "Epoch 980/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5983 - acc: 0.7858 - val_loss: 1.6606 - val_acc: 0.5571\n",
      "Epoch 981/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6217 - acc: 0.7795 - val_loss: 1.4098 - val_acc: 0.5884\n",
      "Epoch 982/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5921 - acc: 0.7888 - val_loss: 1.7328 - val_acc: 0.5420\n",
      "Epoch 983/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6167 - acc: 0.7824 - val_loss: 1.4189 - val_acc: 0.6107\n",
      "Epoch 984/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5983 - acc: 0.7866 - val_loss: 1.3538 - val_acc: 0.6089\n",
      "Epoch 985/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6025 - acc: 0.7854 - val_loss: 1.2806 - val_acc: 0.6277\n",
      "Epoch 986/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5796 - acc: 0.7929 - val_loss: 1.6659 - val_acc: 0.5687\n",
      "Epoch 987/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6211 - acc: 0.7791 - val_loss: 1.5987 - val_acc: 0.5509\n",
      "Epoch 988/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5870 - acc: 0.7916 - val_loss: 1.4808 - val_acc: 0.5804\n",
      "Epoch 989/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6130 - acc: 0.7848 - val_loss: 1.3156 - val_acc: 0.6089\n",
      "Epoch 990/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5974 - acc: 0.7908 - val_loss: 1.1584 - val_acc: 0.6036\n",
      "Epoch 991/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.6089 - acc: 0.7854 - val_loss: 1.2963 - val_acc: 0.6125\n",
      "Epoch 992/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.6093 - acc: 0.7827 - val_loss: 1.2128 - val_acc: 0.6018\n",
      "Epoch 993/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5866 - acc: 0.7926 - val_loss: 1.2435 - val_acc: 0.6357\n",
      "Epoch 994/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6083 - acc: 0.7869 - val_loss: 1.3673 - val_acc: 0.6125\n",
      "Epoch 995/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6002 - acc: 0.7832 - val_loss: 1.1142 - val_acc: 0.6902\n",
      "Epoch 996/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5841 - acc: 0.7876 - val_loss: 1.3225 - val_acc: 0.6009\n",
      "Epoch 997/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5896 - acc: 0.7890 - val_loss: 1.4333 - val_acc: 0.5911\n",
      "Epoch 998/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5974 - acc: 0.7898 - val_loss: 1.2582 - val_acc: 0.6196\n",
      "Epoch 999/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5818 - acc: 0.7945 - val_loss: 1.6385 - val_acc: 0.5491\n",
      "Epoch 1000/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6197 - acc: 0.7830 - val_loss: 1.1402 - val_acc: 0.6420\n",
      "Epoch 1001/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6036 - acc: 0.7866 - val_loss: 1.7861 - val_acc: 0.5643\n",
      "Epoch 1002/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6098 - acc: 0.7840 - val_loss: 1.3402 - val_acc: 0.6607\n",
      "Epoch 1003/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6080 - acc: 0.7842 - val_loss: 1.1075 - val_acc: 0.6545\n",
      "Epoch 1004/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5976 - acc: 0.7823 - val_loss: 1.1722 - val_acc: 0.6188\n",
      "Epoch 1005/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5952 - acc: 0.7860 - val_loss: 0.9843 - val_acc: 0.7080\n",
      "Epoch 1006/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5705 - acc: 0.7932 - val_loss: 1.3340 - val_acc: 0.6357\n",
      "Epoch 1007/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5936 - acc: 0.7873 - val_loss: 1.4506 - val_acc: 0.6232\n",
      "Epoch 1008/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5940 - acc: 0.7874 - val_loss: 1.1646 - val_acc: 0.6509\n",
      "Epoch 1009/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5889 - acc: 0.7927 - val_loss: 1.6025 - val_acc: 0.5473\n",
      "Epoch 1010/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6143 - acc: 0.7840 - val_loss: 1.3322 - val_acc: 0.5652\n",
      "Epoch 1011/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5998 - acc: 0.7870 - val_loss: 1.3661 - val_acc: 0.6009\n",
      "Epoch 1012/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5938 - acc: 0.7878 - val_loss: 1.4942 - val_acc: 0.6232\n",
      "Epoch 1013/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5854 - acc: 0.7910 - val_loss: 1.0299 - val_acc: 0.6839\n",
      "Epoch 1014/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5965 - acc: 0.7860 - val_loss: 1.3337 - val_acc: 0.6000\n",
      "Epoch 1015/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5936 - acc: 0.7850 - val_loss: 1.3667 - val_acc: 0.6321\n",
      "Epoch 1016/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5838 - acc: 0.7936 - val_loss: 1.3359 - val_acc: 0.6312\n",
      "Epoch 1017/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5958 - acc: 0.7863 - val_loss: 1.2038 - val_acc: 0.6429\n",
      "Epoch 1018/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5844 - acc: 0.7906 - val_loss: 1.2125 - val_acc: 0.6473\n",
      "Epoch 1019/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5943 - acc: 0.7866 - val_loss: 1.2441 - val_acc: 0.6536\n",
      "Epoch 1020/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5930 - acc: 0.7857 - val_loss: 1.3456 - val_acc: 0.6277\n",
      "Epoch 1021/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6047 - acc: 0.7841 - val_loss: 1.0481 - val_acc: 0.6821\n",
      "Epoch 1022/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5697 - acc: 0.7967 - val_loss: 1.4668 - val_acc: 0.6196\n",
      "Epoch 1023/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5868 - acc: 0.7888 - val_loss: 1.6652 - val_acc: 0.5643\n",
      "Epoch 1024/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5954 - acc: 0.7900 - val_loss: 1.3122 - val_acc: 0.6205\n",
      "Epoch 1025/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5917 - acc: 0.7870 - val_loss: 1.2680 - val_acc: 0.6527\n",
      "Epoch 1026/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5928 - acc: 0.7876 - val_loss: 1.0505 - val_acc: 0.6643\n",
      "Epoch 1027/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5806 - acc: 0.7907 - val_loss: 1.3055 - val_acc: 0.5964\n",
      "Epoch 1028/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5981 - acc: 0.7847 - val_loss: 1.2113 - val_acc: 0.6536\n",
      "Epoch 1029/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5741 - acc: 0.7912 - val_loss: 1.3863 - val_acc: 0.5598\n",
      "Epoch 1030/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5923 - acc: 0.7898 - val_loss: 1.4182 - val_acc: 0.6268\n",
      "Epoch 1031/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5997 - acc: 0.7853 - val_loss: 1.2845 - val_acc: 0.6009\n",
      "Epoch 1032/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5870 - acc: 0.7874 - val_loss: 1.5847 - val_acc: 0.5554\n",
      "Epoch 1033/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6167 - acc: 0.7850 - val_loss: 1.4921 - val_acc: 0.5821\n",
      "Epoch 1034/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5869 - acc: 0.7914 - val_loss: 1.3297 - val_acc: 0.6134\n",
      "Epoch 1035/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6018 - acc: 0.7896 - val_loss: 1.5727 - val_acc: 0.5768\n",
      "Epoch 1036/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5848 - acc: 0.7937 - val_loss: 1.5138 - val_acc: 0.5348\n",
      "Epoch 1037/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5914 - acc: 0.7920 - val_loss: 1.3386 - val_acc: 0.6018\n",
      "Epoch 1038/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5997 - acc: 0.7868 - val_loss: 1.0059 - val_acc: 0.6696\n",
      "Epoch 1039/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5836 - acc: 0.7948 - val_loss: 1.4231 - val_acc: 0.5839\n",
      "Epoch 1040/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6008 - acc: 0.7850 - val_loss: 1.1993 - val_acc: 0.6402\n",
      "Epoch 1041/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5815 - acc: 0.7914 - val_loss: 1.1668 - val_acc: 0.6420\n",
      "Epoch 1042/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.5763 - acc: 0.7948 - val_loss: 1.6541 - val_acc: 0.5714\n",
      "Epoch 1043/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.6075 - acc: 0.7837 - val_loss: 1.4813 - val_acc: 0.5955\n",
      "Epoch 1044/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.6219 - acc: 0.7838 - val_loss: 1.1576 - val_acc: 0.6518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5901 - acc: 0.7876 - val_loss: 1.2364 - val_acc: 0.6054\n",
      "Epoch 1046/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6046 - acc: 0.7839 - val_loss: 1.4461 - val_acc: 0.5893\n",
      "Epoch 1047/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5895 - acc: 0.7896 - val_loss: 1.4264 - val_acc: 0.6063\n",
      "Epoch 1048/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5941 - acc: 0.7900 - val_loss: 1.1762 - val_acc: 0.6348\n",
      "Epoch 1049/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5770 - acc: 0.7890 - val_loss: 1.3719 - val_acc: 0.6071\n",
      "Epoch 1050/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.6012 - acc: 0.7860 - val_loss: 1.1346 - val_acc: 0.6670\n",
      "Epoch 1051/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5841 - acc: 0.7914 - val_loss: 1.2813 - val_acc: 0.6393\n",
      "Epoch 1052/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5818 - acc: 0.7918 - val_loss: 1.5410 - val_acc: 0.5759\n",
      "Epoch 1053/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5847 - acc: 0.7897 - val_loss: 1.6806 - val_acc: 0.5339\n",
      "Epoch 1054/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.6168 - acc: 0.7861 - val_loss: 1.3140 - val_acc: 0.5991\n",
      "Epoch 1055/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5944 - acc: 0.7863 - val_loss: 1.2284 - val_acc: 0.6455\n",
      "Epoch 1056/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5881 - acc: 0.7899 - val_loss: 1.2519 - val_acc: 0.6348\n",
      "Epoch 1057/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5999 - acc: 0.7860 - val_loss: 1.2654 - val_acc: 0.6018\n",
      "Epoch 1058/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5813 - acc: 0.7907 - val_loss: 1.0952 - val_acc: 0.6277\n",
      "Epoch 1059/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5867 - acc: 0.7902 - val_loss: 1.3600 - val_acc: 0.6295\n",
      "Epoch 1060/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5767 - acc: 0.7970 - val_loss: 1.7053 - val_acc: 0.5482\n",
      "Epoch 1061/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6065 - acc: 0.7858 - val_loss: 1.2331 - val_acc: 0.6500\n",
      "Epoch 1062/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5708 - acc: 0.7970 - val_loss: 1.0844 - val_acc: 0.6491\n",
      "Epoch 1063/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5954 - acc: 0.7877 - val_loss: 1.2310 - val_acc: 0.6366\n",
      "Epoch 1064/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5746 - acc: 0.7936 - val_loss: 1.3046 - val_acc: 0.5964\n",
      "Epoch 1065/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5819 - acc: 0.7940 - val_loss: 1.2353 - val_acc: 0.6286\n",
      "Epoch 1066/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5859 - acc: 0.7938 - val_loss: 1.2129 - val_acc: 0.6384\n",
      "Epoch 1067/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5789 - acc: 0.7876 - val_loss: 1.5614 - val_acc: 0.5759\n",
      "Epoch 1068/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5901 - acc: 0.7914 - val_loss: 1.3906 - val_acc: 0.5946\n",
      "Epoch 1069/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5944 - acc: 0.7878 - val_loss: 1.4316 - val_acc: 0.6063\n",
      "Epoch 1070/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5937 - acc: 0.7891 - val_loss: 1.1078 - val_acc: 0.6911\n",
      "Epoch 1071/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5638 - acc: 0.7967 - val_loss: 1.4380 - val_acc: 0.6036\n",
      "Epoch 1072/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5796 - acc: 0.7935 - val_loss: 1.2913 - val_acc: 0.6384\n",
      "Epoch 1073/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6026 - acc: 0.7834 - val_loss: 1.4379 - val_acc: 0.5973\n",
      "Epoch 1074/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5665 - acc: 0.7973 - val_loss: 1.3039 - val_acc: 0.6116\n",
      "Epoch 1075/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.6063 - acc: 0.7811 - val_loss: 1.1922 - val_acc: 0.6455\n",
      "Epoch 1076/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5674 - acc: 0.7969 - val_loss: 1.8466 - val_acc: 0.5786\n",
      "Epoch 1077/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5850 - acc: 0.7930 - val_loss: 1.6392 - val_acc: 0.5598\n",
      "Epoch 1078/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5999 - acc: 0.7890 - val_loss: 1.0882 - val_acc: 0.6429\n",
      "Epoch 1079/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.5641 - acc: 0.7964 - val_loss: 1.5277 - val_acc: 0.5741\n",
      "Epoch 1080/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5826 - acc: 0.7909 - val_loss: 1.1753 - val_acc: 0.6687\n",
      "Epoch 1081/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5812 - acc: 0.7934 - val_loss: 1.6179 - val_acc: 0.5777\n",
      "Epoch 1082/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5976 - acc: 0.7890 - val_loss: 1.1518 - val_acc: 0.6313\n",
      "Epoch 1083/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5824 - acc: 0.7930 - val_loss: 1.2939 - val_acc: 0.6446\n",
      "Epoch 1084/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5808 - acc: 0.7927 - val_loss: 1.2265 - val_acc: 0.6580\n",
      "Epoch 1085/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5560 - acc: 0.8028 - val_loss: 1.2250 - val_acc: 0.5607\n",
      "Epoch 1086/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5992 - acc: 0.7849 - val_loss: 1.2587 - val_acc: 0.6277\n",
      "Epoch 1087/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5807 - acc: 0.7915 - val_loss: 1.2985 - val_acc: 0.5768\n",
      "Epoch 1088/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5919 - acc: 0.7904 - val_loss: 1.5187 - val_acc: 0.5848\n",
      "Epoch 1089/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5850 - acc: 0.7912 - val_loss: 1.3431 - val_acc: 0.6277\n",
      "Epoch 1090/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5905 - acc: 0.7894 - val_loss: 1.2120 - val_acc: 0.6509\n",
      "Epoch 1091/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5805 - acc: 0.7886 - val_loss: 1.2654 - val_acc: 0.6125\n",
      "Epoch 1092/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.5838 - acc: 0.7929 - val_loss: 1.4015 - val_acc: 0.6009\n",
      "Epoch 1093/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5797 - acc: 0.7962 - val_loss: 1.8050 - val_acc: 0.5732\n",
      "Epoch 1094/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5825 - acc: 0.7928 - val_loss: 1.1375 - val_acc: 0.6429\n",
      "Epoch 1095/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5843 - acc: 0.7922 - val_loss: 1.1641 - val_acc: 0.6679\n",
      "Epoch 1096/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5684 - acc: 0.7948 - val_loss: 1.4326 - val_acc: 0.5768\n",
      "Epoch 1097/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5890 - acc: 0.7887 - val_loss: 1.3659 - val_acc: 0.6125\n",
      "Epoch 1098/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5813 - acc: 0.7971 - val_loss: 1.0213 - val_acc: 0.6759\n",
      "Epoch 1099/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5657 - acc: 0.7948 - val_loss: 1.1463 - val_acc: 0.6670\n",
      "Epoch 1100/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5806 - acc: 0.7942 - val_loss: 1.3112 - val_acc: 0.6205\n",
      "Epoch 1101/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5752 - acc: 0.7978 - val_loss: 1.3980 - val_acc: 0.6063\n",
      "Epoch 1102/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5710 - acc: 0.7971 - val_loss: 1.2401 - val_acc: 0.6732\n",
      "Epoch 1103/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5711 - acc: 0.7950 - val_loss: 1.1989 - val_acc: 0.6518\n",
      "Epoch 1104/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5859 - acc: 0.7915 - val_loss: 1.4477 - val_acc: 0.5946\n",
      "Epoch 1105/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5670 - acc: 0.8010 - val_loss: 1.4643 - val_acc: 0.6089\n",
      "Epoch 1106/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5729 - acc: 0.7970 - val_loss: 1.2369 - val_acc: 0.6688\n",
      "Epoch 1107/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5623 - acc: 0.7965 - val_loss: 1.1113 - val_acc: 0.6839\n",
      "Epoch 1108/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5815 - acc: 0.7948 - val_loss: 1.4735 - val_acc: 0.6018\n",
      "Epoch 1109/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5917 - acc: 0.7884 - val_loss: 1.2688 - val_acc: 0.6152\n",
      "Epoch 1110/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5655 - acc: 0.7990 - val_loss: 1.5079 - val_acc: 0.6134\n",
      "Epoch 1111/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5848 - acc: 0.7904 - val_loss: 1.1680 - val_acc: 0.6527\n",
      "Epoch 1112/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5786 - acc: 0.7940 - val_loss: 1.3537 - val_acc: 0.6125\n",
      "Epoch 1113/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5830 - acc: 0.7895 - val_loss: 1.2079 - val_acc: 0.6446\n",
      "Epoch 1114/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5606 - acc: 0.8013 - val_loss: 1.4632 - val_acc: 0.5875\n",
      "Epoch 1115/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5911 - acc: 0.7902 - val_loss: 1.7155 - val_acc: 0.5714\n",
      "Epoch 1116/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5774 - acc: 0.7978 - val_loss: 1.5050 - val_acc: 0.5696\n",
      "Epoch 1117/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5864 - acc: 0.7922 - val_loss: 1.1986 - val_acc: 0.6589\n",
      "Epoch 1118/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5718 - acc: 0.7965 - val_loss: 1.4095 - val_acc: 0.6250\n",
      "Epoch 1119/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5819 - acc: 0.7925 - val_loss: 1.2138 - val_acc: 0.6518\n",
      "Epoch 1120/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5681 - acc: 0.7956 - val_loss: 1.4519 - val_acc: 0.6098\n",
      "Epoch 1121/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5681 - acc: 0.7946 - val_loss: 1.3419 - val_acc: 0.6259\n",
      "Epoch 1122/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5770 - acc: 0.7974 - val_loss: 1.2188 - val_acc: 0.6446\n",
      "Epoch 1123/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5547 - acc: 0.8010 - val_loss: 1.2195 - val_acc: 0.6330\n",
      "Epoch 1124/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5833 - acc: 0.7916 - val_loss: 1.1308 - val_acc: 0.6482\n",
      "Epoch 1125/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5544 - acc: 0.8014 - val_loss: 1.2717 - val_acc: 0.6446\n",
      "Epoch 1126/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5650 - acc: 0.7982 - val_loss: 1.2343 - val_acc: 0.6384\n",
      "Epoch 1127/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5812 - acc: 0.7936 - val_loss: 1.2348 - val_acc: 0.6321\n",
      "Epoch 1128/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5740 - acc: 0.7960 - val_loss: 1.1124 - val_acc: 0.6893\n",
      "Epoch 1129/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5610 - acc: 0.8019 - val_loss: 1.2312 - val_acc: 0.6527\n",
      "Epoch 1130/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5663 - acc: 0.8012 - val_loss: 1.3182 - val_acc: 0.6402\n",
      "Epoch 1131/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5646 - acc: 0.7964 - val_loss: 1.1727 - val_acc: 0.6321\n",
      "Epoch 1132/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5665 - acc: 0.7986 - val_loss: 1.1678 - val_acc: 0.6393\n",
      "Epoch 1133/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5651 - acc: 0.8012 - val_loss: 1.4489 - val_acc: 0.5893\n",
      "Epoch 1134/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5740 - acc: 0.7970 - val_loss: 1.2312 - val_acc: 0.6589\n",
      "Epoch 1135/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5503 - acc: 0.8028 - val_loss: 1.1026 - val_acc: 0.6723\n",
      "Epoch 1136/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5607 - acc: 0.7983 - val_loss: 1.4268 - val_acc: 0.6045\n",
      "Epoch 1137/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5805 - acc: 0.7937 - val_loss: 1.3762 - val_acc: 0.5911\n",
      "Epoch 1138/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5732 - acc: 0.7989 - val_loss: 1.3906 - val_acc: 0.6259\n",
      "Epoch 1139/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5519 - acc: 0.8022 - val_loss: 1.1839 - val_acc: 0.6464\n",
      "Epoch 1140/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5827 - acc: 0.7936 - val_loss: 1.0540 - val_acc: 0.6902\n",
      "Epoch 1141/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5595 - acc: 0.7999 - val_loss: 1.1894 - val_acc: 0.6313\n",
      "Epoch 1142/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5659 - acc: 0.7960 - val_loss: 1.2956 - val_acc: 0.6411\n",
      "Epoch 1143/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5818 - acc: 0.7966 - val_loss: 1.3230 - val_acc: 0.5946\n",
      "Epoch 1144/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5534 - acc: 0.8040 - val_loss: 1.4572 - val_acc: 0.6152\n",
      "Epoch 1145/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5763 - acc: 0.7936 - val_loss: 1.1975 - val_acc: 0.6616\n",
      "Epoch 1146/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5555 - acc: 0.8024 - val_loss: 1.4995 - val_acc: 0.5982\n",
      "Epoch 1147/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5622 - acc: 0.7988 - val_loss: 1.2892 - val_acc: 0.5920\n",
      "Epoch 1148/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5806 - acc: 0.7917 - val_loss: 1.3237 - val_acc: 0.6196\n",
      "Epoch 1149/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5713 - acc: 0.7973 - val_loss: 1.3687 - val_acc: 0.6313\n",
      "Epoch 1150/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5722 - acc: 0.7962 - val_loss: 1.9330 - val_acc: 0.5196\n",
      "Epoch 1151/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5954 - acc: 0.7928 - val_loss: 1.0977 - val_acc: 0.6607\n",
      "Epoch 1152/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5613 - acc: 0.7990 - val_loss: 1.3313 - val_acc: 0.6241\n",
      "Epoch 1153/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5679 - acc: 0.7942 - val_loss: 1.0436 - val_acc: 0.6661\n",
      "Epoch 1154/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.5623 - acc: 0.7979 - val_loss: 1.3549 - val_acc: 0.6330\n",
      "Epoch 1155/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5582 - acc: 0.8016 - val_loss: 1.1719 - val_acc: 0.6732\n",
      "Epoch 1156/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5623 - acc: 0.8000 - val_loss: 1.1679 - val_acc: 0.6348\n",
      "Epoch 1157/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5613 - acc: 0.7995 - val_loss: 1.3383 - val_acc: 0.6348\n",
      "Epoch 1158/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5648 - acc: 0.7992 - val_loss: 1.2858 - val_acc: 0.5902\n",
      "Epoch 1159/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5859 - acc: 0.7908 - val_loss: 1.2513 - val_acc: 0.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1160/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5703 - acc: 0.7976 - val_loss: 1.3722 - val_acc: 0.5991\n",
      "Epoch 1161/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5798 - acc: 0.7950 - val_loss: 1.1100 - val_acc: 0.6580\n",
      "Epoch 1162/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5474 - acc: 0.8061 - val_loss: 1.3287 - val_acc: 0.6134\n",
      "Epoch 1163/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5572 - acc: 0.7990 - val_loss: 1.2695 - val_acc: 0.6429\n",
      "Epoch 1164/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5550 - acc: 0.8019 - val_loss: 1.4660 - val_acc: 0.5812\n",
      "Epoch 1165/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5720 - acc: 0.7957 - val_loss: 1.1914 - val_acc: 0.6509\n",
      "Epoch 1166/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5640 - acc: 0.7988 - val_loss: 1.4482 - val_acc: 0.6170\n",
      "Epoch 1167/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5632 - acc: 0.7970 - val_loss: 1.4593 - val_acc: 0.6062\n",
      "Epoch 1168/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5697 - acc: 0.7984 - val_loss: 1.4684 - val_acc: 0.5759\n",
      "Epoch 1169/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5715 - acc: 0.7975 - val_loss: 1.4159 - val_acc: 0.5759\n",
      "Epoch 1170/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5755 - acc: 0.7982 - val_loss: 1.4687 - val_acc: 0.5848\n",
      "Epoch 1171/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5630 - acc: 0.7979 - val_loss: 1.1164 - val_acc: 0.6723\n",
      "Epoch 1172/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5527 - acc: 0.8059 - val_loss: 1.3863 - val_acc: 0.6313\n",
      "Epoch 1173/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5657 - acc: 0.7986 - val_loss: 1.1143 - val_acc: 0.6357\n",
      "Epoch 1174/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5495 - acc: 0.8013 - val_loss: 1.7352 - val_acc: 0.5455\n",
      "Epoch 1175/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5794 - acc: 0.7992 - val_loss: 1.4346 - val_acc: 0.5795\n",
      "Epoch 1176/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5611 - acc: 0.7975 - val_loss: 1.6858 - val_acc: 0.5545\n",
      "Epoch 1177/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5624 - acc: 0.8010 - val_loss: 1.2907 - val_acc: 0.6125\n",
      "Epoch 1178/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5607 - acc: 0.7978 - val_loss: 1.1408 - val_acc: 0.6554\n",
      "Epoch 1179/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5352 - acc: 0.8074 - val_loss: 1.3875 - val_acc: 0.5616\n",
      "Epoch 1180/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5541 - acc: 0.8022 - val_loss: 1.5855 - val_acc: 0.5759\n",
      "Epoch 1181/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5743 - acc: 0.7966 - val_loss: 1.1562 - val_acc: 0.6750\n",
      "Epoch 1182/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5466 - acc: 0.8031 - val_loss: 1.2802 - val_acc: 0.6705\n",
      "Epoch 1183/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5693 - acc: 0.7985 - val_loss: 1.2522 - val_acc: 0.6330\n",
      "Epoch 1184/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5608 - acc: 0.7994 - val_loss: 1.4826 - val_acc: 0.6054\n",
      "Epoch 1185/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5610 - acc: 0.8012 - val_loss: 1.5073 - val_acc: 0.5830\n",
      "Epoch 1186/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5661 - acc: 0.7980 - val_loss: 1.1397 - val_acc: 0.6437\n",
      "Epoch 1187/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5461 - acc: 0.8019 - val_loss: 1.2007 - val_acc: 0.6571\n",
      "Epoch 1188/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5674 - acc: 0.8016 - val_loss: 1.3117 - val_acc: 0.6196\n",
      "Epoch 1189/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5567 - acc: 0.8031 - val_loss: 1.2532 - val_acc: 0.6473\n",
      "Epoch 1190/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5457 - acc: 0.8064 - val_loss: 1.1691 - val_acc: 0.6223\n",
      "Epoch 1191/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5608 - acc: 0.7994 - val_loss: 1.2705 - val_acc: 0.6384\n",
      "Epoch 1192/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5497 - acc: 0.8027 - val_loss: 1.4568 - val_acc: 0.6286\n",
      "Epoch 1193/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5616 - acc: 0.8014 - val_loss: 1.3202 - val_acc: 0.6063\n",
      "Epoch 1194/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5623 - acc: 0.7982 - val_loss: 1.1973 - val_acc: 0.6768\n",
      "Epoch 1195/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5538 - acc: 0.8020 - val_loss: 1.3568 - val_acc: 0.6080\n",
      "Epoch 1196/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5648 - acc: 0.8001 - val_loss: 1.6555 - val_acc: 0.5250\n",
      "Epoch 1197/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5889 - acc: 0.7918 - val_loss: 1.1989 - val_acc: 0.6491\n",
      "Epoch 1198/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5609 - acc: 0.8010 - val_loss: 1.2579 - val_acc: 0.6250\n",
      "Epoch 1199/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5485 - acc: 0.8052 - val_loss: 1.2744 - val_acc: 0.6295\n",
      "Epoch 1200/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5724 - acc: 0.7944 - val_loss: 1.3607 - val_acc: 0.5768\n",
      "Epoch 1201/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5569 - acc: 0.8022 - val_loss: 1.3534 - val_acc: 0.6071\n",
      "Epoch 1202/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5660 - acc: 0.7979 - val_loss: 1.6227 - val_acc: 0.5750\n",
      "Epoch 1203/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5519 - acc: 0.8039 - val_loss: 1.1113 - val_acc: 0.6518\n",
      "Epoch 1204/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5320 - acc: 0.8092 - val_loss: 1.4702 - val_acc: 0.5911\n",
      "Epoch 1205/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5748 - acc: 0.7950 - val_loss: 1.3484 - val_acc: 0.6455\n",
      "Epoch 1206/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5517 - acc: 0.8022 - val_loss: 1.3003 - val_acc: 0.6634\n",
      "Epoch 1207/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5359 - acc: 0.8088 - val_loss: 1.1710 - val_acc: 0.6857\n",
      "Epoch 1208/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5484 - acc: 0.8036 - val_loss: 1.5674 - val_acc: 0.5438\n",
      "Epoch 1209/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5630 - acc: 0.8004 - val_loss: 1.3817 - val_acc: 0.6214\n",
      "Epoch 1210/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5567 - acc: 0.8056 - val_loss: 1.2653 - val_acc: 0.6393\n",
      "Epoch 1211/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5500 - acc: 0.8035 - val_loss: 1.1575 - val_acc: 0.6518\n",
      "Epoch 1212/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5490 - acc: 0.8022 - val_loss: 1.2344 - val_acc: 0.6554\n",
      "Epoch 1213/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5464 - acc: 0.8062 - val_loss: 1.5595 - val_acc: 0.6080\n",
      "Epoch 1214/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5635 - acc: 0.7980 - val_loss: 1.2569 - val_acc: 0.6089\n",
      "Epoch 1215/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5634 - acc: 0.7978 - val_loss: 1.3308 - val_acc: 0.6054\n",
      "Epoch 1216/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5559 - acc: 0.7994 - val_loss: 1.4167 - val_acc: 0.6143\n",
      "Epoch 1217/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5492 - acc: 0.8037 - val_loss: 1.1796 - val_acc: 0.6259\n",
      "Epoch 1218/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5636 - acc: 0.8016 - val_loss: 1.3647 - val_acc: 0.6304\n",
      "Epoch 1219/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5535 - acc: 0.8044 - val_loss: 1.2619 - val_acc: 0.6536\n",
      "Epoch 1220/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5385 - acc: 0.8063 - val_loss: 1.4565 - val_acc: 0.5723\n",
      "Epoch 1221/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5640 - acc: 0.7997 - val_loss: 1.7300 - val_acc: 0.5357\n",
      "Epoch 1222/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5743 - acc: 0.8000 - val_loss: 1.2903 - val_acc: 0.6411\n",
      "Epoch 1223/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5518 - acc: 0.8026 - val_loss: 1.4133 - val_acc: 0.5929\n",
      "Epoch 1224/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5502 - acc: 0.8033 - val_loss: 1.3337 - val_acc: 0.6348\n",
      "Epoch 1225/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5483 - acc: 0.8036 - val_loss: 1.3460 - val_acc: 0.6027\n",
      "Epoch 1226/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5546 - acc: 0.8044 - val_loss: 1.0736 - val_acc: 0.6768\n",
      "Epoch 1227/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5559 - acc: 0.8013 - val_loss: 1.2933 - val_acc: 0.6545\n",
      "Epoch 1228/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5432 - acc: 0.8063 - val_loss: 1.3449 - val_acc: 0.6330\n",
      "Epoch 1229/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5536 - acc: 0.8006 - val_loss: 1.2551 - val_acc: 0.5929\n",
      "Epoch 1230/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5559 - acc: 0.7994 - val_loss: 1.2351 - val_acc: 0.6366\n",
      "Epoch 1231/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5511 - acc: 0.8031 - val_loss: 1.8461 - val_acc: 0.5571\n",
      "Epoch 1232/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5570 - acc: 0.8007 - val_loss: 1.0952 - val_acc: 0.7009\n",
      "Epoch 1233/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5543 - acc: 0.7998 - val_loss: 1.5730 - val_acc: 0.5830\n",
      "Epoch 1234/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5475 - acc: 0.8044 - val_loss: 1.3292 - val_acc: 0.6714\n",
      "Epoch 1235/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5436 - acc: 0.8082 - val_loss: 1.5020 - val_acc: 0.5705\n",
      "Epoch 1236/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5526 - acc: 0.8037 - val_loss: 1.1624 - val_acc: 0.6643\n",
      "Epoch 1237/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5402 - acc: 0.8096 - val_loss: 1.5684 - val_acc: 0.6018\n",
      "Epoch 1238/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5550 - acc: 0.8056 - val_loss: 1.4580 - val_acc: 0.5946\n",
      "Epoch 1239/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5444 - acc: 0.8072 - val_loss: 1.1265 - val_acc: 0.6723\n",
      "Epoch 1240/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5440 - acc: 0.8011 - val_loss: 1.1980 - val_acc: 0.6429\n",
      "Epoch 1241/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5425 - acc: 0.8071 - val_loss: 1.3269 - val_acc: 0.6393\n",
      "Epoch 1242/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5532 - acc: 0.8045 - val_loss: 1.1331 - val_acc: 0.6830\n",
      "Epoch 1243/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5495 - acc: 0.8028 - val_loss: 1.5109 - val_acc: 0.5786\n",
      "Epoch 1244/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5588 - acc: 0.8006 - val_loss: 1.2390 - val_acc: 0.6643\n",
      "Epoch 1245/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5381 - acc: 0.8102 - val_loss: 1.2606 - val_acc: 0.6893\n",
      "Epoch 1246/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5411 - acc: 0.8090 - val_loss: 1.7828 - val_acc: 0.5446\n",
      "Epoch 1247/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5625 - acc: 0.8042 - val_loss: 1.3384 - val_acc: 0.6554\n",
      "Epoch 1248/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5453 - acc: 0.8038 - val_loss: 1.4193 - val_acc: 0.6241\n",
      "Epoch 1249/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5417 - acc: 0.8090 - val_loss: 1.4354 - val_acc: 0.5929\n",
      "Epoch 1250/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5600 - acc: 0.7992 - val_loss: 1.3748 - val_acc: 0.6152\n",
      "Epoch 1251/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5406 - acc: 0.8087 - val_loss: 1.4047 - val_acc: 0.5893\n",
      "Epoch 1252/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5525 - acc: 0.8049 - val_loss: 1.2120 - val_acc: 0.6714\n",
      "Epoch 1253/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5582 - acc: 0.8014 - val_loss: 1.4170 - val_acc: 0.6054\n",
      "Epoch 1254/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5456 - acc: 0.8077 - val_loss: 1.2782 - val_acc: 0.6411\n",
      "Epoch 1255/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5607 - acc: 0.7948 - val_loss: 1.1641 - val_acc: 0.6205\n",
      "Epoch 1256/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5578 - acc: 0.8002 - val_loss: 1.2429 - val_acc: 0.6223\n",
      "Epoch 1257/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5494 - acc: 0.8029 - val_loss: 1.7165 - val_acc: 0.5437\n",
      "Epoch 1258/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5537 - acc: 0.8043 - val_loss: 1.2086 - val_acc: 0.6616\n",
      "Epoch 1259/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5450 - acc: 0.8061 - val_loss: 1.6065 - val_acc: 0.5812\n",
      "Epoch 1260/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5443 - acc: 0.8059 - val_loss: 1.3887 - val_acc: 0.6554\n",
      "Epoch 1261/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5514 - acc: 0.8040 - val_loss: 1.3790 - val_acc: 0.6312\n",
      "Epoch 1262/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5327 - acc: 0.8094 - val_loss: 1.4383 - val_acc: 0.5973\n",
      "Epoch 1263/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5474 - acc: 0.8026 - val_loss: 1.5088 - val_acc: 0.6170\n",
      "Epoch 1264/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5438 - acc: 0.8070 - val_loss: 1.0544 - val_acc: 0.6714\n",
      "Epoch 1265/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5492 - acc: 0.8012 - val_loss: 1.1899 - val_acc: 0.6223\n",
      "Epoch 1266/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5415 - acc: 0.8018 - val_loss: 1.2771 - val_acc: 0.6446\n",
      "Epoch 1267/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5595 - acc: 0.8028 - val_loss: 1.3909 - val_acc: 0.6143\n",
      "Epoch 1268/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5374 - acc: 0.8093 - val_loss: 1.2447 - val_acc: 0.6384\n",
      "Epoch 1269/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5492 - acc: 0.8020 - val_loss: 1.5284 - val_acc: 0.5911\n",
      "Epoch 1270/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5423 - acc: 0.8033 - val_loss: 1.1492 - val_acc: 0.6473\n",
      "Epoch 1271/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5446 - acc: 0.8031 - val_loss: 1.4574 - val_acc: 0.6187\n",
      "Epoch 1272/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5380 - acc: 0.8084 - val_loss: 1.0805 - val_acc: 0.6330\n",
      "Epoch 1273/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5591 - acc: 0.8001 - val_loss: 1.4514 - val_acc: 0.6027\n",
      "Epoch 1274/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5519 - acc: 0.8042 - val_loss: 1.3348 - val_acc: 0.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1275/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5351 - acc: 0.8096 - val_loss: 1.4555 - val_acc: 0.6250\n",
      "Epoch 1276/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5365 - acc: 0.8082 - val_loss: 1.3995 - val_acc: 0.6277\n",
      "Epoch 1277/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5375 - acc: 0.8087 - val_loss: 1.3678 - val_acc: 0.6080\n",
      "Epoch 1278/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5463 - acc: 0.8098 - val_loss: 1.2858 - val_acc: 0.6134\n",
      "Epoch 1279/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5643 - acc: 0.8017 - val_loss: 1.2054 - val_acc: 0.6527\n",
      "Epoch 1280/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5460 - acc: 0.8068 - val_loss: 1.4680 - val_acc: 0.5705\n",
      "Epoch 1281/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5510 - acc: 0.8050 - val_loss: 1.4935 - val_acc: 0.6045\n",
      "Epoch 1282/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5364 - acc: 0.8114 - val_loss: 1.2104 - val_acc: 0.6393\n",
      "Epoch 1283/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5384 - acc: 0.8057 - val_loss: 1.6818 - val_acc: 0.5696\n",
      "Epoch 1284/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5627 - acc: 0.8012 - val_loss: 1.6697 - val_acc: 0.5598\n",
      "Epoch 1285/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5587 - acc: 0.8048 - val_loss: 1.1780 - val_acc: 0.6750\n",
      "Epoch 1286/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5360 - acc: 0.8060 - val_loss: 1.2355 - val_acc: 0.6268\n",
      "Epoch 1287/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5286 - acc: 0.8075 - val_loss: 1.2585 - val_acc: 0.6446\n",
      "Epoch 1288/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5402 - acc: 0.8054 - val_loss: 1.2816 - val_acc: 0.6429\n",
      "Epoch 1289/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5421 - acc: 0.8062 - val_loss: 1.2984 - val_acc: 0.6045\n",
      "Epoch 1290/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5370 - acc: 0.8094 - val_loss: 1.3796 - val_acc: 0.6491\n",
      "Epoch 1291/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5313 - acc: 0.8100 - val_loss: 1.1076 - val_acc: 0.6875\n",
      "Epoch 1292/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5275 - acc: 0.8115 - val_loss: 1.2220 - val_acc: 0.6634\n",
      "Epoch 1293/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5436 - acc: 0.8032 - val_loss: 1.2493 - val_acc: 0.6420\n",
      "Epoch 1294/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5468 - acc: 0.8028 - val_loss: 1.2854 - val_acc: 0.6196\n",
      "Epoch 1295/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5323 - acc: 0.8114 - val_loss: 1.2686 - val_acc: 0.6348\n",
      "Epoch 1296/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5307 - acc: 0.8097 - val_loss: 1.3811 - val_acc: 0.6232\n",
      "Epoch 1297/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5450 - acc: 0.8030 - val_loss: 1.3931 - val_acc: 0.5902\n",
      "Epoch 1298/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5533 - acc: 0.8022 - val_loss: 1.2763 - val_acc: 0.6170\n",
      "Epoch 1299/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5274 - acc: 0.8117 - val_loss: 1.2150 - val_acc: 0.6348\n",
      "Epoch 1300/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5394 - acc: 0.8052 - val_loss: 1.4107 - val_acc: 0.5938\n",
      "Epoch 1301/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5368 - acc: 0.8071 - val_loss: 1.4472 - val_acc: 0.6464\n",
      "Epoch 1302/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5407 - acc: 0.8094 - val_loss: 1.1676 - val_acc: 0.6500\n",
      "Epoch 1303/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5240 - acc: 0.8131 - val_loss: 1.4777 - val_acc: 0.6348\n",
      "Epoch 1304/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5379 - acc: 0.8064 - val_loss: 1.2646 - val_acc: 0.6786\n",
      "Epoch 1305/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.5297 - acc: 0.8102 - val_loss: 1.2201 - val_acc: 0.6687\n",
      "Epoch 1306/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5371 - acc: 0.8080 - val_loss: 1.3563 - val_acc: 0.6384\n",
      "Epoch 1307/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5406 - acc: 0.8073 - val_loss: 1.2744 - val_acc: 0.6536\n",
      "Epoch 1308/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5216 - acc: 0.8139 - val_loss: 1.1891 - val_acc: 0.6661\n",
      "Epoch 1309/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5407 - acc: 0.8084 - val_loss: 1.0838 - val_acc: 0.6741\n",
      "Epoch 1310/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5225 - acc: 0.8142 - val_loss: 1.3539 - val_acc: 0.6027\n",
      "Epoch 1311/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5419 - acc: 0.8088 - val_loss: 1.3520 - val_acc: 0.6125\n",
      "Epoch 1312/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5427 - acc: 0.8057 - val_loss: 1.4860 - val_acc: 0.6250\n",
      "Epoch 1313/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5601 - acc: 0.8050 - val_loss: 1.3513 - val_acc: 0.6268\n",
      "Epoch 1314/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5365 - acc: 0.8054 - val_loss: 1.3241 - val_acc: 0.6312\n",
      "Epoch 1315/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5343 - acc: 0.8067 - val_loss: 1.0444 - val_acc: 0.6893\n",
      "Epoch 1316/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5216 - acc: 0.8124 - val_loss: 1.4487 - val_acc: 0.5991\n",
      "Epoch 1317/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5643 - acc: 0.7968 - val_loss: 1.4574 - val_acc: 0.5714\n",
      "Epoch 1318/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5177 - acc: 0.8130 - val_loss: 1.2634 - val_acc: 0.6473\n",
      "Epoch 1319/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5405 - acc: 0.8059 - val_loss: 1.4486 - val_acc: 0.5955\n",
      "Epoch 1320/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5245 - acc: 0.8134 - val_loss: 1.2125 - val_acc: 0.6214\n",
      "Epoch 1321/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5370 - acc: 0.8097 - val_loss: 1.0423 - val_acc: 0.6750\n",
      "Epoch 1322/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5291 - acc: 0.8072 - val_loss: 1.2510 - val_acc: 0.6304\n",
      "Epoch 1323/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5398 - acc: 0.8059 - val_loss: 1.3062 - val_acc: 0.6214\n",
      "Epoch 1324/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5252 - acc: 0.8090 - val_loss: 1.2775 - val_acc: 0.6482\n",
      "Epoch 1325/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5187 - acc: 0.8154 - val_loss: 1.5916 - val_acc: 0.5696\n",
      "Epoch 1326/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5418 - acc: 0.8093 - val_loss: 1.3610 - val_acc: 0.6661\n",
      "Epoch 1327/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5238 - acc: 0.8127 - val_loss: 1.3318 - val_acc: 0.6143\n",
      "Epoch 1328/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5300 - acc: 0.8124 - val_loss: 1.2365 - val_acc: 0.6750\n",
      "Epoch 1329/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 0.5336 - acc: 0.8092 - val_loss: 1.6929 - val_acc: 0.5723\n",
      "Epoch 1330/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5356 - acc: 0.8134 - val_loss: 1.4044 - val_acc: 0.6348\n",
      "Epoch 1331/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5372 - acc: 0.8106 - val_loss: 1.9175 - val_acc: 0.5643\n",
      "Epoch 1332/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5363 - acc: 0.8143 - val_loss: 1.7688 - val_acc: 0.5455\n",
      "Epoch 1333/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5366 - acc: 0.8128 - val_loss: 2.0270 - val_acc: 0.5250\n",
      "Epoch 1334/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5429 - acc: 0.8087 - val_loss: 1.8330 - val_acc: 0.5464\n",
      "Epoch 1335/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5526 - acc: 0.8082 - val_loss: 1.2116 - val_acc: 0.6500\n",
      "Epoch 1336/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5350 - acc: 0.8113 - val_loss: 1.2240 - val_acc: 0.6250\n",
      "Epoch 1337/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5456 - acc: 0.8024 - val_loss: 1.0947 - val_acc: 0.6384\n",
      "Epoch 1338/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5383 - acc: 0.8068 - val_loss: 1.2810 - val_acc: 0.6277\n",
      "Epoch 1339/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5413 - acc: 0.8076 - val_loss: 1.5147 - val_acc: 0.6036\n",
      "Epoch 1340/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5460 - acc: 0.8048 - val_loss: 1.3986 - val_acc: 0.5795\n",
      "Epoch 1341/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5292 - acc: 0.8134 - val_loss: 1.5825 - val_acc: 0.5750\n",
      "Epoch 1342/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5418 - acc: 0.8050 - val_loss: 1.2186 - val_acc: 0.6313\n",
      "Epoch 1343/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5235 - acc: 0.8140 - val_loss: 1.5717 - val_acc: 0.5795\n",
      "Epoch 1344/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5381 - acc: 0.8066 - val_loss: 1.3269 - val_acc: 0.6384\n",
      "Epoch 1345/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5304 - acc: 0.8132 - val_loss: 1.6097 - val_acc: 0.5821\n",
      "Epoch 1346/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5349 - acc: 0.8122 - val_loss: 1.5240 - val_acc: 0.6089\n",
      "Epoch 1347/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5378 - acc: 0.8140 - val_loss: 1.7096 - val_acc: 0.5295\n",
      "Epoch 1348/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5443 - acc: 0.8074 - val_loss: 1.1583 - val_acc: 0.6812\n",
      "Epoch 1349/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5054 - acc: 0.8191 - val_loss: 1.3950 - val_acc: 0.6036\n",
      "Epoch 1350/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5417 - acc: 0.8056 - val_loss: 1.3895 - val_acc: 0.6107\n",
      "Epoch 1351/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5431 - acc: 0.8046 - val_loss: 1.5122 - val_acc: 0.6098\n",
      "Epoch 1352/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5287 - acc: 0.8128 - val_loss: 1.5076 - val_acc: 0.5920\n",
      "Epoch 1353/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5296 - acc: 0.8110 - val_loss: 1.3462 - val_acc: 0.6223\n",
      "Epoch 1354/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5254 - acc: 0.8130 - val_loss: 1.0828 - val_acc: 0.6866\n",
      "Epoch 1355/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5201 - acc: 0.8122 - val_loss: 1.8808 - val_acc: 0.5295\n",
      "Epoch 1356/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5284 - acc: 0.8158 - val_loss: 1.2438 - val_acc: 0.6384\n",
      "Epoch 1357/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5186 - acc: 0.8126 - val_loss: 1.2465 - val_acc: 0.6304\n",
      "Epoch 1358/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5270 - acc: 0.8135 - val_loss: 1.2767 - val_acc: 0.6420\n",
      "Epoch 1359/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5138 - acc: 0.8201 - val_loss: 1.1633 - val_acc: 0.6741\n",
      "Epoch 1360/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5352 - acc: 0.8103 - val_loss: 1.7069 - val_acc: 0.5670\n",
      "Epoch 1361/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5394 - acc: 0.8079 - val_loss: 1.2655 - val_acc: 0.5982\n",
      "Epoch 1362/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5418 - acc: 0.8092 - val_loss: 1.4459 - val_acc: 0.5821\n",
      "Epoch 1363/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5298 - acc: 0.8100 - val_loss: 1.9379 - val_acc: 0.5348\n",
      "Epoch 1364/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5564 - acc: 0.8050 - val_loss: 1.2598 - val_acc: 0.6616\n",
      "Epoch 1365/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5215 - acc: 0.8175 - val_loss: 1.1361 - val_acc: 0.6821\n",
      "Epoch 1366/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5278 - acc: 0.8101 - val_loss: 1.8324 - val_acc: 0.5893\n",
      "Epoch 1367/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5273 - acc: 0.8136 - val_loss: 1.1806 - val_acc: 0.6661\n",
      "Epoch 1368/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5248 - acc: 0.8143 - val_loss: 1.2103 - val_acc: 0.6643\n",
      "Epoch 1369/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5216 - acc: 0.8135 - val_loss: 1.3434 - val_acc: 0.6232\n",
      "Epoch 1370/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5363 - acc: 0.8068 - val_loss: 1.9988 - val_acc: 0.5786\n",
      "Epoch 1371/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5469 - acc: 0.8124 - val_loss: 1.1218 - val_acc: 0.6607\n",
      "Epoch 1372/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5264 - acc: 0.8126 - val_loss: 1.3111 - val_acc: 0.6393\n",
      "Epoch 1373/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5217 - acc: 0.8146 - val_loss: 1.1045 - val_acc: 0.6661\n",
      "Epoch 1374/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5220 - acc: 0.8109 - val_loss: 1.1939 - val_acc: 0.6625\n",
      "Epoch 1375/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5187 - acc: 0.8142 - val_loss: 1.6481 - val_acc: 0.6000\n",
      "Epoch 1376/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5529 - acc: 0.8075 - val_loss: 1.3186 - val_acc: 0.6446\n",
      "Epoch 1377/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5263 - acc: 0.8105 - val_loss: 1.4431 - val_acc: 0.5991\n",
      "Epoch 1378/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5254 - acc: 0.8117 - val_loss: 1.3033 - val_acc: 0.6259\n",
      "Epoch 1379/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5166 - acc: 0.8146 - val_loss: 1.3841 - val_acc: 0.6152\n",
      "Epoch 1380/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5277 - acc: 0.8142 - val_loss: 1.4466 - val_acc: 0.6348\n",
      "Epoch 1381/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5237 - acc: 0.8129 - val_loss: 1.4065 - val_acc: 0.6304\n",
      "Epoch 1382/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5158 - acc: 0.8142 - val_loss: 1.5884 - val_acc: 0.5759\n",
      "Epoch 1383/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5479 - acc: 0.8038 - val_loss: 1.2168 - val_acc: 0.6286\n",
      "Epoch 1384/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5168 - acc: 0.8188 - val_loss: 1.3854 - val_acc: 0.6321\n",
      "Epoch 1385/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5204 - acc: 0.8126 - val_loss: 1.3762 - val_acc: 0.6482\n",
      "Epoch 1386/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5319 - acc: 0.8079 - val_loss: 1.1973 - val_acc: 0.6366\n",
      "Epoch 1387/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5287 - acc: 0.8072 - val_loss: 1.4553 - val_acc: 0.6134\n",
      "Epoch 1388/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5335 - acc: 0.8078 - val_loss: 1.6910 - val_acc: 0.5973\n",
      "Epoch 1389/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5221 - acc: 0.8170 - val_loss: 1.4052 - val_acc: 0.6330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1390/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5235 - acc: 0.8145 - val_loss: 1.1301 - val_acc: 0.6786\n",
      "Epoch 1391/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5036 - acc: 0.8202 - val_loss: 1.2508 - val_acc: 0.6473\n",
      "Epoch 1392/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5271 - acc: 0.8086 - val_loss: 1.1741 - val_acc: 0.6455\n",
      "Epoch 1393/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5268 - acc: 0.8072 - val_loss: 1.2643 - val_acc: 0.6304\n",
      "Epoch 1394/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5207 - acc: 0.8172 - val_loss: 1.3131 - val_acc: 0.6187\n",
      "Epoch 1395/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5200 - acc: 0.8141 - val_loss: 1.5401 - val_acc: 0.5893\n",
      "Epoch 1396/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5360 - acc: 0.8127 - val_loss: 1.6540 - val_acc: 0.5946\n",
      "Epoch 1397/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5328 - acc: 0.8118 - val_loss: 1.1217 - val_acc: 0.6554\n",
      "Epoch 1398/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5236 - acc: 0.8140 - val_loss: 1.2503 - val_acc: 0.6580\n",
      "Epoch 1399/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5142 - acc: 0.8142 - val_loss: 1.6041 - val_acc: 0.5964\n",
      "Epoch 1400/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5477 - acc: 0.8099 - val_loss: 1.1176 - val_acc: 0.6893\n",
      "Epoch 1401/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5194 - acc: 0.8148 - val_loss: 1.5451 - val_acc: 0.5821\n",
      "Epoch 1402/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5202 - acc: 0.8184 - val_loss: 1.5963 - val_acc: 0.6098\n",
      "Epoch 1403/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5291 - acc: 0.8141 - val_loss: 1.3763 - val_acc: 0.6134\n",
      "Epoch 1404/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5204 - acc: 0.8112 - val_loss: 1.1767 - val_acc: 0.6455\n",
      "Epoch 1405/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5255 - acc: 0.8135 - val_loss: 1.4028 - val_acc: 0.6321\n",
      "Epoch 1406/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5206 - acc: 0.8135 - val_loss: 1.4050 - val_acc: 0.6437\n",
      "Epoch 1407/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5117 - acc: 0.8165 - val_loss: 1.2092 - val_acc: 0.6688\n",
      "Epoch 1408/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5053 - acc: 0.8182 - val_loss: 1.1706 - val_acc: 0.6679\n",
      "Epoch 1409/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5051 - acc: 0.8178 - val_loss: 1.4802 - val_acc: 0.6116\n",
      "Epoch 1410/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5132 - acc: 0.8180 - val_loss: 1.0786 - val_acc: 0.6768\n",
      "Epoch 1411/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5298 - acc: 0.8106 - val_loss: 1.0806 - val_acc: 0.7000\n",
      "Epoch 1412/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5150 - acc: 0.8124 - val_loss: 1.1557 - val_acc: 0.6563\n",
      "Epoch 1413/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5134 - acc: 0.8125 - val_loss: 1.3482 - val_acc: 0.6125\n",
      "Epoch 1414/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5287 - acc: 0.8113 - val_loss: 1.4150 - val_acc: 0.6036\n",
      "Epoch 1415/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5239 - acc: 0.8150 - val_loss: 1.3146 - val_acc: 0.6241\n",
      "Epoch 1416/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5253 - acc: 0.8122 - val_loss: 1.3486 - val_acc: 0.6446\n",
      "Epoch 1417/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5141 - acc: 0.8172 - val_loss: 1.6048 - val_acc: 0.5982\n",
      "Epoch 1418/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5215 - acc: 0.8169 - val_loss: 1.3718 - val_acc: 0.6348\n",
      "Epoch 1419/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5333 - acc: 0.8120 - val_loss: 1.1180 - val_acc: 0.6759\n",
      "Epoch 1420/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4972 - acc: 0.8226 - val_loss: 1.1223 - val_acc: 0.6759\n",
      "Epoch 1421/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5200 - acc: 0.8112 - val_loss: 1.3132 - val_acc: 0.6911\n",
      "Epoch 1422/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5123 - acc: 0.8184 - val_loss: 1.2959 - val_acc: 0.6250\n",
      "Epoch 1423/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5279 - acc: 0.8130 - val_loss: 1.2653 - val_acc: 0.6750\n",
      "Epoch 1424/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5064 - acc: 0.8179 - val_loss: 1.3121 - val_acc: 0.6491\n",
      "Epoch 1425/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5086 - acc: 0.8196 - val_loss: 1.4859 - val_acc: 0.6107\n",
      "Epoch 1426/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5087 - acc: 0.8197 - val_loss: 1.1999 - val_acc: 0.6554\n",
      "Epoch 1427/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5312 - acc: 0.8096 - val_loss: 1.2387 - val_acc: 0.6214\n",
      "Epoch 1428/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5168 - acc: 0.8151 - val_loss: 1.1794 - val_acc: 0.6571\n",
      "Epoch 1429/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.5393 - acc: 0.8099 - val_loss: 1.3747 - val_acc: 0.6277\n",
      "Epoch 1430/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5207 - acc: 0.8126 - val_loss: 1.1651 - val_acc: 0.6759\n",
      "Epoch 1431/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5116 - acc: 0.8168 - val_loss: 1.4588 - val_acc: 0.6286\n",
      "Epoch 1432/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5165 - acc: 0.8108 - val_loss: 1.1577 - val_acc: 0.6741\n",
      "Epoch 1433/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5217 - acc: 0.8144 - val_loss: 1.6285 - val_acc: 0.5848\n",
      "Epoch 1434/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5423 - acc: 0.8116 - val_loss: 1.6757 - val_acc: 0.5777\n",
      "Epoch 1435/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5249 - acc: 0.8119 - val_loss: 1.1937 - val_acc: 0.6598\n",
      "Epoch 1436/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5207 - acc: 0.8125 - val_loss: 1.2419 - val_acc: 0.6357\n",
      "Epoch 1437/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5093 - acc: 0.8130 - val_loss: 1.3647 - val_acc: 0.6402\n",
      "Epoch 1438/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5049 - acc: 0.8198 - val_loss: 1.2596 - val_acc: 0.6634\n",
      "Epoch 1439/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5191 - acc: 0.8138 - val_loss: 1.4821 - val_acc: 0.6196\n",
      "Epoch 1440/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5142 - acc: 0.8118 - val_loss: 1.2080 - val_acc: 0.6196\n",
      "Epoch 1441/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.5174 - acc: 0.8111 - val_loss: 1.2169 - val_acc: 0.6500\n",
      "Epoch 1442/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5179 - acc: 0.8150 - val_loss: 1.3200 - val_acc: 0.6295\n",
      "Epoch 1443/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5105 - acc: 0.8193 - val_loss: 1.2175 - val_acc: 0.6571\n",
      "Epoch 1444/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5150 - acc: 0.8151 - val_loss: 1.1962 - val_acc: 0.6696\n",
      "Epoch 1445/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5125 - acc: 0.8205 - val_loss: 1.7511 - val_acc: 0.5518\n",
      "Epoch 1446/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5495 - acc: 0.8055 - val_loss: 1.4782 - val_acc: 0.6188\n",
      "Epoch 1447/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5097 - acc: 0.8210 - val_loss: 1.4054 - val_acc: 0.6402\n",
      "Epoch 1448/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5046 - acc: 0.8182 - val_loss: 1.1245 - val_acc: 0.6938\n",
      "Epoch 1449/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4968 - acc: 0.8190 - val_loss: 1.1131 - val_acc: 0.6857\n",
      "Epoch 1450/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5253 - acc: 0.8150 - val_loss: 1.2462 - val_acc: 0.6902\n",
      "Epoch 1451/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4932 - acc: 0.8220 - val_loss: 1.3973 - val_acc: 0.6063\n",
      "Epoch 1452/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5182 - acc: 0.8148 - val_loss: 1.4368 - val_acc: 0.6009\n",
      "Epoch 1453/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5049 - acc: 0.8166 - val_loss: 1.3005 - val_acc: 0.6259\n",
      "Epoch 1454/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5014 - acc: 0.8209 - val_loss: 1.0957 - val_acc: 0.6946\n",
      "Epoch 1455/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5089 - acc: 0.8214 - val_loss: 1.4336 - val_acc: 0.6062\n",
      "Epoch 1456/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5091 - acc: 0.8193 - val_loss: 1.4471 - val_acc: 0.6214\n",
      "Epoch 1457/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5077 - acc: 0.8188 - val_loss: 1.5117 - val_acc: 0.5571\n",
      "Epoch 1458/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5259 - acc: 0.8092 - val_loss: 1.2163 - val_acc: 0.6384\n",
      "Epoch 1459/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5073 - acc: 0.8212 - val_loss: 1.4596 - val_acc: 0.6375\n",
      "Epoch 1460/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5207 - acc: 0.8139 - val_loss: 1.0472 - val_acc: 0.6920\n",
      "Epoch 1461/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4904 - acc: 0.8222 - val_loss: 1.1785 - val_acc: 0.6366\n",
      "Epoch 1462/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5154 - acc: 0.8153 - val_loss: 1.3685 - val_acc: 0.6464\n",
      "Epoch 1463/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5102 - acc: 0.8177 - val_loss: 1.3844 - val_acc: 0.6366\n",
      "Epoch 1464/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5234 - acc: 0.8128 - val_loss: 1.3451 - val_acc: 0.6250\n",
      "Epoch 1465/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5065 - acc: 0.8160 - val_loss: 1.5745 - val_acc: 0.6286\n",
      "Epoch 1466/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5201 - acc: 0.8168 - val_loss: 1.4903 - val_acc: 0.6063\n",
      "Epoch 1467/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5284 - acc: 0.8154 - val_loss: 1.3169 - val_acc: 0.6295\n",
      "Epoch 1468/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4930 - acc: 0.8247 - val_loss: 1.2735 - val_acc: 0.6116\n",
      "Epoch 1469/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5207 - acc: 0.8142 - val_loss: 1.3016 - val_acc: 0.5848\n",
      "Epoch 1470/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5376 - acc: 0.8101 - val_loss: 1.0981 - val_acc: 0.6848\n",
      "Epoch 1471/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4968 - acc: 0.8221 - val_loss: 1.1120 - val_acc: 0.7071\n",
      "Epoch 1472/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5161 - acc: 0.8128 - val_loss: 1.4150 - val_acc: 0.6054\n",
      "Epoch 1473/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4975 - acc: 0.8204 - val_loss: 1.0330 - val_acc: 0.6938\n",
      "Epoch 1474/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5040 - acc: 0.8183 - val_loss: 1.4128 - val_acc: 0.5991\n",
      "Epoch 1475/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5172 - acc: 0.8185 - val_loss: 1.1865 - val_acc: 0.6652\n",
      "Epoch 1476/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5015 - acc: 0.8187 - val_loss: 1.3790 - val_acc: 0.6464\n",
      "Epoch 1477/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5156 - acc: 0.8140 - val_loss: 1.4023 - val_acc: 0.6330\n",
      "Epoch 1478/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5055 - acc: 0.8184 - val_loss: 1.3564 - val_acc: 0.6286\n",
      "Epoch 1479/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5333 - acc: 0.8109 - val_loss: 1.3005 - val_acc: 0.6804\n",
      "Epoch 1480/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5129 - acc: 0.8170 - val_loss: 1.6079 - val_acc: 0.5509\n",
      "Epoch 1481/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5318 - acc: 0.8166 - val_loss: 1.7762 - val_acc: 0.5589\n",
      "Epoch 1482/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5222 - acc: 0.8147 - val_loss: 1.3821 - val_acc: 0.6098\n",
      "Epoch 1483/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5108 - acc: 0.8192 - val_loss: 1.2186 - val_acc: 0.6946\n",
      "Epoch 1484/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4981 - acc: 0.8227 - val_loss: 1.5355 - val_acc: 0.6134\n",
      "Epoch 1485/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5197 - acc: 0.8144 - val_loss: 1.0935 - val_acc: 0.7000\n",
      "Epoch 1486/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5000 - acc: 0.8186 - val_loss: 1.3165 - val_acc: 0.6268\n",
      "Epoch 1487/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5022 - acc: 0.8219 - val_loss: 1.8729 - val_acc: 0.6018\n",
      "Epoch 1488/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5271 - acc: 0.8174 - val_loss: 1.4876 - val_acc: 0.6107\n",
      "Epoch 1489/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5192 - acc: 0.8213 - val_loss: 1.3338 - val_acc: 0.6598\n",
      "Epoch 1490/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5112 - acc: 0.8170 - val_loss: 1.2042 - val_acc: 0.6545\n",
      "Epoch 1491/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4944 - acc: 0.8232 - val_loss: 1.6001 - val_acc: 0.5804\n",
      "Epoch 1492/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5086 - acc: 0.8217 - val_loss: 1.2592 - val_acc: 0.6795\n",
      "Epoch 1493/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4984 - acc: 0.8230 - val_loss: 1.1898 - val_acc: 0.6634\n",
      "Epoch 1494/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5168 - acc: 0.8146 - val_loss: 1.2390 - val_acc: 0.6714\n",
      "Epoch 1495/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5030 - acc: 0.8174 - val_loss: 1.2830 - val_acc: 0.6554\n",
      "Epoch 1496/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4951 - acc: 0.8262 - val_loss: 1.5504 - val_acc: 0.6027\n",
      "Epoch 1497/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5158 - acc: 0.8167 - val_loss: 1.3837 - val_acc: 0.6098\n",
      "Epoch 1498/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5124 - acc: 0.8175 - val_loss: 1.3532 - val_acc: 0.6402\n",
      "Epoch 1499/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5004 - acc: 0.8199 - val_loss: 1.4051 - val_acc: 0.6018\n",
      "Epoch 1500/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5039 - acc: 0.8202 - val_loss: 1.8853 - val_acc: 0.5866\n",
      "Epoch 1501/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5370 - acc: 0.8161 - val_loss: 1.3343 - val_acc: 0.6223\n",
      "Epoch 1502/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5088 - acc: 0.8157 - val_loss: 1.3496 - val_acc: 0.6170\n",
      "Epoch 1503/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5039 - acc: 0.8230 - val_loss: 1.6703 - val_acc: 0.5705\n",
      "Epoch 1504/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.5077 - acc: 0.8208 - val_loss: 1.2723 - val_acc: 0.6661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1505/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5044 - acc: 0.8185 - val_loss: 1.3412 - val_acc: 0.6616\n",
      "Epoch 1506/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5147 - acc: 0.8177 - val_loss: 1.1231 - val_acc: 0.6982\n",
      "Epoch 1507/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4868 - acc: 0.8225 - val_loss: 1.6088 - val_acc: 0.6384\n",
      "Epoch 1508/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5072 - acc: 0.8242 - val_loss: 1.2242 - val_acc: 0.6527\n",
      "Epoch 1509/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4978 - acc: 0.8230 - val_loss: 1.4353 - val_acc: 0.6214\n",
      "Epoch 1510/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4992 - acc: 0.8227 - val_loss: 1.4958 - val_acc: 0.6402\n",
      "Epoch 1511/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5175 - acc: 0.8160 - val_loss: 1.8818 - val_acc: 0.6036\n",
      "Epoch 1512/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5055 - acc: 0.8238 - val_loss: 1.2232 - val_acc: 0.6643\n",
      "Epoch 1513/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5010 - acc: 0.8186 - val_loss: 1.2913 - val_acc: 0.6482\n",
      "Epoch 1514/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5144 - acc: 0.8180 - val_loss: 1.4497 - val_acc: 0.6054\n",
      "Epoch 1515/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5175 - acc: 0.8134 - val_loss: 1.1933 - val_acc: 0.6884\n",
      "Epoch 1516/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4990 - acc: 0.8236 - val_loss: 1.3642 - val_acc: 0.6277\n",
      "Epoch 1517/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4786 - acc: 0.8270 - val_loss: 1.2786 - val_acc: 0.6554\n",
      "Epoch 1518/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5131 - acc: 0.8205 - val_loss: 1.6143 - val_acc: 0.5982\n",
      "Epoch 1519/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5173 - acc: 0.8157 - val_loss: 1.1659 - val_acc: 0.6661\n",
      "Epoch 1520/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5123 - acc: 0.8204 - val_loss: 1.2368 - val_acc: 0.6509\n",
      "Epoch 1521/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5024 - acc: 0.8190 - val_loss: 1.5934 - val_acc: 0.5527\n",
      "Epoch 1522/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5124 - acc: 0.8204 - val_loss: 1.0570 - val_acc: 0.6696\n",
      "Epoch 1523/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4992 - acc: 0.8196 - val_loss: 1.1889 - val_acc: 0.6670\n",
      "Epoch 1524/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5217 - acc: 0.8134 - val_loss: 1.4651 - val_acc: 0.6321\n",
      "Epoch 1525/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4979 - acc: 0.8225 - val_loss: 1.1016 - val_acc: 0.6875\n",
      "Epoch 1526/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5033 - acc: 0.8191 - val_loss: 1.2740 - val_acc: 0.6589\n",
      "Epoch 1527/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5174 - acc: 0.8178 - val_loss: 1.1537 - val_acc: 0.6821\n",
      "Epoch 1528/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5054 - acc: 0.8176 - val_loss: 1.1414 - val_acc: 0.6679\n",
      "Epoch 1529/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5044 - acc: 0.8198 - val_loss: 1.2746 - val_acc: 0.6134\n",
      "Epoch 1530/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4993 - acc: 0.8230 - val_loss: 1.5931 - val_acc: 0.5786\n",
      "Epoch 1531/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5093 - acc: 0.8211 - val_loss: 1.1994 - val_acc: 0.6634\n",
      "Epoch 1532/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4805 - acc: 0.8281 - val_loss: 1.5522 - val_acc: 0.5938\n",
      "Epoch 1533/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5000 - acc: 0.8236 - val_loss: 1.6504 - val_acc: 0.6241\n",
      "Epoch 1534/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4962 - acc: 0.8253 - val_loss: 1.6430 - val_acc: 0.5821\n",
      "Epoch 1535/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5067 - acc: 0.8204 - val_loss: 1.1571 - val_acc: 0.6768\n",
      "Epoch 1536/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4835 - acc: 0.8272 - val_loss: 1.2169 - val_acc: 0.6580\n",
      "Epoch 1537/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5130 - acc: 0.8169 - val_loss: 1.2553 - val_acc: 0.6750\n",
      "Epoch 1538/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5036 - acc: 0.8234 - val_loss: 1.1635 - val_acc: 0.6777\n",
      "Epoch 1539/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4968 - acc: 0.8216 - val_loss: 1.4581 - val_acc: 0.6321\n",
      "Epoch 1540/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5051 - acc: 0.8216 - val_loss: 1.6197 - val_acc: 0.5911\n",
      "Epoch 1541/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5148 - acc: 0.8182 - val_loss: 1.0715 - val_acc: 0.6875\n",
      "Epoch 1542/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4792 - acc: 0.8291 - val_loss: 1.4515 - val_acc: 0.6080\n",
      "Epoch 1543/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.5195 - acc: 0.8143 - val_loss: 1.1206 - val_acc: 0.6795\n",
      "Epoch 1544/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4897 - acc: 0.8265 - val_loss: 1.2315 - val_acc: 0.6625\n",
      "Epoch 1545/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4903 - acc: 0.8226 - val_loss: 1.2007 - val_acc: 0.6554\n",
      "Epoch 1546/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4963 - acc: 0.8230 - val_loss: 1.1273 - val_acc: 0.6652\n",
      "Epoch 1547/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4940 - acc: 0.8251 - val_loss: 1.1405 - val_acc: 0.6768\n",
      "Epoch 1548/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4848 - acc: 0.8232 - val_loss: 1.3882 - val_acc: 0.6286\n",
      "Epoch 1549/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5027 - acc: 0.8232 - val_loss: 1.3764 - val_acc: 0.6670\n",
      "Epoch 1550/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4862 - acc: 0.8235 - val_loss: 1.3423 - val_acc: 0.6161\n",
      "Epoch 1551/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4860 - acc: 0.8242 - val_loss: 1.3738 - val_acc: 0.6277\n",
      "Epoch 1552/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5168 - acc: 0.8148 - val_loss: 1.3549 - val_acc: 0.6304\n",
      "Epoch 1553/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4913 - acc: 0.8258 - val_loss: 1.6539 - val_acc: 0.5571\n",
      "Epoch 1554/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5166 - acc: 0.8188 - val_loss: 1.9167 - val_acc: 0.5911\n",
      "Epoch 1555/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.5122 - acc: 0.8175 - val_loss: 1.3836 - val_acc: 0.6241\n",
      "Epoch 1556/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5049 - acc: 0.8213 - val_loss: 1.3263 - val_acc: 0.6446\n",
      "Epoch 1557/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5007 - acc: 0.8194 - val_loss: 1.3208 - val_acc: 0.6080\n",
      "Epoch 1558/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5010 - acc: 0.8200 - val_loss: 1.3167 - val_acc: 0.6393\n",
      "Epoch 1559/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5072 - acc: 0.8186 - val_loss: 1.4079 - val_acc: 0.6223\n",
      "Epoch 1560/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4810 - acc: 0.8288 - val_loss: 1.2161 - val_acc: 0.6723\n",
      "Epoch 1561/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4890 - acc: 0.8241 - val_loss: 1.5875 - val_acc: 0.6036\n",
      "Epoch 1562/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5058 - acc: 0.8237 - val_loss: 1.2771 - val_acc: 0.6250\n",
      "Epoch 1563/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5025 - acc: 0.8206 - val_loss: 1.3287 - val_acc: 0.6554\n",
      "Epoch 1564/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4919 - acc: 0.8232 - val_loss: 1.3579 - val_acc: 0.6402\n",
      "Epoch 1565/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4844 - acc: 0.8254 - val_loss: 1.1980 - val_acc: 0.6518\n",
      "Epoch 1566/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4897 - acc: 0.8256 - val_loss: 1.4236 - val_acc: 0.5920\n",
      "Epoch 1567/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4862 - acc: 0.8269 - val_loss: 1.4182 - val_acc: 0.6643\n",
      "Epoch 1568/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5017 - acc: 0.8250 - val_loss: 1.2869 - val_acc: 0.6143\n",
      "Epoch 1569/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5007 - acc: 0.8209 - val_loss: 1.4196 - val_acc: 0.6437\n",
      "Epoch 1570/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4999 - acc: 0.8232 - val_loss: 1.1695 - val_acc: 0.6723\n",
      "Epoch 1571/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4840 - acc: 0.8240 - val_loss: 1.4022 - val_acc: 0.5902\n",
      "Epoch 1572/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4998 - acc: 0.8235 - val_loss: 1.3076 - val_acc: 0.6473\n",
      "Epoch 1573/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4880 - acc: 0.8223 - val_loss: 1.6006 - val_acc: 0.5670\n",
      "Epoch 1574/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4947 - acc: 0.8253 - val_loss: 1.4577 - val_acc: 0.6429\n",
      "Epoch 1575/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4973 - acc: 0.8206 - val_loss: 1.8124 - val_acc: 0.5598\n",
      "Epoch 1576/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5103 - acc: 0.8237 - val_loss: 1.1003 - val_acc: 0.6634\n",
      "Epoch 1577/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4769 - acc: 0.8324 - val_loss: 1.5858 - val_acc: 0.5580\n",
      "Epoch 1578/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5202 - acc: 0.8166 - val_loss: 1.3401 - val_acc: 0.6143\n",
      "Epoch 1579/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.5013 - acc: 0.8209 - val_loss: 1.2308 - val_acc: 0.6339\n",
      "Epoch 1580/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5112 - acc: 0.8166 - val_loss: 1.1957 - val_acc: 0.6545\n",
      "Epoch 1581/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4934 - acc: 0.8221 - val_loss: 1.2569 - val_acc: 0.6491\n",
      "Epoch 1582/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4955 - acc: 0.8206 - val_loss: 1.3127 - val_acc: 0.6545\n",
      "Epoch 1583/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4885 - acc: 0.8260 - val_loss: 1.7240 - val_acc: 0.5973\n",
      "Epoch 1584/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4955 - acc: 0.8279 - val_loss: 1.6800 - val_acc: 0.6036\n",
      "Epoch 1585/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5179 - acc: 0.8208 - val_loss: 1.1557 - val_acc: 0.6696\n",
      "Epoch 1586/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5138 - acc: 0.8132 - val_loss: 1.5096 - val_acc: 0.6116\n",
      "Epoch 1587/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5069 - acc: 0.8256 - val_loss: 1.4099 - val_acc: 0.6393\n",
      "Epoch 1588/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4934 - acc: 0.8237 - val_loss: 1.5091 - val_acc: 0.6223\n",
      "Epoch 1589/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4857 - acc: 0.8264 - val_loss: 1.3855 - val_acc: 0.6295\n",
      "Epoch 1590/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5120 - acc: 0.8172 - val_loss: 1.7278 - val_acc: 0.5661\n",
      "Epoch 1591/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.5319 - acc: 0.8140 - val_loss: 1.2029 - val_acc: 0.6723\n",
      "Epoch 1592/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4901 - acc: 0.8256 - val_loss: 1.2680 - val_acc: 0.6500\n",
      "Epoch 1593/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4807 - acc: 0.8281 - val_loss: 1.4257 - val_acc: 0.6116\n",
      "Epoch 1594/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4961 - acc: 0.8215 - val_loss: 1.1298 - val_acc: 0.7134\n",
      "Epoch 1595/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4766 - acc: 0.8290 - val_loss: 1.2274 - val_acc: 0.6768\n",
      "Epoch 1596/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4937 - acc: 0.8215 - val_loss: 1.4588 - val_acc: 0.6107\n",
      "Epoch 1597/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4922 - acc: 0.8272 - val_loss: 1.1600 - val_acc: 0.6643\n",
      "Epoch 1598/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4945 - acc: 0.8236 - val_loss: 1.4793 - val_acc: 0.5964\n",
      "Epoch 1599/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4881 - acc: 0.8240 - val_loss: 1.1763 - val_acc: 0.6634\n",
      "Epoch 1600/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4877 - acc: 0.8252 - val_loss: 1.4191 - val_acc: 0.6018\n",
      "Epoch 1601/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4946 - acc: 0.8202 - val_loss: 1.5178 - val_acc: 0.6214\n",
      "Epoch 1602/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4891 - acc: 0.8261 - val_loss: 1.2548 - val_acc: 0.6304\n",
      "Epoch 1603/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4917 - acc: 0.8270 - val_loss: 1.3304 - val_acc: 0.6241\n",
      "Epoch 1604/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4976 - acc: 0.8245 - val_loss: 1.2997 - val_acc: 0.6643\n",
      "Epoch 1605/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4928 - acc: 0.8234 - val_loss: 1.4858 - val_acc: 0.6339\n",
      "Epoch 1606/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4823 - acc: 0.8273 - val_loss: 1.3445 - val_acc: 0.6187\n",
      "Epoch 1607/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5019 - acc: 0.8236 - val_loss: 1.2500 - val_acc: 0.6313\n",
      "Epoch 1608/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5042 - acc: 0.8236 - val_loss: 1.7657 - val_acc: 0.5607\n",
      "Epoch 1609/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5114 - acc: 0.8173 - val_loss: 1.1087 - val_acc: 0.6562\n",
      "Epoch 1610/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4815 - acc: 0.8250 - val_loss: 1.4164 - val_acc: 0.6196\n",
      "Epoch 1611/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4858 - acc: 0.8278 - val_loss: 1.4182 - val_acc: 0.6473\n",
      "Epoch 1612/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4966 - acc: 0.8237 - val_loss: 1.2211 - val_acc: 0.6464\n",
      "Epoch 1613/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4765 - acc: 0.8297 - val_loss: 1.5216 - val_acc: 0.6036\n",
      "Epoch 1614/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4786 - acc: 0.8314 - val_loss: 1.8150 - val_acc: 0.5580\n",
      "Epoch 1615/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5393 - acc: 0.8178 - val_loss: 1.2504 - val_acc: 0.6857\n",
      "Epoch 1616/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4931 - acc: 0.8239 - val_loss: 1.4579 - val_acc: 0.6232\n",
      "Epoch 1617/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4999 - acc: 0.8245 - val_loss: 1.4489 - val_acc: 0.6393\n",
      "Epoch 1618/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4937 - acc: 0.8258 - val_loss: 1.6122 - val_acc: 0.5670\n",
      "Epoch 1619/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5047 - acc: 0.8218 - val_loss: 1.4844 - val_acc: 0.6045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1620/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5055 - acc: 0.8190 - val_loss: 1.4491 - val_acc: 0.6295\n",
      "Epoch 1621/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4800 - acc: 0.8290 - val_loss: 1.4372 - val_acc: 0.5955\n",
      "Epoch 1622/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.5209 - acc: 0.8180 - val_loss: 1.4181 - val_acc: 0.6250\n",
      "Epoch 1623/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4853 - acc: 0.8290 - val_loss: 1.2242 - val_acc: 0.6714\n",
      "Epoch 1624/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4820 - acc: 0.8262 - val_loss: 1.4270 - val_acc: 0.6295\n",
      "Epoch 1625/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4879 - acc: 0.8232 - val_loss: 1.0840 - val_acc: 0.7018\n",
      "Epoch 1626/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4823 - acc: 0.8253 - val_loss: 1.2004 - val_acc: 0.6991\n",
      "Epoch 1627/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4847 - acc: 0.8268 - val_loss: 1.1987 - val_acc: 0.6455\n",
      "Epoch 1628/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4813 - acc: 0.8258 - val_loss: 1.1877 - val_acc: 0.6679\n",
      "Epoch 1629/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5095 - acc: 0.8207 - val_loss: 1.5128 - val_acc: 0.5973\n",
      "Epoch 1630/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4748 - acc: 0.8302 - val_loss: 1.7385 - val_acc: 0.5893\n",
      "Epoch 1631/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5005 - acc: 0.8223 - val_loss: 1.4650 - val_acc: 0.6295\n",
      "Epoch 1632/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4913 - acc: 0.8222 - val_loss: 1.5565 - val_acc: 0.6062\n",
      "Epoch 1633/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4926 - acc: 0.8258 - val_loss: 1.2685 - val_acc: 0.6518\n",
      "Epoch 1634/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4761 - acc: 0.8254 - val_loss: 1.1459 - val_acc: 0.6616\n",
      "Epoch 1635/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4809 - acc: 0.8284 - val_loss: 1.3413 - val_acc: 0.6509\n",
      "Epoch 1636/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4857 - acc: 0.8260 - val_loss: 1.2151 - val_acc: 0.6598\n",
      "Epoch 1637/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4860 - acc: 0.8256 - val_loss: 1.2998 - val_acc: 0.6491\n",
      "Epoch 1638/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4836 - acc: 0.8281 - val_loss: 1.2874 - val_acc: 0.6580\n",
      "Epoch 1639/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4791 - acc: 0.8282 - val_loss: 1.4150 - val_acc: 0.6500\n",
      "Epoch 1640/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4968 - acc: 0.8210 - val_loss: 1.3220 - val_acc: 0.6384\n",
      "Epoch 1641/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4996 - acc: 0.8220 - val_loss: 1.5652 - val_acc: 0.5946\n",
      "Epoch 1642/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.5016 - acc: 0.8218 - val_loss: 1.5175 - val_acc: 0.6125\n",
      "Epoch 1643/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4999 - acc: 0.8225 - val_loss: 1.4176 - val_acc: 0.6098\n",
      "Epoch 1644/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4863 - acc: 0.8246 - val_loss: 1.6179 - val_acc: 0.6232\n",
      "Epoch 1645/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5065 - acc: 0.8227 - val_loss: 1.3334 - val_acc: 0.6437\n",
      "Epoch 1646/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4758 - acc: 0.8316 - val_loss: 1.5541 - val_acc: 0.5973\n",
      "Epoch 1647/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.5093 - acc: 0.8218 - val_loss: 1.3572 - val_acc: 0.6313\n",
      "Epoch 1648/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4913 - acc: 0.8228 - val_loss: 1.1284 - val_acc: 0.6830\n",
      "Epoch 1649/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4921 - acc: 0.8250 - val_loss: 1.3035 - val_acc: 0.6518\n",
      "Epoch 1650/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4807 - acc: 0.8290 - val_loss: 1.4411 - val_acc: 0.5893\n",
      "Epoch 1651/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4918 - acc: 0.8272 - val_loss: 1.5256 - val_acc: 0.6223\n",
      "Epoch 1652/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4807 - acc: 0.8254 - val_loss: 1.1780 - val_acc: 0.6634\n",
      "Epoch 1653/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4774 - acc: 0.8276 - val_loss: 1.1076 - val_acc: 0.6946\n",
      "Epoch 1654/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4778 - acc: 0.8281 - val_loss: 1.2161 - val_acc: 0.6830\n",
      "Epoch 1655/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4839 - acc: 0.8247 - val_loss: 1.1666 - val_acc: 0.6741\n",
      "Epoch 1656/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4754 - acc: 0.8286 - val_loss: 1.0695 - val_acc: 0.7107\n",
      "Epoch 1657/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4683 - acc: 0.8298 - val_loss: 1.3658 - val_acc: 0.6643\n",
      "Epoch 1658/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4949 - acc: 0.8248 - val_loss: 1.3345 - val_acc: 0.6357\n",
      "Epoch 1659/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4860 - acc: 0.8279 - val_loss: 1.2888 - val_acc: 0.6688\n",
      "Epoch 1660/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4646 - acc: 0.8345 - val_loss: 1.2808 - val_acc: 0.6357\n",
      "Epoch 1661/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4872 - acc: 0.8250 - val_loss: 1.3681 - val_acc: 0.6446\n",
      "Epoch 1662/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4838 - acc: 0.8266 - val_loss: 1.3462 - val_acc: 0.6125\n",
      "Epoch 1663/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4903 - acc: 0.8265 - val_loss: 1.5979 - val_acc: 0.6143\n",
      "Epoch 1664/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4953 - acc: 0.8266 - val_loss: 1.4466 - val_acc: 0.6304\n",
      "Epoch 1665/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4827 - acc: 0.8295 - val_loss: 1.9458 - val_acc: 0.5866\n",
      "Epoch 1666/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4968 - acc: 0.8276 - val_loss: 1.3381 - val_acc: 0.6384\n",
      "Epoch 1667/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.5037 - acc: 0.8221 - val_loss: 1.0479 - val_acc: 0.7205\n",
      "Epoch 1668/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4636 - acc: 0.8322 - val_loss: 1.2231 - val_acc: 0.6795\n",
      "Epoch 1669/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4782 - acc: 0.8298 - val_loss: 1.5275 - val_acc: 0.6330\n",
      "Epoch 1670/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5243 - acc: 0.8194 - val_loss: 1.1238 - val_acc: 0.6866\n",
      "Epoch 1671/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4705 - acc: 0.8290 - val_loss: 1.3866 - val_acc: 0.6179\n",
      "Epoch 1672/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4945 - acc: 0.8308 - val_loss: 1.3321 - val_acc: 0.6518\n",
      "Epoch 1673/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4823 - acc: 0.8259 - val_loss: 1.3338 - val_acc: 0.6723\n",
      "Epoch 1674/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5026 - acc: 0.8234 - val_loss: 1.1399 - val_acc: 0.6768\n",
      "Epoch 1675/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4655 - acc: 0.8313 - val_loss: 1.3162 - val_acc: 0.6330\n",
      "Epoch 1676/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4627 - acc: 0.8308 - val_loss: 1.3407 - val_acc: 0.6170\n",
      "Epoch 1677/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4824 - acc: 0.8281 - val_loss: 1.5558 - val_acc: 0.6286\n",
      "Epoch 1678/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4804 - acc: 0.8292 - val_loss: 1.2646 - val_acc: 0.6920\n",
      "Epoch 1679/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4880 - acc: 0.8271 - val_loss: 1.5694 - val_acc: 0.6134\n",
      "Epoch 1680/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.5098 - acc: 0.8208 - val_loss: 1.2881 - val_acc: 0.6777\n",
      "Epoch 1681/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4729 - acc: 0.8328 - val_loss: 1.2789 - val_acc: 0.6509\n",
      "Epoch 1682/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4762 - acc: 0.8328 - val_loss: 1.4540 - val_acc: 0.6500\n",
      "Epoch 1683/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4928 - acc: 0.8226 - val_loss: 1.2304 - val_acc: 0.6875\n",
      "Epoch 1684/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4772 - acc: 0.8294 - val_loss: 1.2388 - val_acc: 0.6643\n",
      "Epoch 1685/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4661 - acc: 0.8306 - val_loss: 1.4570 - val_acc: 0.6161\n",
      "Epoch 1686/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4752 - acc: 0.8276 - val_loss: 1.4185 - val_acc: 0.6402\n",
      "Epoch 1687/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4841 - acc: 0.8254 - val_loss: 1.3663 - val_acc: 0.6607\n",
      "Epoch 1688/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4893 - acc: 0.8237 - val_loss: 1.4950 - val_acc: 0.6411\n",
      "Epoch 1689/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4902 - acc: 0.8262 - val_loss: 1.5207 - val_acc: 0.6357\n",
      "Epoch 1690/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4775 - acc: 0.8314 - val_loss: 1.4463 - val_acc: 0.5920\n",
      "Epoch 1691/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4812 - acc: 0.8292 - val_loss: 1.2907 - val_acc: 0.6491\n",
      "Epoch 1692/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4756 - acc: 0.8300 - val_loss: 1.3396 - val_acc: 0.6500\n",
      "Epoch 1693/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4868 - acc: 0.8284 - val_loss: 1.5351 - val_acc: 0.6080\n",
      "Epoch 1694/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4875 - acc: 0.8288 - val_loss: 1.5739 - val_acc: 0.6196\n",
      "Epoch 1695/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4819 - acc: 0.8244 - val_loss: 1.4432 - val_acc: 0.6321\n",
      "Epoch 1696/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4865 - acc: 0.8258 - val_loss: 1.1947 - val_acc: 0.6750\n",
      "Epoch 1697/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4817 - acc: 0.8244 - val_loss: 1.2960 - val_acc: 0.6866\n",
      "Epoch 1698/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4693 - acc: 0.8292 - val_loss: 1.1508 - val_acc: 0.6589\n",
      "Epoch 1699/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4613 - acc: 0.8357 - val_loss: 1.4320 - val_acc: 0.6205\n",
      "Epoch 1700/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4839 - acc: 0.8307 - val_loss: 1.3491 - val_acc: 0.6545\n",
      "Epoch 1701/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4663 - acc: 0.8336 - val_loss: 1.4239 - val_acc: 0.6446\n",
      "Epoch 1702/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4880 - acc: 0.8280 - val_loss: 1.4158 - val_acc: 0.6500\n",
      "Epoch 1703/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4754 - acc: 0.8341 - val_loss: 1.4128 - val_acc: 0.6321\n",
      "Epoch 1704/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4863 - acc: 0.8260 - val_loss: 1.3514 - val_acc: 0.6268\n",
      "Epoch 1705/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4865 - acc: 0.8284 - val_loss: 1.2591 - val_acc: 0.6634\n",
      "Epoch 1706/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4817 - acc: 0.8270 - val_loss: 1.1558 - val_acc: 0.6589\n",
      "Epoch 1707/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4588 - acc: 0.8346 - val_loss: 1.2139 - val_acc: 0.6732\n",
      "Epoch 1708/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4713 - acc: 0.8327 - val_loss: 1.2550 - val_acc: 0.6786\n",
      "Epoch 1709/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4760 - acc: 0.8274 - val_loss: 1.5356 - val_acc: 0.6134\n",
      "Epoch 1710/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4728 - acc: 0.8309 - val_loss: 1.4770 - val_acc: 0.5991\n",
      "Epoch 1711/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4960 - acc: 0.8266 - val_loss: 1.2834 - val_acc: 0.6563\n",
      "Epoch 1712/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4687 - acc: 0.8304 - val_loss: 1.3939 - val_acc: 0.6589\n",
      "Epoch 1713/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4719 - acc: 0.8294 - val_loss: 1.8092 - val_acc: 0.5973\n",
      "Epoch 1714/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4928 - acc: 0.8284 - val_loss: 1.4992 - val_acc: 0.6268\n",
      "Epoch 1715/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4832 - acc: 0.8320 - val_loss: 1.3518 - val_acc: 0.6634\n",
      "Epoch 1716/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4877 - acc: 0.8272 - val_loss: 1.1064 - val_acc: 0.6964\n",
      "Epoch 1717/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4767 - acc: 0.8316 - val_loss: 1.2888 - val_acc: 0.6464\n",
      "Epoch 1718/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4648 - acc: 0.8302 - val_loss: 1.7545 - val_acc: 0.5848\n",
      "Epoch 1719/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4903 - acc: 0.8302 - val_loss: 1.2462 - val_acc: 0.6625\n",
      "Epoch 1720/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4713 - acc: 0.8311 - val_loss: 1.3556 - val_acc: 0.6554\n",
      "Epoch 1721/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4729 - acc: 0.8343 - val_loss: 1.1467 - val_acc: 0.6955\n",
      "Epoch 1722/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4620 - acc: 0.8346 - val_loss: 1.3100 - val_acc: 0.6527\n",
      "Epoch 1723/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4859 - acc: 0.8242 - val_loss: 1.5533 - val_acc: 0.6295\n",
      "Epoch 1724/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4874 - acc: 0.8294 - val_loss: 1.3570 - val_acc: 0.6188\n",
      "Epoch 1725/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4579 - acc: 0.8368 - val_loss: 1.4088 - val_acc: 0.6232\n",
      "Epoch 1726/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4693 - acc: 0.8308 - val_loss: 1.2205 - val_acc: 0.6589\n",
      "Epoch 1727/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4819 - acc: 0.8272 - val_loss: 1.2814 - val_acc: 0.6491\n",
      "Epoch 1728/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4818 - acc: 0.8238 - val_loss: 1.4528 - val_acc: 0.6455\n",
      "Epoch 1729/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4674 - acc: 0.8329 - val_loss: 1.2197 - val_acc: 0.6562\n",
      "Epoch 1730/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4701 - acc: 0.8322 - val_loss: 1.7484 - val_acc: 0.5804\n",
      "Epoch 1731/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4803 - acc: 0.8282 - val_loss: 1.3521 - val_acc: 0.6464\n",
      "Epoch 1732/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4880 - acc: 0.8238 - val_loss: 1.5069 - val_acc: 0.6518\n",
      "Epoch 1733/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4593 - acc: 0.8360 - val_loss: 1.5455 - val_acc: 0.6321\n",
      "Epoch 1734/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4851 - acc: 0.8286 - val_loss: 1.2856 - val_acc: 0.6518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1735/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4684 - acc: 0.8353 - val_loss: 1.2031 - val_acc: 0.6616\n",
      "Epoch 1736/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4662 - acc: 0.8325 - val_loss: 1.3170 - val_acc: 0.6696\n",
      "Epoch 1737/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4798 - acc: 0.8291 - val_loss: 1.6531 - val_acc: 0.6250\n",
      "Epoch 1738/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4747 - acc: 0.8280 - val_loss: 1.3428 - val_acc: 0.6741\n",
      "Epoch 1739/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4564 - acc: 0.8384 - val_loss: 1.3764 - val_acc: 0.6196\n",
      "Epoch 1740/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4729 - acc: 0.8316 - val_loss: 1.1853 - val_acc: 0.6741\n",
      "Epoch 1741/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4778 - acc: 0.8279 - val_loss: 1.2140 - val_acc: 0.6598\n",
      "Epoch 1742/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4558 - acc: 0.8388 - val_loss: 1.7201 - val_acc: 0.6062\n",
      "Epoch 1743/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4944 - acc: 0.8290 - val_loss: 1.3463 - val_acc: 0.6518\n",
      "Epoch 1744/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4666 - acc: 0.8334 - val_loss: 1.1828 - val_acc: 0.6679\n",
      "Epoch 1745/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4720 - acc: 0.8282 - val_loss: 1.0921 - val_acc: 0.7107\n",
      "Epoch 1746/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4739 - acc: 0.8276 - val_loss: 1.4292 - val_acc: 0.6348\n",
      "Epoch 1747/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4673 - acc: 0.8366 - val_loss: 1.4841 - val_acc: 0.6134\n",
      "Epoch 1748/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4731 - acc: 0.8298 - val_loss: 1.2263 - val_acc: 0.6777\n",
      "Epoch 1749/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4858 - acc: 0.8283 - val_loss: 1.4696 - val_acc: 0.6339\n",
      "Epoch 1750/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4688 - acc: 0.8312 - val_loss: 1.4195 - val_acc: 0.6268\n",
      "Epoch 1751/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4653 - acc: 0.8329 - val_loss: 1.2363 - val_acc: 0.6821\n",
      "Epoch 1752/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4874 - acc: 0.8268 - val_loss: 1.6894 - val_acc: 0.6125\n",
      "Epoch 1753/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4706 - acc: 0.8303 - val_loss: 1.3613 - val_acc: 0.6607\n",
      "Epoch 1754/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4675 - acc: 0.8346 - val_loss: 1.4816 - val_acc: 0.6446\n",
      "Epoch 1755/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4820 - acc: 0.8294 - val_loss: 1.2453 - val_acc: 0.6804\n",
      "Epoch 1756/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4641 - acc: 0.8349 - val_loss: 1.4981 - val_acc: 0.6455\n",
      "Epoch 1757/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4591 - acc: 0.8360 - val_loss: 1.5694 - val_acc: 0.6223\n",
      "Epoch 1758/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4685 - acc: 0.8330 - val_loss: 1.1373 - val_acc: 0.6875\n",
      "Epoch 1759/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4769 - acc: 0.8273 - val_loss: 1.3940 - val_acc: 0.6643\n",
      "Epoch 1760/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4693 - acc: 0.8294 - val_loss: 1.5696 - val_acc: 0.5982\n",
      "Epoch 1761/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4792 - acc: 0.8290 - val_loss: 1.2583 - val_acc: 0.6589\n",
      "Epoch 1762/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4693 - acc: 0.8306 - val_loss: 1.5335 - val_acc: 0.6205\n",
      "Epoch 1763/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4863 - acc: 0.8298 - val_loss: 1.5216 - val_acc: 0.5911\n",
      "Epoch 1764/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4642 - acc: 0.8325 - val_loss: 1.3319 - val_acc: 0.6625\n",
      "Epoch 1765/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4611 - acc: 0.8332 - val_loss: 1.3600 - val_acc: 0.6705\n",
      "Epoch 1766/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4623 - acc: 0.8296 - val_loss: 1.3653 - val_acc: 0.6357\n",
      "Epoch 1767/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.4605 - acc: 0.8328 - val_loss: 1.3871 - val_acc: 0.6214\n",
      "Epoch 1768/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4871 - acc: 0.8242 - val_loss: 1.4609 - val_acc: 0.6134\n",
      "Epoch 1769/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4722 - acc: 0.8321 - val_loss: 1.3248 - val_acc: 0.6259\n",
      "Epoch 1770/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4731 - acc: 0.8319 - val_loss: 1.8626 - val_acc: 0.5937\n",
      "Epoch 1771/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4807 - acc: 0.8291 - val_loss: 1.1988 - val_acc: 0.6866\n",
      "Epoch 1772/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4671 - acc: 0.8309 - val_loss: 1.5390 - val_acc: 0.6089\n",
      "Epoch 1773/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4641 - acc: 0.8340 - val_loss: 1.2066 - val_acc: 0.6652\n",
      "Epoch 1774/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4670 - acc: 0.8332 - val_loss: 1.4075 - val_acc: 0.6152\n",
      "Epoch 1775/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4764 - acc: 0.8314 - val_loss: 1.4395 - val_acc: 0.6652\n",
      "Epoch 1776/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4711 - acc: 0.8331 - val_loss: 1.4248 - val_acc: 0.6339\n",
      "Epoch 1777/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4687 - acc: 0.8343 - val_loss: 1.4134 - val_acc: 0.6268\n",
      "Epoch 1778/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4787 - acc: 0.8286 - val_loss: 1.8858 - val_acc: 0.6170\n",
      "Epoch 1779/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4716 - acc: 0.8339 - val_loss: 1.2333 - val_acc: 0.6607\n",
      "Epoch 1780/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4773 - acc: 0.8300 - val_loss: 1.1342 - val_acc: 0.6929\n",
      "Epoch 1781/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4645 - acc: 0.8328 - val_loss: 1.3420 - val_acc: 0.6830\n",
      "Epoch 1782/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4473 - acc: 0.8412 - val_loss: 1.7286 - val_acc: 0.6045\n",
      "Epoch 1783/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4941 - acc: 0.8304 - val_loss: 1.5449 - val_acc: 0.6196\n",
      "Epoch 1784/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4672 - acc: 0.8343 - val_loss: 1.5355 - val_acc: 0.6223\n",
      "Epoch 1785/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4775 - acc: 0.8313 - val_loss: 1.1285 - val_acc: 0.7018\n",
      "Epoch 1786/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4575 - acc: 0.8348 - val_loss: 1.1754 - val_acc: 0.7071\n",
      "Epoch 1787/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4351 - acc: 0.8440 - val_loss: 1.2905 - val_acc: 0.6750\n",
      "Epoch 1788/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4767 - acc: 0.8286 - val_loss: 1.0356 - val_acc: 0.6804\n",
      "Epoch 1789/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4508 - acc: 0.8355 - val_loss: 1.5584 - val_acc: 0.6589\n",
      "Epoch 1790/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4712 - acc: 0.8329 - val_loss: 1.4877 - val_acc: 0.6232\n",
      "Epoch 1791/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4618 - acc: 0.8330 - val_loss: 1.1997 - val_acc: 0.6821\n",
      "Epoch 1792/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4782 - acc: 0.8311 - val_loss: 1.2417 - val_acc: 0.6446\n",
      "Epoch 1793/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4510 - acc: 0.8390 - val_loss: 1.1155 - val_acc: 0.6795\n",
      "Epoch 1794/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4705 - acc: 0.8278 - val_loss: 1.0783 - val_acc: 0.6982\n",
      "Epoch 1795/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4717 - acc: 0.8306 - val_loss: 1.2848 - val_acc: 0.6384\n",
      "Epoch 1796/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4599 - acc: 0.8334 - val_loss: 1.2934 - val_acc: 0.6375\n",
      "Epoch 1797/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4667 - acc: 0.8304 - val_loss: 1.2709 - val_acc: 0.6661\n",
      "Epoch 1798/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4623 - acc: 0.8343 - val_loss: 1.2364 - val_acc: 0.6625\n",
      "Epoch 1799/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4701 - acc: 0.8300 - val_loss: 1.3255 - val_acc: 0.6464\n",
      "Epoch 1800/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4523 - acc: 0.8366 - val_loss: 1.5557 - val_acc: 0.5750\n",
      "Epoch 1801/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4602 - acc: 0.8408 - val_loss: 1.3777 - val_acc: 0.6339\n",
      "Epoch 1802/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4680 - acc: 0.8347 - val_loss: 1.2239 - val_acc: 0.6402\n",
      "Epoch 1803/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4816 - acc: 0.8288 - val_loss: 1.3451 - val_acc: 0.6420\n",
      "Epoch 1804/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4581 - acc: 0.8370 - val_loss: 1.1443 - val_acc: 0.6937\n",
      "Epoch 1805/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4498 - acc: 0.8390 - val_loss: 1.3163 - val_acc: 0.6402\n",
      "Epoch 1806/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4663 - acc: 0.8329 - val_loss: 1.4874 - val_acc: 0.6170\n",
      "Epoch 1807/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4665 - acc: 0.8349 - val_loss: 1.4049 - val_acc: 0.6438\n",
      "Epoch 1808/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4817 - acc: 0.8293 - val_loss: 1.3506 - val_acc: 0.6384\n",
      "Epoch 1809/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4736 - acc: 0.8312 - val_loss: 1.2202 - val_acc: 0.6750\n",
      "Epoch 1810/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4640 - acc: 0.8347 - val_loss: 1.5529 - val_acc: 0.5929\n",
      "Epoch 1811/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4646 - acc: 0.8332 - val_loss: 1.2366 - val_acc: 0.6366\n",
      "Epoch 1812/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4730 - acc: 0.8341 - val_loss: 1.1544 - val_acc: 0.6732\n",
      "Epoch 1813/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4576 - acc: 0.8322 - val_loss: 1.4320 - val_acc: 0.6420\n",
      "Epoch 1814/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4718 - acc: 0.8332 - val_loss: 1.4139 - val_acc: 0.6563\n",
      "Epoch 1815/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4539 - acc: 0.8337 - val_loss: 1.1180 - val_acc: 0.6866\n",
      "Epoch 1816/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4653 - acc: 0.8310 - val_loss: 1.4925 - val_acc: 0.6643\n",
      "Epoch 1817/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4593 - acc: 0.8366 - val_loss: 1.2290 - val_acc: 0.6616\n",
      "Epoch 1818/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4549 - acc: 0.8340 - val_loss: 1.4702 - val_acc: 0.6313\n",
      "Epoch 1819/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4592 - acc: 0.8368 - val_loss: 1.3103 - val_acc: 0.6473\n",
      "Epoch 1820/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4784 - acc: 0.8276 - val_loss: 1.3860 - val_acc: 0.6375\n",
      "Epoch 1821/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4572 - acc: 0.8387 - val_loss: 1.4934 - val_acc: 0.6339\n",
      "Epoch 1822/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4582 - acc: 0.8356 - val_loss: 1.3213 - val_acc: 0.6795\n",
      "Epoch 1823/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4446 - acc: 0.8382 - val_loss: 1.3585 - val_acc: 0.6598\n",
      "Epoch 1824/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4723 - acc: 0.8307 - val_loss: 1.8732 - val_acc: 0.6027\n",
      "Epoch 1825/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4633 - acc: 0.8358 - val_loss: 1.1023 - val_acc: 0.6964\n",
      "Epoch 1826/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4704 - acc: 0.8326 - val_loss: 1.4518 - val_acc: 0.6170\n",
      "Epoch 1827/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4598 - acc: 0.8332 - val_loss: 1.3686 - val_acc: 0.6313\n",
      "Epoch 1828/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4581 - acc: 0.8371 - val_loss: 1.6511 - val_acc: 0.6205\n",
      "Epoch 1829/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4718 - acc: 0.8322 - val_loss: 1.0939 - val_acc: 0.6839\n",
      "Epoch 1830/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4544 - acc: 0.8347 - val_loss: 1.1100 - val_acc: 0.6991\n",
      "Epoch 1831/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4532 - acc: 0.8322 - val_loss: 1.7817 - val_acc: 0.5991\n",
      "Epoch 1832/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4750 - acc: 0.8287 - val_loss: 1.4008 - val_acc: 0.6268\n",
      "Epoch 1833/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4512 - acc: 0.8360 - val_loss: 1.3581 - val_acc: 0.6804\n",
      "Epoch 1834/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4598 - acc: 0.8340 - val_loss: 1.3784 - val_acc: 0.6286\n",
      "Epoch 1835/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4493 - acc: 0.8406 - val_loss: 1.5207 - val_acc: 0.6241\n",
      "Epoch 1836/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4822 - acc: 0.8303 - val_loss: 1.3747 - val_acc: 0.6634\n",
      "Epoch 1837/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4521 - acc: 0.8409 - val_loss: 1.3105 - val_acc: 0.6437\n",
      "Epoch 1838/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4654 - acc: 0.8333 - val_loss: 1.3571 - val_acc: 0.6571\n",
      "Epoch 1839/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4583 - acc: 0.8380 - val_loss: 1.2043 - val_acc: 0.6473\n",
      "Epoch 1840/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4604 - acc: 0.8347 - val_loss: 1.0734 - val_acc: 0.7018\n",
      "Epoch 1841/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4581 - acc: 0.8364 - val_loss: 1.3203 - val_acc: 0.6830\n",
      "Epoch 1842/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4629 - acc: 0.8342 - val_loss: 1.2720 - val_acc: 0.6821\n",
      "Epoch 1843/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4458 - acc: 0.8400 - val_loss: 2.3088 - val_acc: 0.5170\n",
      "Epoch 1844/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.5127 - acc: 0.8288 - val_loss: 1.9798 - val_acc: 0.5875\n",
      "Epoch 1845/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4558 - acc: 0.8396 - val_loss: 1.3937 - val_acc: 0.6491\n",
      "Epoch 1846/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4715 - acc: 0.8353 - val_loss: 1.2458 - val_acc: 0.6875\n",
      "Epoch 1847/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4503 - acc: 0.8388 - val_loss: 1.3984 - val_acc: 0.6518\n",
      "Epoch 1848/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4560 - acc: 0.8390 - val_loss: 1.5288 - val_acc: 0.6143\n",
      "Epoch 1849/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4836 - acc: 0.8260 - val_loss: 1.2891 - val_acc: 0.6777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1850/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4470 - acc: 0.8408 - val_loss: 1.3975 - val_acc: 0.6384\n",
      "Epoch 1851/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4624 - acc: 0.8353 - val_loss: 1.4743 - val_acc: 0.6321\n",
      "Epoch 1852/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4643 - acc: 0.8380 - val_loss: 1.1594 - val_acc: 0.7161\n",
      "Epoch 1853/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4484 - acc: 0.8411 - val_loss: 1.4198 - val_acc: 0.6527\n",
      "Epoch 1854/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4639 - acc: 0.8324 - val_loss: 1.4427 - val_acc: 0.6179\n",
      "Epoch 1855/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4502 - acc: 0.8397 - val_loss: 1.2929 - val_acc: 0.6652\n",
      "Epoch 1856/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4609 - acc: 0.8349 - val_loss: 1.6589 - val_acc: 0.6036\n",
      "Epoch 1857/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4631 - acc: 0.8330 - val_loss: 1.5126 - val_acc: 0.6286\n",
      "Epoch 1858/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4681 - acc: 0.8330 - val_loss: 1.5314 - val_acc: 0.6036\n",
      "Epoch 1859/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4607 - acc: 0.8351 - val_loss: 1.2453 - val_acc: 0.6679\n",
      "Epoch 1860/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4450 - acc: 0.8398 - val_loss: 1.1877 - val_acc: 0.6848\n",
      "Epoch 1861/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4546 - acc: 0.8374 - val_loss: 1.4283 - val_acc: 0.6304\n",
      "Epoch 1862/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4566 - acc: 0.8390 - val_loss: 1.3159 - val_acc: 0.6670\n",
      "Epoch 1863/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4567 - acc: 0.8370 - val_loss: 1.8487 - val_acc: 0.5812\n",
      "Epoch 1864/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4973 - acc: 0.8286 - val_loss: 1.2278 - val_acc: 0.6938\n",
      "Epoch 1865/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4657 - acc: 0.8312 - val_loss: 1.6997 - val_acc: 0.5821\n",
      "Epoch 1866/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4733 - acc: 0.8312 - val_loss: 1.2389 - val_acc: 0.6848\n",
      "Epoch 1867/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4521 - acc: 0.8416 - val_loss: 1.5731 - val_acc: 0.6009\n",
      "Epoch 1868/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4773 - acc: 0.8314 - val_loss: 1.2115 - val_acc: 0.6518\n",
      "Epoch 1869/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4635 - acc: 0.8327 - val_loss: 1.5221 - val_acc: 0.6045\n",
      "Epoch 1870/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4611 - acc: 0.8388 - val_loss: 1.2747 - val_acc: 0.6839\n",
      "Epoch 1871/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4550 - acc: 0.8347 - val_loss: 1.5631 - val_acc: 0.6214\n",
      "Epoch 1872/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4562 - acc: 0.8398 - val_loss: 1.4158 - val_acc: 0.6429\n",
      "Epoch 1873/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4802 - acc: 0.8292 - val_loss: 1.2154 - val_acc: 0.6991\n",
      "Epoch 1874/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4550 - acc: 0.8361 - val_loss: 1.3379 - val_acc: 0.6643\n",
      "Epoch 1875/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4519 - acc: 0.8370 - val_loss: 1.3115 - val_acc: 0.6670\n",
      "Epoch 1876/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4563 - acc: 0.8375 - val_loss: 1.2455 - val_acc: 0.6554\n",
      "Epoch 1877/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4481 - acc: 0.8383 - val_loss: 1.5023 - val_acc: 0.6446\n",
      "Epoch 1878/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4627 - acc: 0.8368 - val_loss: 1.1973 - val_acc: 0.7027\n",
      "Epoch 1879/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4437 - acc: 0.8417 - val_loss: 1.4593 - val_acc: 0.6188\n",
      "Epoch 1880/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4478 - acc: 0.8390 - val_loss: 1.2211 - val_acc: 0.6607\n",
      "Epoch 1881/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4499 - acc: 0.8398 - val_loss: 1.2501 - val_acc: 0.6714\n",
      "Epoch 1882/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4400 - acc: 0.8412 - val_loss: 1.6806 - val_acc: 0.6241\n",
      "Epoch 1883/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4473 - acc: 0.8414 - val_loss: 1.3166 - val_acc: 0.6482\n",
      "Epoch 1884/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4596 - acc: 0.8373 - val_loss: 1.4719 - val_acc: 0.6571\n",
      "Epoch 1885/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4548 - acc: 0.8371 - val_loss: 1.3930 - val_acc: 0.6268\n",
      "Epoch 1886/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4597 - acc: 0.8330 - val_loss: 1.1392 - val_acc: 0.6821\n",
      "Epoch 1887/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4500 - acc: 0.8394 - val_loss: 1.3356 - val_acc: 0.6732\n",
      "Epoch 1888/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4448 - acc: 0.8413 - val_loss: 1.3003 - val_acc: 0.6527\n",
      "Epoch 1889/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4517 - acc: 0.8347 - val_loss: 2.1948 - val_acc: 0.5437\n",
      "Epoch 1890/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4688 - acc: 0.8397 - val_loss: 1.1802 - val_acc: 0.6500\n",
      "Epoch 1891/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4585 - acc: 0.8346 - val_loss: 1.5021 - val_acc: 0.6429\n",
      "Epoch 1892/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4638 - acc: 0.8352 - val_loss: 1.3372 - val_acc: 0.6634\n",
      "Epoch 1893/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4582 - acc: 0.8336 - val_loss: 1.2936 - val_acc: 0.6330\n",
      "Epoch 1894/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4591 - acc: 0.8383 - val_loss: 1.2549 - val_acc: 0.6679\n",
      "Epoch 1895/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4633 - acc: 0.8376 - val_loss: 1.5773 - val_acc: 0.6411\n",
      "Epoch 1896/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4495 - acc: 0.8426 - val_loss: 1.6182 - val_acc: 0.6366\n",
      "Epoch 1897/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4493 - acc: 0.8388 - val_loss: 1.1776 - val_acc: 0.7000\n",
      "Epoch 1898/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4556 - acc: 0.8305 - val_loss: 1.5622 - val_acc: 0.6482\n",
      "Epoch 1899/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4530 - acc: 0.8354 - val_loss: 1.1015 - val_acc: 0.7036\n",
      "Epoch 1900/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4404 - acc: 0.8426 - val_loss: 1.3736 - val_acc: 0.6348\n",
      "Epoch 1901/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4640 - acc: 0.8346 - val_loss: 1.6037 - val_acc: 0.6411\n",
      "Epoch 1902/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4586 - acc: 0.8348 - val_loss: 1.2989 - val_acc: 0.6518\n",
      "Epoch 1903/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4628 - acc: 0.8344 - val_loss: 1.4579 - val_acc: 0.6080\n",
      "Epoch 1904/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4766 - acc: 0.8320 - val_loss: 1.2777 - val_acc: 0.6723\n",
      "Epoch 1905/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4433 - acc: 0.8415 - val_loss: 1.7401 - val_acc: 0.5759\n",
      "Epoch 1906/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4521 - acc: 0.8406 - val_loss: 1.3620 - val_acc: 0.6152\n",
      "Epoch 1907/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4489 - acc: 0.8407 - val_loss: 1.4473 - val_acc: 0.6696\n",
      "Epoch 1908/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4498 - acc: 0.8386 - val_loss: 1.4362 - val_acc: 0.6295\n",
      "Epoch 1909/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4532 - acc: 0.8359 - val_loss: 1.8950 - val_acc: 0.5920\n",
      "Epoch 1910/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4814 - acc: 0.8342 - val_loss: 1.3662 - val_acc: 0.6393\n",
      "Epoch 1911/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4486 - acc: 0.8395 - val_loss: 1.5077 - val_acc: 0.6464\n",
      "Epoch 1912/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4743 - acc: 0.8324 - val_loss: 1.5059 - val_acc: 0.6446\n",
      "Epoch 1913/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4405 - acc: 0.8441 - val_loss: 1.4146 - val_acc: 0.6304\n",
      "Epoch 1914/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4602 - acc: 0.8352 - val_loss: 1.3044 - val_acc: 0.6875\n",
      "Epoch 1915/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4438 - acc: 0.8378 - val_loss: 1.5877 - val_acc: 0.6446\n",
      "Epoch 1916/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4582 - acc: 0.8412 - val_loss: 1.4556 - val_acc: 0.6223\n",
      "Epoch 1917/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4521 - acc: 0.8353 - val_loss: 1.4725 - val_acc: 0.6330\n",
      "Epoch 1918/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4616 - acc: 0.8366 - val_loss: 1.2403 - val_acc: 0.6795\n",
      "Epoch 1919/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4421 - acc: 0.8406 - val_loss: 1.2877 - val_acc: 0.6786\n",
      "Epoch 1920/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4476 - acc: 0.8400 - val_loss: 1.1780 - val_acc: 0.6902\n",
      "Epoch 1921/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4398 - acc: 0.8420 - val_loss: 1.8392 - val_acc: 0.5598\n",
      "Epoch 1922/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4618 - acc: 0.8394 - val_loss: 1.4607 - val_acc: 0.6277\n",
      "Epoch 1923/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4549 - acc: 0.8375 - val_loss: 1.1660 - val_acc: 0.6750\n",
      "Epoch 1924/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4432 - acc: 0.8355 - val_loss: 1.4999 - val_acc: 0.6670\n",
      "Epoch 1925/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4373 - acc: 0.8404 - val_loss: 1.6669 - val_acc: 0.6518\n",
      "Epoch 1926/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4749 - acc: 0.8324 - val_loss: 1.4272 - val_acc: 0.6339\n",
      "Epoch 1927/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4431 - acc: 0.8408 - val_loss: 1.3865 - val_acc: 0.6402\n",
      "Epoch 1928/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4631 - acc: 0.8337 - val_loss: 1.6197 - val_acc: 0.6205\n",
      "Epoch 1929/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4502 - acc: 0.8385 - val_loss: 1.3606 - val_acc: 0.6679\n",
      "Epoch 1930/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4392 - acc: 0.8413 - val_loss: 1.2506 - val_acc: 0.6536\n",
      "Epoch 1931/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4523 - acc: 0.8376 - val_loss: 1.2429 - val_acc: 0.6723\n",
      "Epoch 1932/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4452 - acc: 0.8405 - val_loss: 1.6386 - val_acc: 0.6223\n",
      "Epoch 1933/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4463 - acc: 0.8368 - val_loss: 1.3869 - val_acc: 0.6518\n",
      "Epoch 1934/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4513 - acc: 0.8381 - val_loss: 1.3539 - val_acc: 0.6714\n",
      "Epoch 1935/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4374 - acc: 0.8396 - val_loss: 1.3332 - val_acc: 0.6438\n",
      "Epoch 1936/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4648 - acc: 0.8334 - val_loss: 1.3543 - val_acc: 0.6598\n",
      "Epoch 1937/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4371 - acc: 0.8424 - val_loss: 1.5494 - val_acc: 0.6268\n",
      "Epoch 1938/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4550 - acc: 0.8354 - val_loss: 1.2473 - val_acc: 0.6813\n",
      "Epoch 1939/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4436 - acc: 0.8406 - val_loss: 1.7708 - val_acc: 0.5955\n",
      "Epoch 1940/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4494 - acc: 0.8385 - val_loss: 1.5216 - val_acc: 0.5857\n",
      "Epoch 1941/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4436 - acc: 0.8401 - val_loss: 1.4501 - val_acc: 0.6446\n",
      "Epoch 1942/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4401 - acc: 0.8425 - val_loss: 1.3706 - val_acc: 0.6554\n",
      "Epoch 1943/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4301 - acc: 0.8487 - val_loss: 1.4802 - val_acc: 0.6375\n",
      "Epoch 1944/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4422 - acc: 0.8398 - val_loss: 1.3589 - val_acc: 0.6679\n",
      "Epoch 1945/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4579 - acc: 0.8374 - val_loss: 1.5054 - val_acc: 0.6375\n",
      "Epoch 1946/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4436 - acc: 0.8424 - val_loss: 1.4535 - val_acc: 0.6571\n",
      "Epoch 1947/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4406 - acc: 0.8434 - val_loss: 1.4759 - val_acc: 0.6589\n",
      "Epoch 1948/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4394 - acc: 0.8427 - val_loss: 1.2381 - val_acc: 0.6607\n",
      "Epoch 1949/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4437 - acc: 0.8406 - val_loss: 1.4370 - val_acc: 0.6509\n",
      "Epoch 1950/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4514 - acc: 0.8410 - val_loss: 1.5797 - val_acc: 0.6152\n",
      "Epoch 1951/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4556 - acc: 0.8384 - val_loss: 1.7234 - val_acc: 0.5562\n",
      "Epoch 1952/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4576 - acc: 0.8370 - val_loss: 1.2373 - val_acc: 0.6955\n",
      "Epoch 1953/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4425 - acc: 0.8411 - val_loss: 1.6130 - val_acc: 0.5955\n",
      "Epoch 1954/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4584 - acc: 0.8380 - val_loss: 1.1760 - val_acc: 0.7063\n",
      "Epoch 1955/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4501 - acc: 0.8357 - val_loss: 1.3629 - val_acc: 0.6723\n",
      "Epoch 1956/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4392 - acc: 0.8439 - val_loss: 1.2571 - val_acc: 0.7009\n",
      "Epoch 1957/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4420 - acc: 0.8378 - val_loss: 1.2306 - val_acc: 0.6482\n",
      "Epoch 1958/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4488 - acc: 0.8372 - val_loss: 1.4047 - val_acc: 0.6491\n",
      "Epoch 1959/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4428 - acc: 0.8418 - val_loss: 1.2679 - val_acc: 0.6527\n",
      "Epoch 1960/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4533 - acc: 0.8379 - val_loss: 1.5129 - val_acc: 0.6018\n",
      "Epoch 1961/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4806 - acc: 0.8305 - val_loss: 1.3907 - val_acc: 0.6214\n",
      "Epoch 1962/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4497 - acc: 0.8389 - val_loss: 1.6651 - val_acc: 0.5893\n",
      "Epoch 1963/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4641 - acc: 0.8342 - val_loss: 1.4785 - val_acc: 0.6179\n",
      "Epoch 1964/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4507 - acc: 0.8402 - val_loss: 1.5475 - val_acc: 0.6277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1965/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4459 - acc: 0.8426 - val_loss: 1.2241 - val_acc: 0.6848\n",
      "Epoch 1966/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4364 - acc: 0.8406 - val_loss: 1.3300 - val_acc: 0.6634\n",
      "Epoch 1967/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4458 - acc: 0.8396 - val_loss: 1.2779 - val_acc: 0.6277\n",
      "Epoch 1968/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4650 - acc: 0.8369 - val_loss: 1.1065 - val_acc: 0.6964\n",
      "Epoch 1969/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4324 - acc: 0.8447 - val_loss: 1.3200 - val_acc: 0.6518\n",
      "Epoch 1970/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4516 - acc: 0.8382 - val_loss: 1.6447 - val_acc: 0.6286\n",
      "Epoch 1971/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4399 - acc: 0.8434 - val_loss: 1.7505 - val_acc: 0.6277\n",
      "Epoch 1972/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4635 - acc: 0.8388 - val_loss: 1.3828 - val_acc: 0.6670\n",
      "Epoch 1973/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4610 - acc: 0.8354 - val_loss: 1.2881 - val_acc: 0.6536\n",
      "Epoch 1974/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4414 - acc: 0.8406 - val_loss: 1.1230 - val_acc: 0.6991\n",
      "Epoch 1975/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4389 - acc: 0.8368 - val_loss: 1.1184 - val_acc: 0.7214\n",
      "Epoch 1976/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4400 - acc: 0.8416 - val_loss: 1.6367 - val_acc: 0.5857\n",
      "Epoch 1977/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4756 - acc: 0.8348 - val_loss: 1.4815 - val_acc: 0.6321\n",
      "Epoch 1978/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4425 - acc: 0.8422 - val_loss: 1.2129 - val_acc: 0.6723\n",
      "Epoch 1979/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4377 - acc: 0.8437 - val_loss: 1.4121 - val_acc: 0.6509\n",
      "Epoch 1980/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4340 - acc: 0.8412 - val_loss: 1.7351 - val_acc: 0.6161\n",
      "Epoch 1981/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4464 - acc: 0.8382 - val_loss: 1.4736 - val_acc: 0.6375\n",
      "Epoch 1982/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4483 - acc: 0.8368 - val_loss: 1.9805 - val_acc: 0.5750\n",
      "Epoch 1983/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4679 - acc: 0.8368 - val_loss: 1.2675 - val_acc: 0.6920\n",
      "Epoch 1984/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4425 - acc: 0.8452 - val_loss: 1.1085 - val_acc: 0.6991\n",
      "Epoch 1985/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4395 - acc: 0.8400 - val_loss: 1.5088 - val_acc: 0.6446\n",
      "Epoch 1986/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4652 - acc: 0.8291 - val_loss: 1.4270 - val_acc: 0.6161\n",
      "Epoch 1987/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4490 - acc: 0.8392 - val_loss: 1.4722 - val_acc: 0.6062\n",
      "Epoch 1988/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4468 - acc: 0.8378 - val_loss: 1.4896 - val_acc: 0.6455\n",
      "Epoch 1989/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4384 - acc: 0.8443 - val_loss: 1.3541 - val_acc: 0.6348\n",
      "Epoch 1990/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4390 - acc: 0.8402 - val_loss: 1.2416 - val_acc: 0.7000\n",
      "Epoch 1991/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4350 - acc: 0.8436 - val_loss: 1.3142 - val_acc: 0.6554\n",
      "Epoch 1992/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.4367 - acc: 0.8473 - val_loss: 1.4633 - val_acc: 0.6545\n",
      "Epoch 1993/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4435 - acc: 0.8406 - val_loss: 1.2272 - val_acc: 0.6893\n",
      "Epoch 1994/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4389 - acc: 0.8409 - val_loss: 1.2987 - val_acc: 0.6955\n",
      "Epoch 1995/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4427 - acc: 0.8422 - val_loss: 1.3764 - val_acc: 0.6393\n",
      "Epoch 1996/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4535 - acc: 0.8386 - val_loss: 1.7009 - val_acc: 0.6473\n",
      "Epoch 1997/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4521 - acc: 0.8393 - val_loss: 1.2594 - val_acc: 0.6732\n",
      "Epoch 1998/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4331 - acc: 0.8441 - val_loss: 1.2324 - val_acc: 0.6911\n",
      "Epoch 1999/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4303 - acc: 0.8441 - val_loss: 1.4123 - val_acc: 0.6536\n",
      "Epoch 2000/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4551 - acc: 0.8376 - val_loss: 1.2500 - val_acc: 0.6857\n",
      "Epoch 2001/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4389 - acc: 0.8418 - val_loss: 1.6618 - val_acc: 0.5884\n",
      "Epoch 2002/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4342 - acc: 0.8447 - val_loss: 1.2549 - val_acc: 0.6964\n",
      "Epoch 2003/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4208 - acc: 0.8473 - val_loss: 1.1514 - val_acc: 0.6857\n",
      "Epoch 2004/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4488 - acc: 0.8394 - val_loss: 1.4634 - val_acc: 0.6277\n",
      "Epoch 2005/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4394 - acc: 0.8421 - val_loss: 1.5249 - val_acc: 0.6054\n",
      "Epoch 2006/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4531 - acc: 0.8360 - val_loss: 1.4404 - val_acc: 0.6268\n",
      "Epoch 2007/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4369 - acc: 0.8429 - val_loss: 1.1671 - val_acc: 0.6830\n",
      "Epoch 2008/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4401 - acc: 0.8390 - val_loss: 1.3566 - val_acc: 0.6625\n",
      "Epoch 2009/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4494 - acc: 0.8437 - val_loss: 2.0034 - val_acc: 0.5446\n",
      "Epoch 2010/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4548 - acc: 0.8384 - val_loss: 2.1497 - val_acc: 0.5321\n",
      "Epoch 2011/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4507 - acc: 0.8439 - val_loss: 1.7394 - val_acc: 0.6196\n",
      "Epoch 2012/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4689 - acc: 0.8340 - val_loss: 1.5427 - val_acc: 0.6429\n",
      "Epoch 2013/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4419 - acc: 0.8447 - val_loss: 1.2681 - val_acc: 0.6429\n",
      "Epoch 2014/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4505 - acc: 0.8379 - val_loss: 1.5532 - val_acc: 0.6268\n",
      "Epoch 2015/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4360 - acc: 0.8450 - val_loss: 1.3477 - val_acc: 0.6607\n",
      "Epoch 2016/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4342 - acc: 0.8430 - val_loss: 1.3444 - val_acc: 0.7000\n",
      "Epoch 2017/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4311 - acc: 0.8418 - val_loss: 1.5480 - val_acc: 0.6375\n",
      "Epoch 2018/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4277 - acc: 0.8469 - val_loss: 1.4452 - val_acc: 0.6580\n",
      "Epoch 2019/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4462 - acc: 0.8420 - val_loss: 1.4173 - val_acc: 0.6509\n",
      "Epoch 2020/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4347 - acc: 0.8436 - val_loss: 1.1723 - val_acc: 0.6884\n",
      "Epoch 2021/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4412 - acc: 0.8413 - val_loss: 1.6564 - val_acc: 0.6295\n",
      "Epoch 2022/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4441 - acc: 0.8416 - val_loss: 1.2173 - val_acc: 0.6884\n",
      "Epoch 2023/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4228 - acc: 0.8503 - val_loss: 1.2900 - val_acc: 0.6991\n",
      "Epoch 2024/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4457 - acc: 0.8402 - val_loss: 1.2957 - val_acc: 0.6786\n",
      "Epoch 2025/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4342 - acc: 0.8430 - val_loss: 1.4537 - val_acc: 0.6402\n",
      "Epoch 2026/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4522 - acc: 0.8374 - val_loss: 1.6129 - val_acc: 0.5759\n",
      "Epoch 2027/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4475 - acc: 0.8434 - val_loss: 1.3556 - val_acc: 0.6857\n",
      "Epoch 2028/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4415 - acc: 0.8432 - val_loss: 1.4025 - val_acc: 0.6705\n",
      "Epoch 2029/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4243 - acc: 0.8456 - val_loss: 1.2604 - val_acc: 0.6786\n",
      "Epoch 2030/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4294 - acc: 0.8445 - val_loss: 1.7706 - val_acc: 0.5848\n",
      "Epoch 2031/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4494 - acc: 0.8402 - val_loss: 1.3982 - val_acc: 0.6098\n",
      "Epoch 2032/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4360 - acc: 0.8458 - val_loss: 1.4369 - val_acc: 0.6518\n",
      "Epoch 2033/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4397 - acc: 0.8409 - val_loss: 1.7038 - val_acc: 0.6179\n",
      "Epoch 2034/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4484 - acc: 0.8408 - val_loss: 1.3155 - val_acc: 0.6893\n",
      "Epoch 2035/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4121 - acc: 0.8530 - val_loss: 1.5511 - val_acc: 0.6170\n",
      "Epoch 2036/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4645 - acc: 0.8374 - val_loss: 1.2573 - val_acc: 0.6616\n",
      "Epoch 2037/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4348 - acc: 0.8434 - val_loss: 1.7864 - val_acc: 0.6330\n",
      "Epoch 2038/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4254 - acc: 0.8489 - val_loss: 1.2921 - val_acc: 0.6723\n",
      "Epoch 2039/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4450 - acc: 0.8408 - val_loss: 1.0855 - val_acc: 0.6955\n",
      "Epoch 2040/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4238 - acc: 0.8495 - val_loss: 1.7307 - val_acc: 0.5973\n",
      "Epoch 2041/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4713 - acc: 0.8381 - val_loss: 1.6384 - val_acc: 0.6375\n",
      "Epoch 2042/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4366 - acc: 0.8458 - val_loss: 1.8080 - val_acc: 0.5741\n",
      "Epoch 2043/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4512 - acc: 0.8415 - val_loss: 1.4448 - val_acc: 0.6554\n",
      "Epoch 2044/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4421 - acc: 0.8440 - val_loss: 1.1890 - val_acc: 0.6750\n",
      "Epoch 2045/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4291 - acc: 0.8457 - val_loss: 1.5974 - val_acc: 0.6170\n",
      "Epoch 2046/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4427 - acc: 0.8449 - val_loss: 1.3972 - val_acc: 0.6777\n",
      "Epoch 2047/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4478 - acc: 0.8430 - val_loss: 1.2272 - val_acc: 0.6902\n",
      "Epoch 2048/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4254 - acc: 0.8482 - val_loss: 1.5067 - val_acc: 0.6536\n",
      "Epoch 2049/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4510 - acc: 0.8412 - val_loss: 1.2471 - val_acc: 0.6455\n",
      "Epoch 2050/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4537 - acc: 0.8404 - val_loss: 1.5337 - val_acc: 0.6446\n",
      "Epoch 2051/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4338 - acc: 0.8437 - val_loss: 1.3800 - val_acc: 0.6134\n",
      "Epoch 2052/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4479 - acc: 0.8398 - val_loss: 1.3437 - val_acc: 0.6857\n",
      "Epoch 2053/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4295 - acc: 0.8451 - val_loss: 1.6487 - val_acc: 0.5768\n",
      "Epoch 2054/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4329 - acc: 0.8480 - val_loss: 1.4359 - val_acc: 0.6589\n",
      "Epoch 2055/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4294 - acc: 0.8454 - val_loss: 1.7248 - val_acc: 0.6179\n",
      "Epoch 2056/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4422 - acc: 0.8439 - val_loss: 1.4976 - val_acc: 0.6446\n",
      "Epoch 2057/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4284 - acc: 0.8469 - val_loss: 1.2787 - val_acc: 0.7018\n",
      "Epoch 2058/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4323 - acc: 0.8453 - val_loss: 1.6154 - val_acc: 0.6000\n",
      "Epoch 2059/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4551 - acc: 0.8403 - val_loss: 1.2516 - val_acc: 0.6589\n",
      "Epoch 2060/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4424 - acc: 0.8418 - val_loss: 1.1372 - val_acc: 0.6920\n",
      "Epoch 2061/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4318 - acc: 0.8441 - val_loss: 1.2833 - val_acc: 0.6527\n",
      "Epoch 2062/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4424 - acc: 0.8420 - val_loss: 1.5177 - val_acc: 0.6366\n",
      "Epoch 2063/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4327 - acc: 0.8437 - val_loss: 1.5654 - val_acc: 0.6375\n",
      "Epoch 2064/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4455 - acc: 0.8393 - val_loss: 1.5575 - val_acc: 0.6196\n",
      "Epoch 2065/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4440 - acc: 0.8403 - val_loss: 1.4887 - val_acc: 0.6429\n",
      "Epoch 2066/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4473 - acc: 0.8392 - val_loss: 1.2246 - val_acc: 0.6920\n",
      "Epoch 2067/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4297 - acc: 0.8461 - val_loss: 1.2566 - val_acc: 0.6670\n",
      "Epoch 2068/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4242 - acc: 0.8452 - val_loss: 1.4601 - val_acc: 0.6705\n",
      "Epoch 2069/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4216 - acc: 0.8494 - val_loss: 1.2188 - val_acc: 0.6696\n",
      "Epoch 2070/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4447 - acc: 0.8407 - val_loss: 1.1111 - val_acc: 0.6973\n",
      "Epoch 2071/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4241 - acc: 0.8485 - val_loss: 1.4336 - val_acc: 0.6420\n",
      "Epoch 2072/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4422 - acc: 0.8417 - val_loss: 1.4348 - val_acc: 0.6652\n",
      "Epoch 2073/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4284 - acc: 0.8507 - val_loss: 1.1325 - val_acc: 0.7000\n",
      "Epoch 2074/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4521 - acc: 0.8400 - val_loss: 1.5422 - val_acc: 0.6625\n",
      "Epoch 2075/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4239 - acc: 0.8481 - val_loss: 1.7080 - val_acc: 0.6232\n",
      "Epoch 2076/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4624 - acc: 0.8413 - val_loss: 1.3752 - val_acc: 0.6696\n",
      "Epoch 2077/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4325 - acc: 0.8420 - val_loss: 1.4880 - val_acc: 0.6518\n",
      "Epoch 2078/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4232 - acc: 0.8498 - val_loss: 1.3833 - val_acc: 0.6491\n",
      "Epoch 2079/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4483 - acc: 0.8412 - val_loss: 1.4207 - val_acc: 0.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2080/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4416 - acc: 0.8409 - val_loss: 1.1954 - val_acc: 0.7071\n",
      "Epoch 2081/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4340 - acc: 0.8417 - val_loss: 1.3670 - val_acc: 0.6473\n",
      "Epoch 2082/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4366 - acc: 0.8466 - val_loss: 1.4320 - val_acc: 0.6589\n",
      "Epoch 2083/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4277 - acc: 0.8472 - val_loss: 1.5333 - val_acc: 0.6259\n",
      "Epoch 2084/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4597 - acc: 0.8344 - val_loss: 1.2803 - val_acc: 0.6777\n",
      "Epoch 2085/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4164 - acc: 0.8505 - val_loss: 1.3216 - val_acc: 0.6839\n",
      "Epoch 2086/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4397 - acc: 0.8448 - val_loss: 1.3649 - val_acc: 0.6955\n",
      "Epoch 2087/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4313 - acc: 0.8465 - val_loss: 1.3974 - val_acc: 0.6500\n",
      "Epoch 2088/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4391 - acc: 0.8402 - val_loss: 1.0937 - val_acc: 0.7295\n",
      "Epoch 2089/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4228 - acc: 0.8474 - val_loss: 1.4427 - val_acc: 0.6696\n",
      "Epoch 2090/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4215 - acc: 0.8481 - val_loss: 1.1680 - val_acc: 0.7009\n",
      "Epoch 2091/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4050 - acc: 0.8545 - val_loss: 1.6293 - val_acc: 0.6045\n",
      "Epoch 2092/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4520 - acc: 0.8424 - val_loss: 1.4602 - val_acc: 0.6187\n",
      "Epoch 2093/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4556 - acc: 0.8378 - val_loss: 1.3059 - val_acc: 0.6839\n",
      "Epoch 2094/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4167 - acc: 0.8492 - val_loss: 1.5214 - val_acc: 0.6536\n",
      "Epoch 2095/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4366 - acc: 0.8447 - val_loss: 1.5692 - val_acc: 0.6027\n",
      "Epoch 2096/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4566 - acc: 0.8420 - val_loss: 1.4607 - val_acc: 0.6589\n",
      "Epoch 2097/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4347 - acc: 0.8449 - val_loss: 2.0452 - val_acc: 0.5848\n",
      "Epoch 2098/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4423 - acc: 0.8443 - val_loss: 1.7352 - val_acc: 0.5821\n",
      "Epoch 2099/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4498 - acc: 0.8439 - val_loss: 1.4456 - val_acc: 0.6366\n",
      "Epoch 2100/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4324 - acc: 0.8449 - val_loss: 1.3962 - val_acc: 0.6643\n",
      "Epoch 2101/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4219 - acc: 0.8484 - val_loss: 1.4285 - val_acc: 0.6625\n",
      "Epoch 2102/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4345 - acc: 0.8438 - val_loss: 1.6367 - val_acc: 0.6250\n",
      "Epoch 2103/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4602 - acc: 0.8412 - val_loss: 1.6856 - val_acc: 0.5955\n",
      "Epoch 2104/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4370 - acc: 0.8414 - val_loss: 1.6298 - val_acc: 0.6268\n",
      "Epoch 2105/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4283 - acc: 0.8496 - val_loss: 1.6101 - val_acc: 0.6500\n",
      "Epoch 2106/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4513 - acc: 0.8409 - val_loss: 1.2548 - val_acc: 0.7054\n",
      "Epoch 2107/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4180 - acc: 0.8500 - val_loss: 1.6637 - val_acc: 0.6348\n",
      "Epoch 2108/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4403 - acc: 0.8435 - val_loss: 1.5510 - val_acc: 0.6188\n",
      "Epoch 2109/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4567 - acc: 0.8412 - val_loss: 1.6774 - val_acc: 0.6214\n",
      "Epoch 2110/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4447 - acc: 0.8398 - val_loss: 1.6155 - val_acc: 0.6482\n",
      "Epoch 2111/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4396 - acc: 0.8441 - val_loss: 1.5327 - val_acc: 0.6509\n",
      "Epoch 2112/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4434 - acc: 0.8460 - val_loss: 1.3919 - val_acc: 0.6812\n",
      "Epoch 2113/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4296 - acc: 0.8435 - val_loss: 1.1993 - val_acc: 0.6812\n",
      "Epoch 2114/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4114 - acc: 0.8506 - val_loss: 1.4830 - val_acc: 0.6321\n",
      "Epoch 2115/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4292 - acc: 0.8460 - val_loss: 1.5657 - val_acc: 0.6339\n",
      "Epoch 2116/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4262 - acc: 0.8491 - val_loss: 1.7491 - val_acc: 0.6223\n",
      "Epoch 2117/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4352 - acc: 0.8495 - val_loss: 1.6399 - val_acc: 0.6571\n",
      "Epoch 2118/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4274 - acc: 0.8471 - val_loss: 2.4080 - val_acc: 0.5134\n",
      "Epoch 2119/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4727 - acc: 0.8403 - val_loss: 1.4025 - val_acc: 0.6268\n",
      "Epoch 2120/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4387 - acc: 0.8440 - val_loss: 1.4505 - val_acc: 0.6509\n",
      "Epoch 2121/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4323 - acc: 0.8470 - val_loss: 1.2190 - val_acc: 0.6884\n",
      "Epoch 2122/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4119 - acc: 0.8493 - val_loss: 1.2385 - val_acc: 0.6446\n",
      "Epoch 2123/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4275 - acc: 0.8454 - val_loss: 1.5853 - val_acc: 0.6420\n",
      "Epoch 2124/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4264 - acc: 0.8498 - val_loss: 1.4692 - val_acc: 0.6304\n",
      "Epoch 2125/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4350 - acc: 0.8464 - val_loss: 1.5650 - val_acc: 0.6330\n",
      "Epoch 2126/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4182 - acc: 0.8533 - val_loss: 1.2881 - val_acc: 0.6741\n",
      "Epoch 2127/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4303 - acc: 0.8461 - val_loss: 1.3929 - val_acc: 0.6705\n",
      "Epoch 2128/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4244 - acc: 0.8497 - val_loss: 1.4197 - val_acc: 0.6777\n",
      "Epoch 2129/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4278 - acc: 0.8459 - val_loss: 1.4734 - val_acc: 0.6580\n",
      "Epoch 2130/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4240 - acc: 0.8478 - val_loss: 1.9489 - val_acc: 0.5955\n",
      "Epoch 2131/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4558 - acc: 0.8426 - val_loss: 1.3155 - val_acc: 0.6857\n",
      "Epoch 2132/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4262 - acc: 0.8487 - val_loss: 1.1654 - val_acc: 0.7080\n",
      "Epoch 2133/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4067 - acc: 0.8506 - val_loss: 1.2132 - val_acc: 0.6786\n",
      "Epoch 2134/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4211 - acc: 0.8463 - val_loss: 1.3271 - val_acc: 0.6473\n",
      "Epoch 2135/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4472 - acc: 0.8441 - val_loss: 1.4362 - val_acc: 0.6607\n",
      "Epoch 2136/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4436 - acc: 0.8439 - val_loss: 1.3935 - val_acc: 0.6804\n",
      "Epoch 2137/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4255 - acc: 0.8479 - val_loss: 1.3959 - val_acc: 0.6857\n",
      "Epoch 2138/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4263 - acc: 0.8489 - val_loss: 1.6601 - val_acc: 0.6179\n",
      "Epoch 2139/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4468 - acc: 0.8438 - val_loss: 1.2601 - val_acc: 0.7027\n",
      "Epoch 2140/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4286 - acc: 0.8453 - val_loss: 1.2002 - val_acc: 0.6705\n",
      "Epoch 2141/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4112 - acc: 0.8522 - val_loss: 1.7093 - val_acc: 0.6455\n",
      "Epoch 2142/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4231 - acc: 0.8499 - val_loss: 1.4593 - val_acc: 0.6589\n",
      "Epoch 2143/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4299 - acc: 0.8457 - val_loss: 1.5960 - val_acc: 0.6500\n",
      "Epoch 2144/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4487 - acc: 0.8422 - val_loss: 1.2384 - val_acc: 0.6714\n",
      "Epoch 2145/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4161 - acc: 0.8499 - val_loss: 1.2988 - val_acc: 0.6259\n",
      "Epoch 2146/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4233 - acc: 0.8470 - val_loss: 1.4058 - val_acc: 0.6777\n",
      "Epoch 2147/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4379 - acc: 0.8441 - val_loss: 1.2202 - val_acc: 0.6964\n",
      "Epoch 2148/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4133 - acc: 0.8513 - val_loss: 1.4849 - val_acc: 0.6607\n",
      "Epoch 2149/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4380 - acc: 0.8410 - val_loss: 1.2371 - val_acc: 0.6955\n",
      "Epoch 2150/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4050 - acc: 0.8554 - val_loss: 1.6927 - val_acc: 0.6098\n",
      "Epoch 2151/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4575 - acc: 0.8364 - val_loss: 1.3126 - val_acc: 0.6679\n",
      "Epoch 2152/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4163 - acc: 0.8487 - val_loss: 1.2165 - val_acc: 0.6696\n",
      "Epoch 2153/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4462 - acc: 0.8380 - val_loss: 1.8551 - val_acc: 0.6063\n",
      "Epoch 2154/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4311 - acc: 0.8475 - val_loss: 1.3986 - val_acc: 0.6545\n",
      "Epoch 2155/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4410 - acc: 0.8430 - val_loss: 1.7575 - val_acc: 0.6348\n",
      "Epoch 2156/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4155 - acc: 0.8513 - val_loss: 1.2877 - val_acc: 0.6589\n",
      "Epoch 2157/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4451 - acc: 0.8435 - val_loss: 1.1822 - val_acc: 0.6795\n",
      "Epoch 2158/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4130 - acc: 0.8492 - val_loss: 1.2397 - val_acc: 0.7045\n",
      "Epoch 2159/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4057 - acc: 0.8509 - val_loss: 1.3033 - val_acc: 0.6857\n",
      "Epoch 2160/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4174 - acc: 0.8511 - val_loss: 1.5128 - val_acc: 0.6670\n",
      "Epoch 2161/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4518 - acc: 0.8428 - val_loss: 1.3699 - val_acc: 0.6616\n",
      "Epoch 2162/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4114 - acc: 0.8522 - val_loss: 1.2362 - val_acc: 0.6973\n",
      "Epoch 2163/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4060 - acc: 0.8520 - val_loss: 1.6646 - val_acc: 0.6420\n",
      "Epoch 2164/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4351 - acc: 0.8465 - val_loss: 1.2055 - val_acc: 0.7134\n",
      "Epoch 2165/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4130 - acc: 0.8501 - val_loss: 1.6536 - val_acc: 0.6188\n",
      "Epoch 2166/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4192 - acc: 0.8509 - val_loss: 1.4433 - val_acc: 0.6563\n",
      "Epoch 2167/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4172 - acc: 0.8522 - val_loss: 1.2230 - val_acc: 0.6821\n",
      "Epoch 2168/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4310 - acc: 0.8451 - val_loss: 1.3857 - val_acc: 0.6286\n",
      "Epoch 2169/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4300 - acc: 0.8502 - val_loss: 1.4765 - val_acc: 0.6661\n",
      "Epoch 2170/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4283 - acc: 0.8453 - val_loss: 1.6482 - val_acc: 0.6473\n",
      "Epoch 2171/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4246 - acc: 0.8458 - val_loss: 1.9417 - val_acc: 0.5857\n",
      "Epoch 2172/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4269 - acc: 0.8475 - val_loss: 1.4888 - val_acc: 0.6402\n",
      "Epoch 2173/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4236 - acc: 0.8494 - val_loss: 1.2727 - val_acc: 0.6839\n",
      "Epoch 2174/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4163 - acc: 0.8504 - val_loss: 1.5173 - val_acc: 0.6473\n",
      "Epoch 2175/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4296 - acc: 0.8486 - val_loss: 1.5713 - val_acc: 0.6375\n",
      "Epoch 2176/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4373 - acc: 0.8430 - val_loss: 1.4345 - val_acc: 0.6848\n",
      "Epoch 2177/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4125 - acc: 0.8515 - val_loss: 1.6468 - val_acc: 0.6054\n",
      "Epoch 2178/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4236 - acc: 0.8488 - val_loss: 1.6797 - val_acc: 0.6384\n",
      "Epoch 2179/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4535 - acc: 0.8436 - val_loss: 1.2726 - val_acc: 0.6813\n",
      "Epoch 2180/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.4111 - acc: 0.8487 - val_loss: 1.2858 - val_acc: 0.6857\n",
      "Epoch 2181/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4287 - acc: 0.8441 - val_loss: 1.2911 - val_acc: 0.6643\n",
      "Epoch 2182/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4179 - acc: 0.8499 - val_loss: 1.2918 - val_acc: 0.6875\n",
      "Epoch 2183/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4283 - acc: 0.8473 - val_loss: 1.7695 - val_acc: 0.5920\n",
      "Epoch 2184/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4325 - acc: 0.8446 - val_loss: 1.3845 - val_acc: 0.6875\n",
      "Epoch 2185/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4220 - acc: 0.8477 - val_loss: 1.7147 - val_acc: 0.6268\n",
      "Epoch 2186/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4265 - acc: 0.8481 - val_loss: 1.4604 - val_acc: 0.6696\n",
      "Epoch 2187/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4371 - acc: 0.8459 - val_loss: 1.3071 - val_acc: 0.6670\n",
      "Epoch 2188/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4121 - acc: 0.8513 - val_loss: 1.5687 - val_acc: 0.6446\n",
      "Epoch 2189/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4197 - acc: 0.8495 - val_loss: 1.4029 - val_acc: 0.6652\n",
      "Epoch 2190/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4373 - acc: 0.8426 - val_loss: 1.2983 - val_acc: 0.6902\n",
      "Epoch 2191/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4050 - acc: 0.8579 - val_loss: 1.4175 - val_acc: 0.6652\n",
      "Epoch 2192/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4202 - acc: 0.8471 - val_loss: 1.3133 - val_acc: 0.6759\n",
      "Epoch 2193/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4092 - acc: 0.8547 - val_loss: 1.5097 - val_acc: 0.6616\n",
      "Epoch 2194/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4211 - acc: 0.8503 - val_loss: 1.5452 - val_acc: 0.6384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2195/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4224 - acc: 0.8501 - val_loss: 1.6726 - val_acc: 0.6286\n",
      "Epoch 2196/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4323 - acc: 0.8457 - val_loss: 1.3559 - val_acc: 0.6866\n",
      "Epoch 2197/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4203 - acc: 0.8476 - val_loss: 1.4699 - val_acc: 0.6330\n",
      "Epoch 2198/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4244 - acc: 0.8447 - val_loss: 1.4176 - val_acc: 0.6830\n",
      "Epoch 2199/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4088 - acc: 0.8566 - val_loss: 1.5144 - val_acc: 0.6241\n",
      "Epoch 2200/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4238 - acc: 0.8480 - val_loss: 1.3632 - val_acc: 0.6679\n",
      "Epoch 2201/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4259 - acc: 0.8479 - val_loss: 1.5694 - val_acc: 0.6321\n",
      "Epoch 2202/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4384 - acc: 0.8419 - val_loss: 1.8153 - val_acc: 0.6411\n",
      "Epoch 2203/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4399 - acc: 0.8464 - val_loss: 1.2710 - val_acc: 0.6821\n",
      "Epoch 2204/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4056 - acc: 0.8524 - val_loss: 1.4295 - val_acc: 0.6661\n",
      "Epoch 2205/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4182 - acc: 0.8503 - val_loss: 1.3454 - val_acc: 0.6625\n",
      "Epoch 2206/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4081 - acc: 0.8541 - val_loss: 1.3721 - val_acc: 0.6518\n",
      "Epoch 2207/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4281 - acc: 0.8449 - val_loss: 1.5540 - val_acc: 0.6179\n",
      "Epoch 2208/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4241 - acc: 0.8497 - val_loss: 1.2195 - val_acc: 0.6911\n",
      "Epoch 2209/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4278 - acc: 0.8475 - val_loss: 1.4742 - val_acc: 0.6571\n",
      "Epoch 2210/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4119 - acc: 0.8521 - val_loss: 1.2483 - val_acc: 0.6857\n",
      "Epoch 2211/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4247 - acc: 0.8432 - val_loss: 1.4987 - val_acc: 0.6795\n",
      "Epoch 2212/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4070 - acc: 0.8531 - val_loss: 1.4158 - val_acc: 0.6688\n",
      "Epoch 2213/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4254 - acc: 0.8493 - val_loss: 1.6621 - val_acc: 0.6446\n",
      "Epoch 2214/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4345 - acc: 0.8463 - val_loss: 1.4219 - val_acc: 0.6857\n",
      "Epoch 2215/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4358 - acc: 0.8483 - val_loss: 1.7209 - val_acc: 0.6214\n",
      "Epoch 2216/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4232 - acc: 0.8522 - val_loss: 1.4354 - val_acc: 0.6500\n",
      "Epoch 2217/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4132 - acc: 0.8512 - val_loss: 1.3533 - val_acc: 0.6232\n",
      "Epoch 2218/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4380 - acc: 0.8480 - val_loss: 1.1976 - val_acc: 0.6893\n",
      "Epoch 2219/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4104 - acc: 0.8501 - val_loss: 1.7500 - val_acc: 0.6330\n",
      "Epoch 2220/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4337 - acc: 0.8479 - val_loss: 1.2432 - val_acc: 0.6527\n",
      "Epoch 2221/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4152 - acc: 0.8504 - val_loss: 1.0696 - val_acc: 0.7205\n",
      "Epoch 2222/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4071 - acc: 0.8530 - val_loss: 1.5991 - val_acc: 0.6250\n",
      "Epoch 2223/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4259 - acc: 0.8487 - val_loss: 1.4275 - val_acc: 0.6437\n",
      "Epoch 2224/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4189 - acc: 0.8511 - val_loss: 1.5921 - val_acc: 0.6143\n",
      "Epoch 2225/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4249 - acc: 0.8474 - val_loss: 1.2519 - val_acc: 0.6536\n",
      "Epoch 2226/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4182 - acc: 0.8475 - val_loss: 1.3175 - val_acc: 0.6893\n",
      "Epoch 2227/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4225 - acc: 0.8465 - val_loss: 1.7085 - val_acc: 0.6411\n",
      "Epoch 2228/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4262 - acc: 0.8500 - val_loss: 1.2085 - val_acc: 0.7116\n",
      "Epoch 2229/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4149 - acc: 0.8515 - val_loss: 1.5963 - val_acc: 0.6357\n",
      "Epoch 2230/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4182 - acc: 0.8517 - val_loss: 1.3950 - val_acc: 0.6866\n",
      "Epoch 2231/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4117 - acc: 0.8525 - val_loss: 1.6019 - val_acc: 0.6098\n",
      "Epoch 2232/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4234 - acc: 0.8517 - val_loss: 1.4199 - val_acc: 0.6598\n",
      "Epoch 2233/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4149 - acc: 0.8538 - val_loss: 1.0812 - val_acc: 0.7205\n",
      "Epoch 2234/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4267 - acc: 0.8462 - val_loss: 1.0626 - val_acc: 0.7045\n",
      "Epoch 2235/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3997 - acc: 0.8559 - val_loss: 1.6118 - val_acc: 0.6393\n",
      "Epoch 2236/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4310 - acc: 0.8461 - val_loss: 1.5244 - val_acc: 0.6625\n",
      "Epoch 2237/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4308 - acc: 0.8442 - val_loss: 1.5024 - val_acc: 0.6384\n",
      "Epoch 2238/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4230 - acc: 0.8521 - val_loss: 1.7209 - val_acc: 0.5973\n",
      "Epoch 2239/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4209 - acc: 0.8527 - val_loss: 1.7560 - val_acc: 0.5857\n",
      "Epoch 2240/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4335 - acc: 0.8475 - val_loss: 1.4298 - val_acc: 0.6214\n",
      "Epoch 2241/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4062 - acc: 0.8549 - val_loss: 1.5506 - val_acc: 0.6768\n",
      "Epoch 2242/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4383 - acc: 0.8459 - val_loss: 1.3766 - val_acc: 0.6750\n",
      "Epoch 2243/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4133 - acc: 0.8535 - val_loss: 1.5666 - val_acc: 0.6313\n",
      "Epoch 2244/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4423 - acc: 0.8435 - val_loss: 1.1439 - val_acc: 0.6973\n",
      "Epoch 2245/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4095 - acc: 0.8525 - val_loss: 1.7618 - val_acc: 0.6063\n",
      "Epoch 2246/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4223 - acc: 0.8535 - val_loss: 1.7556 - val_acc: 0.6312\n",
      "Epoch 2247/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4201 - acc: 0.8493 - val_loss: 1.6666 - val_acc: 0.6321\n",
      "Epoch 2248/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4254 - acc: 0.8515 - val_loss: 1.7220 - val_acc: 0.6268\n",
      "Epoch 2249/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4233 - acc: 0.8499 - val_loss: 1.3463 - val_acc: 0.6705\n",
      "Epoch 2250/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4291 - acc: 0.8460 - val_loss: 1.2796 - val_acc: 0.6991\n",
      "Epoch 2251/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4096 - acc: 0.8488 - val_loss: 1.3554 - val_acc: 0.6821\n",
      "Epoch 2252/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4089 - acc: 0.8537 - val_loss: 1.6225 - val_acc: 0.6455\n",
      "Epoch 2253/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4175 - acc: 0.8513 - val_loss: 1.4747 - val_acc: 0.6509\n",
      "Epoch 2254/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4137 - acc: 0.8501 - val_loss: 1.3711 - val_acc: 0.6500\n",
      "Epoch 2255/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.4222 - acc: 0.8471 - val_loss: 1.4183 - val_acc: 0.6634\n",
      "Epoch 2256/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4136 - acc: 0.8500 - val_loss: 1.6980 - val_acc: 0.6179\n",
      "Epoch 2257/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4228 - acc: 0.8461 - val_loss: 1.1804 - val_acc: 0.6955\n",
      "Epoch 2258/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4231 - acc: 0.8463 - val_loss: 1.5057 - val_acc: 0.6750\n",
      "Epoch 2259/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4084 - acc: 0.8538 - val_loss: 1.1501 - val_acc: 0.7330\n",
      "Epoch 2260/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4040 - acc: 0.8527 - val_loss: 1.1959 - val_acc: 0.6893\n",
      "Epoch 2261/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4100 - acc: 0.8542 - val_loss: 1.2740 - val_acc: 0.6902\n",
      "Epoch 2262/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4362 - acc: 0.8415 - val_loss: 1.4461 - val_acc: 0.6554\n",
      "Epoch 2263/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4056 - acc: 0.8531 - val_loss: 1.3974 - val_acc: 0.6687\n",
      "Epoch 2264/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4118 - acc: 0.8541 - val_loss: 2.2351 - val_acc: 0.5277\n",
      "Epoch 2265/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4443 - acc: 0.8499 - val_loss: 1.3060 - val_acc: 0.6464\n",
      "Epoch 2266/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4033 - acc: 0.8571 - val_loss: 1.3827 - val_acc: 0.6464\n",
      "Epoch 2267/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4226 - acc: 0.8503 - val_loss: 1.4682 - val_acc: 0.6786\n",
      "Epoch 2268/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4086 - acc: 0.8529 - val_loss: 1.4133 - val_acc: 0.6509\n",
      "Epoch 2269/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4275 - acc: 0.8441 - val_loss: 1.2502 - val_acc: 0.6839\n",
      "Epoch 2270/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4112 - acc: 0.8505 - val_loss: 1.5220 - val_acc: 0.6384\n",
      "Epoch 2271/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4292 - acc: 0.8475 - val_loss: 1.3649 - val_acc: 0.6875\n",
      "Epoch 2272/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4016 - acc: 0.8542 - val_loss: 1.2734 - val_acc: 0.6848\n",
      "Epoch 2273/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4104 - acc: 0.8511 - val_loss: 1.7522 - val_acc: 0.6500\n",
      "Epoch 2274/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4250 - acc: 0.8511 - val_loss: 1.2678 - val_acc: 0.6732\n",
      "Epoch 2275/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4262 - acc: 0.8477 - val_loss: 1.4991 - val_acc: 0.6509\n",
      "Epoch 2276/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4314 - acc: 0.8489 - val_loss: 1.1580 - val_acc: 0.6768\n",
      "Epoch 2277/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4120 - acc: 0.8555 - val_loss: 1.3526 - val_acc: 0.6884\n",
      "Epoch 2278/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4196 - acc: 0.8480 - val_loss: 1.1712 - val_acc: 0.7375\n",
      "Epoch 2279/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4080 - acc: 0.8545 - val_loss: 1.4173 - val_acc: 0.6857\n",
      "Epoch 2280/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4191 - acc: 0.8515 - val_loss: 1.5280 - val_acc: 0.6348\n",
      "Epoch 2281/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4186 - acc: 0.8512 - val_loss: 1.4575 - val_acc: 0.6187\n",
      "Epoch 2282/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4211 - acc: 0.8500 - val_loss: 1.1938 - val_acc: 0.6839\n",
      "Epoch 2283/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4107 - acc: 0.8508 - val_loss: 1.4574 - val_acc: 0.6429\n",
      "Epoch 2284/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4057 - acc: 0.8540 - val_loss: 1.2705 - val_acc: 0.6679\n",
      "Epoch 2285/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4066 - acc: 0.8547 - val_loss: 1.3757 - val_acc: 0.6589\n",
      "Epoch 2286/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4141 - acc: 0.8507 - val_loss: 1.1293 - val_acc: 0.7196\n",
      "Epoch 2287/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4071 - acc: 0.8495 - val_loss: 1.3061 - val_acc: 0.6696\n",
      "Epoch 2288/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4058 - acc: 0.8557 - val_loss: 1.1976 - val_acc: 0.6938\n",
      "Epoch 2289/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4110 - acc: 0.8537 - val_loss: 1.2734 - val_acc: 0.7196\n",
      "Epoch 2290/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4088 - acc: 0.8554 - val_loss: 1.4655 - val_acc: 0.6571\n",
      "Epoch 2291/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4119 - acc: 0.8513 - val_loss: 1.2821 - val_acc: 0.7125\n",
      "Epoch 2292/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3969 - acc: 0.8547 - val_loss: 1.2138 - val_acc: 0.6848\n",
      "Epoch 2293/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4039 - acc: 0.8542 - val_loss: 1.3679 - val_acc: 0.6366\n",
      "Epoch 2294/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4119 - acc: 0.8553 - val_loss: 1.5673 - val_acc: 0.6152\n",
      "Epoch 2295/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4103 - acc: 0.8531 - val_loss: 1.1758 - val_acc: 0.6920\n",
      "Epoch 2296/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4084 - acc: 0.8530 - val_loss: 1.4648 - val_acc: 0.6563\n",
      "Epoch 2297/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4104 - acc: 0.8539 - val_loss: 1.5765 - val_acc: 0.6455\n",
      "Epoch 2298/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4174 - acc: 0.8539 - val_loss: 1.5072 - val_acc: 0.6643\n",
      "Epoch 2299/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4277 - acc: 0.8487 - val_loss: 1.5389 - val_acc: 0.6321\n",
      "Epoch 2300/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4167 - acc: 0.8543 - val_loss: 1.2339 - val_acc: 0.7089\n",
      "Epoch 2301/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4069 - acc: 0.8505 - val_loss: 1.6542 - val_acc: 0.5991\n",
      "Epoch 2302/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4209 - acc: 0.8505 - val_loss: 1.6379 - val_acc: 0.6179\n",
      "Epoch 2303/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4139 - acc: 0.8524 - val_loss: 1.3163 - val_acc: 0.6741\n",
      "Epoch 2304/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4092 - acc: 0.8527 - val_loss: 1.7340 - val_acc: 0.6125\n",
      "Epoch 2305/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4444 - acc: 0.8505 - val_loss: 1.5134 - val_acc: 0.6393\n",
      "Epoch 2306/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4034 - acc: 0.8575 - val_loss: 1.3594 - val_acc: 0.6768\n",
      "Epoch 2307/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4156 - acc: 0.8511 - val_loss: 1.3587 - val_acc: 0.6839\n",
      "Epoch 2308/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4189 - acc: 0.8506 - val_loss: 2.2014 - val_acc: 0.5795\n",
      "Epoch 2309/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4222 - acc: 0.8531 - val_loss: 1.3604 - val_acc: 0.6920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2310/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4161 - acc: 0.8487 - val_loss: 1.4057 - val_acc: 0.6723\n",
      "Epoch 2311/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4135 - acc: 0.8510 - val_loss: 1.4462 - val_acc: 0.6652\n",
      "Epoch 2312/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4243 - acc: 0.8506 - val_loss: 1.4480 - val_acc: 0.6268\n",
      "Epoch 2313/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4105 - acc: 0.8537 - val_loss: 1.7922 - val_acc: 0.6143\n",
      "Epoch 2314/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4226 - acc: 0.8507 - val_loss: 1.4706 - val_acc: 0.6670\n",
      "Epoch 2315/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4135 - acc: 0.8517 - val_loss: 1.3206 - val_acc: 0.6759\n",
      "Epoch 2316/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4137 - acc: 0.8517 - val_loss: 1.1617 - val_acc: 0.7018\n",
      "Epoch 2317/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3983 - acc: 0.8565 - val_loss: 1.2015 - val_acc: 0.7214\n",
      "Epoch 2318/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3966 - acc: 0.8567 - val_loss: 1.4173 - val_acc: 0.6759\n",
      "Epoch 2319/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4110 - acc: 0.8509 - val_loss: 1.5359 - val_acc: 0.6429\n",
      "Epoch 2320/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4202 - acc: 0.8536 - val_loss: 1.4061 - val_acc: 0.6705\n",
      "Epoch 2321/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4058 - acc: 0.8549 - val_loss: 1.3651 - val_acc: 0.6330\n",
      "Epoch 2322/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4097 - acc: 0.8543 - val_loss: 1.3254 - val_acc: 0.6830\n",
      "Epoch 2323/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3948 - acc: 0.8564 - val_loss: 1.3524 - val_acc: 0.6982\n",
      "Epoch 2324/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4125 - acc: 0.8509 - val_loss: 1.4601 - val_acc: 0.6652\n",
      "Epoch 2325/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4145 - acc: 0.8512 - val_loss: 1.4399 - val_acc: 0.6643\n",
      "Epoch 2326/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4061 - acc: 0.8551 - val_loss: 1.2676 - val_acc: 0.6688\n",
      "Epoch 2327/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4067 - acc: 0.8532 - val_loss: 1.2913 - val_acc: 0.6830\n",
      "Epoch 2328/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4057 - acc: 0.8517 - val_loss: 1.5668 - val_acc: 0.6509\n",
      "Epoch 2329/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4285 - acc: 0.8479 - val_loss: 1.3567 - val_acc: 0.6438\n",
      "Epoch 2330/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4101 - acc: 0.8510 - val_loss: 1.4583 - val_acc: 0.6857\n",
      "Epoch 2331/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4086 - acc: 0.8531 - val_loss: 1.3002 - val_acc: 0.6741\n",
      "Epoch 2332/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4045 - acc: 0.8574 - val_loss: 1.4754 - val_acc: 0.6750\n",
      "Epoch 2333/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4067 - acc: 0.8554 - val_loss: 1.4918 - val_acc: 0.6277\n",
      "Epoch 2334/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4094 - acc: 0.8554 - val_loss: 1.3148 - val_acc: 0.6893\n",
      "Epoch 2335/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4113 - acc: 0.8526 - val_loss: 1.4783 - val_acc: 0.6598\n",
      "Epoch 2336/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4052 - acc: 0.8541 - val_loss: 1.4371 - val_acc: 0.6679\n",
      "Epoch 2337/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3991 - acc: 0.8575 - val_loss: 2.1145 - val_acc: 0.5536\n",
      "Epoch 2338/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4416 - acc: 0.8455 - val_loss: 1.2671 - val_acc: 0.6848\n",
      "Epoch 2339/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3857 - acc: 0.8603 - val_loss: 1.4472 - val_acc: 0.6830\n",
      "Epoch 2340/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4079 - acc: 0.8536 - val_loss: 1.3736 - val_acc: 0.6830\n",
      "Epoch 2341/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4024 - acc: 0.8552 - val_loss: 1.6426 - val_acc: 0.6009\n",
      "Epoch 2342/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4268 - acc: 0.8467 - val_loss: 1.5600 - val_acc: 0.6268\n",
      "Epoch 2343/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4116 - acc: 0.8521 - val_loss: 1.8709 - val_acc: 0.6259\n",
      "Epoch 2344/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4274 - acc: 0.8508 - val_loss: 1.4246 - val_acc: 0.6500\n",
      "Epoch 2345/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4286 - acc: 0.8489 - val_loss: 1.2776 - val_acc: 0.6946\n",
      "Epoch 2346/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4036 - acc: 0.8535 - val_loss: 1.4913 - val_acc: 0.6652\n",
      "Epoch 2347/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4005 - acc: 0.8571 - val_loss: 1.3737 - val_acc: 0.6589\n",
      "Epoch 2348/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4046 - acc: 0.8565 - val_loss: 1.3671 - val_acc: 0.6750\n",
      "Epoch 2349/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3973 - acc: 0.8581 - val_loss: 1.3398 - val_acc: 0.6348\n",
      "Epoch 2350/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4333 - acc: 0.8438 - val_loss: 1.2767 - val_acc: 0.6893\n",
      "Epoch 2351/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3957 - acc: 0.8594 - val_loss: 1.6102 - val_acc: 0.6455\n",
      "Epoch 2352/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4262 - acc: 0.8503 - val_loss: 1.6994 - val_acc: 0.6348\n",
      "Epoch 2353/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4073 - acc: 0.8558 - val_loss: 1.7351 - val_acc: 0.6491\n",
      "Epoch 2354/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3985 - acc: 0.8559 - val_loss: 1.3867 - val_acc: 0.6893\n",
      "Epoch 2355/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4209 - acc: 0.8480 - val_loss: 1.2178 - val_acc: 0.6955\n",
      "Epoch 2356/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3966 - acc: 0.8554 - val_loss: 1.7273 - val_acc: 0.6196\n",
      "Epoch 2357/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4273 - acc: 0.8521 - val_loss: 1.4041 - val_acc: 0.6723\n",
      "Epoch 2358/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3957 - acc: 0.8559 - val_loss: 1.5463 - val_acc: 0.6518\n",
      "Epoch 2359/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4289 - acc: 0.8521 - val_loss: 1.3378 - val_acc: 0.6625\n",
      "Epoch 2360/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4046 - acc: 0.8533 - val_loss: 1.5949 - val_acc: 0.6473\n",
      "Epoch 2361/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4086 - acc: 0.8557 - val_loss: 1.3208 - val_acc: 0.6625\n",
      "Epoch 2362/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4278 - acc: 0.8476 - val_loss: 1.3539 - val_acc: 0.6625\n",
      "Epoch 2363/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4292 - acc: 0.8452 - val_loss: 1.3714 - val_acc: 0.6536\n",
      "Epoch 2364/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4124 - acc: 0.8517 - val_loss: 1.3974 - val_acc: 0.6777\n",
      "Epoch 2365/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4317 - acc: 0.8447 - val_loss: 1.3107 - val_acc: 0.7107\n",
      "Epoch 2366/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3892 - acc: 0.8603 - val_loss: 1.5936 - val_acc: 0.6446\n",
      "Epoch 2367/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4073 - acc: 0.8535 - val_loss: 1.4156 - val_acc: 0.6589\n",
      "Epoch 2368/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4065 - acc: 0.8519 - val_loss: 1.6123 - val_acc: 0.6116\n",
      "Epoch 2369/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4244 - acc: 0.8487 - val_loss: 1.3623 - val_acc: 0.7000\n",
      "Epoch 2370/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4071 - acc: 0.8502 - val_loss: 1.6468 - val_acc: 0.6313\n",
      "Epoch 2371/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4075 - acc: 0.8533 - val_loss: 1.9421 - val_acc: 0.5821\n",
      "Epoch 2372/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4286 - acc: 0.8529 - val_loss: 1.3795 - val_acc: 0.6482\n",
      "Epoch 2373/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4116 - acc: 0.8499 - val_loss: 1.6500 - val_acc: 0.5929\n",
      "Epoch 2374/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4183 - acc: 0.8509 - val_loss: 1.5051 - val_acc: 0.6429\n",
      "Epoch 2375/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4107 - acc: 0.8509 - val_loss: 1.3804 - val_acc: 0.6982\n",
      "Epoch 2376/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3964 - acc: 0.8577 - val_loss: 1.2324 - val_acc: 0.6812\n",
      "Epoch 2377/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3983 - acc: 0.8546 - val_loss: 1.2368 - val_acc: 0.6821\n",
      "Epoch 2378/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3951 - acc: 0.8591 - val_loss: 1.5468 - val_acc: 0.6598\n",
      "Epoch 2379/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4080 - acc: 0.8560 - val_loss: 2.0955 - val_acc: 0.5884\n",
      "Epoch 2380/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4125 - acc: 0.8555 - val_loss: 1.5346 - val_acc: 0.6241\n",
      "Epoch 2381/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4078 - acc: 0.8552 - val_loss: 1.1701 - val_acc: 0.7089\n",
      "Epoch 2382/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4025 - acc: 0.8569 - val_loss: 1.6088 - val_acc: 0.6161\n",
      "Epoch 2383/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4157 - acc: 0.8521 - val_loss: 1.5065 - val_acc: 0.6500\n",
      "Epoch 2384/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4189 - acc: 0.8477 - val_loss: 1.4399 - val_acc: 0.6813\n",
      "Epoch 2385/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3994 - acc: 0.8541 - val_loss: 1.6771 - val_acc: 0.6179\n",
      "Epoch 2386/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4052 - acc: 0.8548 - val_loss: 1.3549 - val_acc: 0.6509\n",
      "Epoch 2387/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3974 - acc: 0.8560 - val_loss: 1.2643 - val_acc: 0.7295\n",
      "Epoch 2388/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3808 - acc: 0.8602 - val_loss: 1.4222 - val_acc: 0.6732\n",
      "Epoch 2389/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3926 - acc: 0.8569 - val_loss: 2.0356 - val_acc: 0.6027\n",
      "Epoch 2390/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4170 - acc: 0.8560 - val_loss: 1.5562 - val_acc: 0.6277\n",
      "Epoch 2391/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4334 - acc: 0.8497 - val_loss: 1.4033 - val_acc: 0.6402\n",
      "Epoch 2392/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4118 - acc: 0.8484 - val_loss: 1.5408 - val_acc: 0.6446\n",
      "Epoch 2393/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4204 - acc: 0.8507 - val_loss: 1.1505 - val_acc: 0.7143\n",
      "Epoch 2394/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3815 - acc: 0.8616 - val_loss: 1.5717 - val_acc: 0.6420\n",
      "Epoch 2395/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4040 - acc: 0.8545 - val_loss: 1.5864 - val_acc: 0.6455\n",
      "Epoch 2396/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4192 - acc: 0.8525 - val_loss: 1.1706 - val_acc: 0.6911\n",
      "Epoch 2397/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3977 - acc: 0.8578 - val_loss: 1.3168 - val_acc: 0.6982\n",
      "Epoch 2398/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4007 - acc: 0.8533 - val_loss: 1.7081 - val_acc: 0.6330\n",
      "Epoch 2399/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4261 - acc: 0.8499 - val_loss: 1.5840 - val_acc: 0.6527\n",
      "Epoch 2400/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4032 - acc: 0.8534 - val_loss: 1.7296 - val_acc: 0.6152\n",
      "Epoch 2401/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4078 - acc: 0.8524 - val_loss: 1.5439 - val_acc: 0.6464\n",
      "Epoch 2402/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4034 - acc: 0.8550 - val_loss: 1.6255 - val_acc: 0.6357\n",
      "Epoch 2403/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3976 - acc: 0.8609 - val_loss: 1.3348 - val_acc: 0.6687\n",
      "Epoch 2404/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3941 - acc: 0.8585 - val_loss: 1.6609 - val_acc: 0.6714\n",
      "Epoch 2405/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4129 - acc: 0.8514 - val_loss: 1.5039 - val_acc: 0.6509\n",
      "Epoch 2406/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3947 - acc: 0.8613 - val_loss: 2.1628 - val_acc: 0.5741\n",
      "Epoch 2407/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4415 - acc: 0.8513 - val_loss: 1.6038 - val_acc: 0.6179\n",
      "Epoch 2408/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3976 - acc: 0.8545 - val_loss: 1.4030 - val_acc: 0.7027\n",
      "Epoch 2409/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4109 - acc: 0.8555 - val_loss: 1.4364 - val_acc: 0.6839\n",
      "Epoch 2410/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3923 - acc: 0.8587 - val_loss: 1.9166 - val_acc: 0.6036\n",
      "Epoch 2411/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4148 - acc: 0.8553 - val_loss: 1.3875 - val_acc: 0.6875\n",
      "Epoch 2412/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3943 - acc: 0.8591 - val_loss: 1.4367 - val_acc: 0.7000\n",
      "Epoch 2413/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4092 - acc: 0.8547 - val_loss: 1.5648 - val_acc: 0.6616\n",
      "Epoch 2414/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3992 - acc: 0.8545 - val_loss: 1.7583 - val_acc: 0.6366\n",
      "Epoch 2415/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4298 - acc: 0.8487 - val_loss: 1.3309 - val_acc: 0.6732\n",
      "Epoch 2416/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4110 - acc: 0.8531 - val_loss: 1.1600 - val_acc: 0.6875\n",
      "Epoch 2417/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3926 - acc: 0.8561 - val_loss: 1.3467 - val_acc: 0.6188\n",
      "Epoch 2418/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3998 - acc: 0.8589 - val_loss: 1.5547 - val_acc: 0.6241\n",
      "Epoch 2419/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4131 - acc: 0.8465 - val_loss: 1.6784 - val_acc: 0.6429\n",
      "Epoch 2420/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4072 - acc: 0.8561 - val_loss: 1.2951 - val_acc: 0.6821\n",
      "Epoch 2421/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3984 - acc: 0.8537 - val_loss: 1.3925 - val_acc: 0.6473\n",
      "Epoch 2422/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3977 - acc: 0.8575 - val_loss: 1.5397 - val_acc: 0.6482\n",
      "Epoch 2423/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4010 - acc: 0.8567 - val_loss: 1.4631 - val_acc: 0.6580\n",
      "Epoch 2424/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3887 - acc: 0.8595 - val_loss: 1.4239 - val_acc: 0.6777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2425/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4072 - acc: 0.8555 - val_loss: 1.3277 - val_acc: 0.6455\n",
      "Epoch 2426/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4272 - acc: 0.8489 - val_loss: 1.4432 - val_acc: 0.6848\n",
      "Epoch 2427/5000\n",
      "15008/15008 [==============================] - 1s 82us/step - loss: 0.3890 - acc: 0.8612 - val_loss: 1.7620 - val_acc: 0.6054\n",
      "Epoch 2428/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4418 - acc: 0.8477 - val_loss: 1.4045 - val_acc: 0.6750\n",
      "Epoch 2429/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3929 - acc: 0.8593 - val_loss: 1.4040 - val_acc: 0.6545\n",
      "Epoch 2430/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4010 - acc: 0.8571 - val_loss: 1.7312 - val_acc: 0.6054\n",
      "Epoch 2431/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4071 - acc: 0.8561 - val_loss: 1.6906 - val_acc: 0.6411\n",
      "Epoch 2432/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4077 - acc: 0.8526 - val_loss: 1.8459 - val_acc: 0.5902\n",
      "Epoch 2433/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4036 - acc: 0.8557 - val_loss: 1.4079 - val_acc: 0.6562\n",
      "Epoch 2434/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4114 - acc: 0.8537 - val_loss: 1.3272 - val_acc: 0.6902\n",
      "Epoch 2435/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3930 - acc: 0.8571 - val_loss: 1.3329 - val_acc: 0.6598\n",
      "Epoch 2436/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4070 - acc: 0.8523 - val_loss: 1.3396 - val_acc: 0.6768\n",
      "Epoch 2437/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3925 - acc: 0.8577 - val_loss: 2.1026 - val_acc: 0.5821\n",
      "Epoch 2438/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4131 - acc: 0.8544 - val_loss: 1.4390 - val_acc: 0.6679\n",
      "Epoch 2439/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4130 - acc: 0.8546 - val_loss: 1.2010 - val_acc: 0.6929\n",
      "Epoch 2440/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3898 - acc: 0.8585 - val_loss: 1.2991 - val_acc: 0.6741\n",
      "Epoch 2441/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4050 - acc: 0.8539 - val_loss: 1.2768 - val_acc: 0.6964\n",
      "Epoch 2442/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3903 - acc: 0.8607 - val_loss: 1.3341 - val_acc: 0.6625\n",
      "Epoch 2443/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4028 - acc: 0.8544 - val_loss: 1.7198 - val_acc: 0.6277\n",
      "Epoch 2444/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4081 - acc: 0.8555 - val_loss: 1.3972 - val_acc: 0.6759\n",
      "Epoch 2445/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4100 - acc: 0.8525 - val_loss: 1.5923 - val_acc: 0.6527\n",
      "Epoch 2446/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4005 - acc: 0.8534 - val_loss: 1.7593 - val_acc: 0.5893\n",
      "Epoch 2447/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4067 - acc: 0.8570 - val_loss: 1.4356 - val_acc: 0.6312\n",
      "Epoch 2448/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4019 - acc: 0.8582 - val_loss: 1.2426 - val_acc: 0.6741\n",
      "Epoch 2449/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3973 - acc: 0.8557 - val_loss: 1.1995 - val_acc: 0.7107\n",
      "Epoch 2450/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3795 - acc: 0.8641 - val_loss: 1.3146 - val_acc: 0.6768\n",
      "Epoch 2451/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4148 - acc: 0.8482 - val_loss: 1.3494 - val_acc: 0.7018\n",
      "Epoch 2452/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3957 - acc: 0.8623 - val_loss: 1.7777 - val_acc: 0.6313\n",
      "Epoch 2453/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4019 - acc: 0.8556 - val_loss: 1.2794 - val_acc: 0.6946\n",
      "Epoch 2454/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3966 - acc: 0.8529 - val_loss: 1.2961 - val_acc: 0.6598\n",
      "Epoch 2455/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 0.3914 - acc: 0.8599 - val_loss: 1.4285 - val_acc: 0.6830\n",
      "Epoch 2456/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4053 - acc: 0.8555 - val_loss: 1.6189 - val_acc: 0.6152\n",
      "Epoch 2457/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3990 - acc: 0.8565 - val_loss: 1.2909 - val_acc: 0.6679\n",
      "Epoch 2458/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3936 - acc: 0.8568 - val_loss: 1.4537 - val_acc: 0.6598\n",
      "Epoch 2459/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3988 - acc: 0.8585 - val_loss: 1.9428 - val_acc: 0.6089\n",
      "Epoch 2460/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4058 - acc: 0.8553 - val_loss: 1.5032 - val_acc: 0.6429\n",
      "Epoch 2461/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3895 - acc: 0.8595 - val_loss: 2.0324 - val_acc: 0.5857\n",
      "Epoch 2462/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4177 - acc: 0.8538 - val_loss: 1.2741 - val_acc: 0.7134\n",
      "Epoch 2463/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3875 - acc: 0.8611 - val_loss: 1.4009 - val_acc: 0.6786\n",
      "Epoch 2464/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4061 - acc: 0.8559 - val_loss: 1.2059 - val_acc: 0.6911\n",
      "Epoch 2465/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3896 - acc: 0.8581 - val_loss: 1.2135 - val_acc: 0.7089\n",
      "Epoch 2466/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3832 - acc: 0.8609 - val_loss: 1.9861 - val_acc: 0.5920\n",
      "Epoch 2467/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 0.4090 - acc: 0.8573 - val_loss: 1.8570 - val_acc: 0.5911\n",
      "Epoch 2468/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.4313 - acc: 0.8485 - val_loss: 1.7961 - val_acc: 0.6152\n",
      "Epoch 2469/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4099 - acc: 0.8567 - val_loss: 1.3012 - val_acc: 0.6366\n",
      "Epoch 2470/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3955 - acc: 0.8601 - val_loss: 1.6700 - val_acc: 0.6455\n",
      "Epoch 2471/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3857 - acc: 0.8619 - val_loss: 1.4142 - val_acc: 0.6902\n",
      "Epoch 2472/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3966 - acc: 0.8589 - val_loss: 1.5659 - val_acc: 0.6375\n",
      "Epoch 2473/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3976 - acc: 0.8551 - val_loss: 1.8905 - val_acc: 0.5884\n",
      "Epoch 2474/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4154 - acc: 0.8565 - val_loss: 2.0683 - val_acc: 0.5830\n",
      "Epoch 2475/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4406 - acc: 0.8468 - val_loss: 1.4709 - val_acc: 0.6634\n",
      "Epoch 2476/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4041 - acc: 0.8564 - val_loss: 1.7943 - val_acc: 0.6018\n",
      "Epoch 2477/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4223 - acc: 0.8509 - val_loss: 1.4665 - val_acc: 0.6536\n",
      "Epoch 2478/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3868 - acc: 0.8617 - val_loss: 1.2227 - val_acc: 0.6768\n",
      "Epoch 2479/5000\n",
      "15008/15008 [==============================] - 1s 96us/step - loss: 0.3918 - acc: 0.8572 - val_loss: 1.6170 - val_acc: 0.6696\n",
      "Epoch 2480/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.4000 - acc: 0.8594 - val_loss: 1.3299 - val_acc: 0.6866\n",
      "Epoch 2481/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3831 - acc: 0.8594 - val_loss: 1.4648 - val_acc: 0.6607\n",
      "Epoch 2482/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3988 - acc: 0.8564 - val_loss: 1.4851 - val_acc: 0.6563\n",
      "Epoch 2483/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3987 - acc: 0.8545 - val_loss: 1.3245 - val_acc: 0.6902\n",
      "Epoch 2484/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3979 - acc: 0.8562 - val_loss: 1.5430 - val_acc: 0.6500\n",
      "Epoch 2485/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4054 - acc: 0.8563 - val_loss: 1.4763 - val_acc: 0.6562\n",
      "Epoch 2486/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3838 - acc: 0.8629 - val_loss: 1.3459 - val_acc: 0.6759\n",
      "Epoch 2487/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3978 - acc: 0.8574 - val_loss: 1.4552 - val_acc: 0.6661\n",
      "Epoch 2488/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4095 - acc: 0.8522 - val_loss: 1.6490 - val_acc: 0.6125\n",
      "Epoch 2489/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3922 - acc: 0.8606 - val_loss: 1.3110 - val_acc: 0.6634\n",
      "Epoch 2490/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3995 - acc: 0.8541 - val_loss: 1.4205 - val_acc: 0.6759\n",
      "Epoch 2491/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.4024 - acc: 0.8569 - val_loss: 1.4200 - val_acc: 0.6866\n",
      "Epoch 2492/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3889 - acc: 0.8563 - val_loss: 1.5627 - val_acc: 0.6277\n",
      "Epoch 2493/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4014 - acc: 0.8548 - val_loss: 1.5597 - val_acc: 0.6732\n",
      "Epoch 2494/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3886 - acc: 0.8599 - val_loss: 1.7242 - val_acc: 0.6286\n",
      "Epoch 2495/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4325 - acc: 0.8513 - val_loss: 1.8229 - val_acc: 0.6134\n",
      "Epoch 2496/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3953 - acc: 0.8605 - val_loss: 1.7185 - val_acc: 0.6286\n",
      "Epoch 2497/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4077 - acc: 0.8579 - val_loss: 1.3040 - val_acc: 0.7036\n",
      "Epoch 2498/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3811 - acc: 0.8591 - val_loss: 1.2980 - val_acc: 0.6848\n",
      "Epoch 2499/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3878 - acc: 0.8580 - val_loss: 1.3482 - val_acc: 0.7062\n",
      "Epoch 2500/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3925 - acc: 0.8594 - val_loss: 1.5610 - val_acc: 0.6393\n",
      "Epoch 2501/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4013 - acc: 0.8564 - val_loss: 1.7554 - val_acc: 0.5946\n",
      "Epoch 2502/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4139 - acc: 0.8548 - val_loss: 1.5847 - val_acc: 0.6509\n",
      "Epoch 2503/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3808 - acc: 0.8628 - val_loss: 1.8935 - val_acc: 0.6143\n",
      "Epoch 2504/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.4285 - acc: 0.8498 - val_loss: 1.4266 - val_acc: 0.6929\n",
      "Epoch 2505/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3832 - acc: 0.8605 - val_loss: 1.3635 - val_acc: 0.6688\n",
      "Epoch 2506/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3825 - acc: 0.8608 - val_loss: 1.6656 - val_acc: 0.6384\n",
      "Epoch 2507/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4021 - acc: 0.8566 - val_loss: 1.2346 - val_acc: 0.6955\n",
      "Epoch 2508/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3873 - acc: 0.8592 - val_loss: 1.6249 - val_acc: 0.6509\n",
      "Epoch 2509/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3993 - acc: 0.8576 - val_loss: 1.3488 - val_acc: 0.6777\n",
      "Epoch 2510/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4019 - acc: 0.8549 - val_loss: 1.7267 - val_acc: 0.6384\n",
      "Epoch 2511/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4059 - acc: 0.8609 - val_loss: 2.1705 - val_acc: 0.5723\n",
      "Epoch 2512/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4001 - acc: 0.8605 - val_loss: 2.5935 - val_acc: 0.5330\n",
      "Epoch 2513/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4146 - acc: 0.8550 - val_loss: 1.6311 - val_acc: 0.6634\n",
      "Epoch 2514/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3893 - acc: 0.8598 - val_loss: 1.4797 - val_acc: 0.6402\n",
      "Epoch 2515/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3986 - acc: 0.8565 - val_loss: 1.8101 - val_acc: 0.6196\n",
      "Epoch 2516/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4164 - acc: 0.8522 - val_loss: 1.3279 - val_acc: 0.6795\n",
      "Epoch 2517/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3713 - acc: 0.8651 - val_loss: 1.3406 - val_acc: 0.6955\n",
      "Epoch 2518/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3920 - acc: 0.8560 - val_loss: 1.2671 - val_acc: 0.6830\n",
      "Epoch 2519/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3669 - acc: 0.8649 - val_loss: 1.4414 - val_acc: 0.6946\n",
      "Epoch 2520/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4029 - acc: 0.8565 - val_loss: 1.4175 - val_acc: 0.6679\n",
      "Epoch 2521/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3868 - acc: 0.8594 - val_loss: 1.6227 - val_acc: 0.6411\n",
      "Epoch 2522/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4227 - acc: 0.8499 - val_loss: 1.4067 - val_acc: 0.7071\n",
      "Epoch 2523/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3916 - acc: 0.8601 - val_loss: 1.2145 - val_acc: 0.7116\n",
      "Epoch 2524/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3893 - acc: 0.8597 - val_loss: 1.5500 - val_acc: 0.6491\n",
      "Epoch 2525/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3806 - acc: 0.8653 - val_loss: 1.4553 - val_acc: 0.6554\n",
      "Epoch 2526/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4072 - acc: 0.8518 - val_loss: 1.8560 - val_acc: 0.5920\n",
      "Epoch 2527/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3905 - acc: 0.8617 - val_loss: 1.4564 - val_acc: 0.6946\n",
      "Epoch 2528/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4033 - acc: 0.8579 - val_loss: 1.8234 - val_acc: 0.6179\n",
      "Epoch 2529/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.4112 - acc: 0.8565 - val_loss: 1.3201 - val_acc: 0.6723\n",
      "Epoch 2530/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3878 - acc: 0.8615 - val_loss: 1.4277 - val_acc: 0.6625\n",
      "Epoch 2531/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3808 - acc: 0.8620 - val_loss: 1.7518 - val_acc: 0.6527\n",
      "Epoch 2532/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4033 - acc: 0.8551 - val_loss: 1.4542 - val_acc: 0.6839\n",
      "Epoch 2533/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3809 - acc: 0.8623 - val_loss: 1.2629 - val_acc: 0.6964\n",
      "Epoch 2534/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3871 - acc: 0.8602 - val_loss: 1.3738 - val_acc: 0.6750\n",
      "Epoch 2535/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3681 - acc: 0.8669 - val_loss: 1.2385 - val_acc: 0.6920\n",
      "Epoch 2536/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3840 - acc: 0.8623 - val_loss: 1.7425 - val_acc: 0.6295\n",
      "Epoch 2537/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3917 - acc: 0.8628 - val_loss: 1.5202 - val_acc: 0.6652\n",
      "Epoch 2538/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3856 - acc: 0.8636 - val_loss: 1.1801 - val_acc: 0.7080\n",
      "Epoch 2539/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3710 - acc: 0.8685 - val_loss: 1.4655 - val_acc: 0.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2540/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3936 - acc: 0.8577 - val_loss: 1.8921 - val_acc: 0.6107\n",
      "Epoch 2541/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3888 - acc: 0.8623 - val_loss: 2.2215 - val_acc: 0.5562\n",
      "Epoch 2542/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4329 - acc: 0.8529 - val_loss: 1.3358 - val_acc: 0.7089\n",
      "Epoch 2543/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3793 - acc: 0.8624 - val_loss: 1.6046 - val_acc: 0.6098\n",
      "Epoch 2544/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3946 - acc: 0.8587 - val_loss: 1.3733 - val_acc: 0.6679\n",
      "Epoch 2545/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3820 - acc: 0.8591 - val_loss: 1.6149 - val_acc: 0.6830\n",
      "Epoch 2546/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3848 - acc: 0.8637 - val_loss: 1.4255 - val_acc: 0.6616\n",
      "Epoch 2547/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4081 - acc: 0.8562 - val_loss: 1.4171 - val_acc: 0.6937\n",
      "Epoch 2548/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3989 - acc: 0.8563 - val_loss: 1.4584 - val_acc: 0.6830\n",
      "Epoch 2549/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3947 - acc: 0.8559 - val_loss: 1.5444 - val_acc: 0.6509\n",
      "Epoch 2550/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3817 - acc: 0.8628 - val_loss: 1.2091 - val_acc: 0.7170\n",
      "Epoch 2551/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3804 - acc: 0.8600 - val_loss: 1.7957 - val_acc: 0.6161\n",
      "Epoch 2552/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3984 - acc: 0.8581 - val_loss: 1.2418 - val_acc: 0.7009\n",
      "Epoch 2553/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3830 - acc: 0.8631 - val_loss: 1.8049 - val_acc: 0.6491\n",
      "Epoch 2554/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3938 - acc: 0.8625 - val_loss: 1.4202 - val_acc: 0.7000\n",
      "Epoch 2555/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3869 - acc: 0.8614 - val_loss: 1.4249 - val_acc: 0.6839\n",
      "Epoch 2556/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3837 - acc: 0.8617 - val_loss: 1.4141 - val_acc: 0.6795\n",
      "Epoch 2557/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3903 - acc: 0.8601 - val_loss: 1.3255 - val_acc: 0.7161\n",
      "Epoch 2558/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3831 - acc: 0.8633 - val_loss: 1.4323 - val_acc: 0.6687\n",
      "Epoch 2559/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4025 - acc: 0.8576 - val_loss: 1.5598 - val_acc: 0.6464\n",
      "Epoch 2560/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3849 - acc: 0.8637 - val_loss: 1.1125 - val_acc: 0.7446\n",
      "Epoch 2561/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3686 - acc: 0.8673 - val_loss: 1.8831 - val_acc: 0.6348\n",
      "Epoch 2562/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4239 - acc: 0.8533 - val_loss: 1.6098 - val_acc: 0.6527\n",
      "Epoch 2563/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4107 - acc: 0.8550 - val_loss: 1.5640 - val_acc: 0.6554\n",
      "Epoch 2564/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3705 - acc: 0.8683 - val_loss: 1.5316 - val_acc: 0.6527\n",
      "Epoch 2565/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.4031 - acc: 0.8541 - val_loss: 1.3180 - val_acc: 0.6661\n",
      "Epoch 2566/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4017 - acc: 0.8537 - val_loss: 1.3826 - val_acc: 0.6937\n",
      "Epoch 2567/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3784 - acc: 0.8613 - val_loss: 1.4360 - val_acc: 0.6616\n",
      "Epoch 2568/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3917 - acc: 0.8596 - val_loss: 1.5374 - val_acc: 0.6714\n",
      "Epoch 2569/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3820 - acc: 0.8607 - val_loss: 1.6266 - val_acc: 0.6643\n",
      "Epoch 2570/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3911 - acc: 0.8589 - val_loss: 1.2990 - val_acc: 0.6937\n",
      "Epoch 2571/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3815 - acc: 0.8593 - val_loss: 2.4244 - val_acc: 0.5920\n",
      "Epoch 2572/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4100 - acc: 0.8635 - val_loss: 1.6121 - val_acc: 0.6643\n",
      "Epoch 2573/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3913 - acc: 0.8621 - val_loss: 1.4369 - val_acc: 0.6607\n",
      "Epoch 2574/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3961 - acc: 0.8590 - val_loss: 1.2727 - val_acc: 0.7045\n",
      "Epoch 2575/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3869 - acc: 0.8619 - val_loss: 1.4323 - val_acc: 0.6464\n",
      "Epoch 2576/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4089 - acc: 0.8509 - val_loss: 1.4125 - val_acc: 0.6777\n",
      "Epoch 2577/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3829 - acc: 0.8606 - val_loss: 1.4402 - val_acc: 0.6295\n",
      "Epoch 2578/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3889 - acc: 0.8599 - val_loss: 1.8355 - val_acc: 0.6107\n",
      "Epoch 2579/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4085 - acc: 0.8565 - val_loss: 1.6413 - val_acc: 0.6491\n",
      "Epoch 2580/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3853 - acc: 0.8642 - val_loss: 1.4437 - val_acc: 0.6741\n",
      "Epoch 2581/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3847 - acc: 0.8632 - val_loss: 1.5435 - val_acc: 0.6571\n",
      "Epoch 2582/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3840 - acc: 0.8634 - val_loss: 1.3234 - val_acc: 0.6598\n",
      "Epoch 2583/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3740 - acc: 0.8629 - val_loss: 1.8970 - val_acc: 0.6045\n",
      "Epoch 2584/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3940 - acc: 0.8603 - val_loss: 1.3417 - val_acc: 0.7018\n",
      "Epoch 2585/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3814 - acc: 0.8644 - val_loss: 1.4332 - val_acc: 0.6973\n",
      "Epoch 2586/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3899 - acc: 0.8629 - val_loss: 1.5352 - val_acc: 0.6625\n",
      "Epoch 2587/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3853 - acc: 0.8600 - val_loss: 1.4198 - val_acc: 0.6830\n",
      "Epoch 2588/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3940 - acc: 0.8582 - val_loss: 1.3873 - val_acc: 0.6902\n",
      "Epoch 2589/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3795 - acc: 0.8636 - val_loss: 1.4458 - val_acc: 0.7018\n",
      "Epoch 2590/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3879 - acc: 0.8611 - val_loss: 1.3176 - val_acc: 0.6812\n",
      "Epoch 2591/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3819 - acc: 0.8608 - val_loss: 1.2595 - val_acc: 0.7098\n",
      "Epoch 2592/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3797 - acc: 0.8636 - val_loss: 1.4242 - val_acc: 0.6723\n",
      "Epoch 2593/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3927 - acc: 0.8609 - val_loss: 1.3613 - val_acc: 0.6795\n",
      "Epoch 2594/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3828 - acc: 0.8608 - val_loss: 1.4572 - val_acc: 0.6911\n",
      "Epoch 2595/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4120 - acc: 0.8565 - val_loss: 1.3892 - val_acc: 0.6911\n",
      "Epoch 2596/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4132 - acc: 0.8552 - val_loss: 1.3984 - val_acc: 0.6571\n",
      "Epoch 2597/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3869 - acc: 0.8629 - val_loss: 1.5786 - val_acc: 0.6509\n",
      "Epoch 2598/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4054 - acc: 0.8562 - val_loss: 1.5390 - val_acc: 0.6384\n",
      "Epoch 2599/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3947 - acc: 0.8588 - val_loss: 1.2298 - val_acc: 0.6893\n",
      "Epoch 2600/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3682 - acc: 0.8651 - val_loss: 2.0199 - val_acc: 0.6125\n",
      "Epoch 2601/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4271 - acc: 0.8581 - val_loss: 1.3364 - val_acc: 0.7027\n",
      "Epoch 2602/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3859 - acc: 0.8596 - val_loss: 1.2176 - val_acc: 0.6705\n",
      "Epoch 2603/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3756 - acc: 0.8638 - val_loss: 1.2772 - val_acc: 0.6857\n",
      "Epoch 2604/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3883 - acc: 0.8609 - val_loss: 1.4087 - val_acc: 0.7045\n",
      "Epoch 2605/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3961 - acc: 0.8582 - val_loss: 1.3224 - val_acc: 0.6911\n",
      "Epoch 2606/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3843 - acc: 0.8595 - val_loss: 1.2362 - val_acc: 0.7125\n",
      "Epoch 2607/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3789 - acc: 0.8593 - val_loss: 1.5488 - val_acc: 0.6509\n",
      "Epoch 2608/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4000 - acc: 0.8571 - val_loss: 1.3619 - val_acc: 0.6563\n",
      "Epoch 2609/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3930 - acc: 0.8602 - val_loss: 1.6453 - val_acc: 0.6500\n",
      "Epoch 2610/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4064 - acc: 0.8570 - val_loss: 1.4668 - val_acc: 0.6812\n",
      "Epoch 2611/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3795 - acc: 0.8625 - val_loss: 1.4486 - val_acc: 0.6759\n",
      "Epoch 2612/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3780 - acc: 0.8639 - val_loss: 1.6429 - val_acc: 0.6563\n",
      "Epoch 2613/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3996 - acc: 0.8551 - val_loss: 1.6532 - val_acc: 0.6679\n",
      "Epoch 2614/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3832 - acc: 0.8627 - val_loss: 1.5907 - val_acc: 0.6491\n",
      "Epoch 2615/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3950 - acc: 0.8587 - val_loss: 1.6954 - val_acc: 0.6089\n",
      "Epoch 2616/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.4015 - acc: 0.8601 - val_loss: 1.2483 - val_acc: 0.7054\n",
      "Epoch 2617/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3870 - acc: 0.8579 - val_loss: 1.5132 - val_acc: 0.6661\n",
      "Epoch 2618/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3814 - acc: 0.8625 - val_loss: 1.5342 - val_acc: 0.6455\n",
      "Epoch 2619/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3881 - acc: 0.8583 - val_loss: 1.7290 - val_acc: 0.6268\n",
      "Epoch 2620/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3891 - acc: 0.8625 - val_loss: 1.4701 - val_acc: 0.6741\n",
      "Epoch 2621/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3864 - acc: 0.8591 - val_loss: 1.2728 - val_acc: 0.6929\n",
      "Epoch 2622/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3841 - acc: 0.8639 - val_loss: 1.4475 - val_acc: 0.6464\n",
      "Epoch 2623/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3905 - acc: 0.8577 - val_loss: 1.3854 - val_acc: 0.6821\n",
      "Epoch 2624/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3779 - acc: 0.8655 - val_loss: 2.2268 - val_acc: 0.5723\n",
      "Epoch 2625/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3934 - acc: 0.8642 - val_loss: 1.3600 - val_acc: 0.6982\n",
      "Epoch 2626/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3840 - acc: 0.8597 - val_loss: 1.4677 - val_acc: 0.6455\n",
      "Epoch 2627/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4028 - acc: 0.8581 - val_loss: 1.5363 - val_acc: 0.6687\n",
      "Epoch 2628/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3976 - acc: 0.8583 - val_loss: 1.2648 - val_acc: 0.7027\n",
      "Epoch 2629/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3701 - acc: 0.8678 - val_loss: 1.3957 - val_acc: 0.6857\n",
      "Epoch 2630/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3796 - acc: 0.8649 - val_loss: 1.3305 - val_acc: 0.6902\n",
      "Epoch 2631/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3858 - acc: 0.8611 - val_loss: 1.3697 - val_acc: 0.6786\n",
      "Epoch 2632/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3833 - acc: 0.8625 - val_loss: 1.5055 - val_acc: 0.6580\n",
      "Epoch 2633/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3877 - acc: 0.8621 - val_loss: 1.2928 - val_acc: 0.6964\n",
      "Epoch 2634/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3709 - acc: 0.8668 - val_loss: 1.3346 - val_acc: 0.6670\n",
      "Epoch 2635/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3847 - acc: 0.8599 - val_loss: 1.3370 - val_acc: 0.6839\n",
      "Epoch 2636/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3719 - acc: 0.8627 - val_loss: 1.8368 - val_acc: 0.6205\n",
      "Epoch 2637/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3978 - acc: 0.8597 - val_loss: 1.3489 - val_acc: 0.6723\n",
      "Epoch 2638/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3850 - acc: 0.8598 - val_loss: 1.8449 - val_acc: 0.6437\n",
      "Epoch 2639/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3855 - acc: 0.8623 - val_loss: 1.3601 - val_acc: 0.6884\n",
      "Epoch 2640/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3893 - acc: 0.8611 - val_loss: 1.5990 - val_acc: 0.6277\n",
      "Epoch 2641/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3745 - acc: 0.8645 - val_loss: 1.6926 - val_acc: 0.6348\n",
      "Epoch 2642/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.4045 - acc: 0.8587 - val_loss: 1.3199 - val_acc: 0.6982\n",
      "Epoch 2643/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3850 - acc: 0.8611 - val_loss: 1.5724 - val_acc: 0.6143\n",
      "Epoch 2644/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4004 - acc: 0.8549 - val_loss: 1.4083 - val_acc: 0.6616\n",
      "Epoch 2645/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3859 - acc: 0.8621 - val_loss: 1.5433 - val_acc: 0.6455\n",
      "Epoch 2646/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3937 - acc: 0.8585 - val_loss: 1.3193 - val_acc: 0.6902\n",
      "Epoch 2647/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3824 - acc: 0.8631 - val_loss: 1.3769 - val_acc: 0.6750\n",
      "Epoch 2648/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3843 - acc: 0.8582 - val_loss: 1.5312 - val_acc: 0.6545\n",
      "Epoch 2649/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3808 - acc: 0.8637 - val_loss: 1.3809 - val_acc: 0.6937\n",
      "Epoch 2650/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3744 - acc: 0.8670 - val_loss: 1.6756 - val_acc: 0.6196\n",
      "Epoch 2651/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4030 - acc: 0.8595 - val_loss: 1.4044 - val_acc: 0.6821\n",
      "Epoch 2652/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3777 - acc: 0.8631 - val_loss: 1.5661 - val_acc: 0.6375\n",
      "Epoch 2653/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4049 - acc: 0.8575 - val_loss: 1.7121 - val_acc: 0.6464\n",
      "Epoch 2654/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3795 - acc: 0.8628 - val_loss: 1.3315 - val_acc: 0.6821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2655/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3679 - acc: 0.8668 - val_loss: 1.3027 - val_acc: 0.6946\n",
      "Epoch 2656/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3811 - acc: 0.8593 - val_loss: 1.6540 - val_acc: 0.6643\n",
      "Epoch 2657/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3837 - acc: 0.8640 - val_loss: 1.5299 - val_acc: 0.6491\n",
      "Epoch 2658/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3858 - acc: 0.8615 - val_loss: 1.3347 - val_acc: 0.6893\n",
      "Epoch 2659/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3695 - acc: 0.8653 - val_loss: 1.3257 - val_acc: 0.6982\n",
      "Epoch 2660/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3776 - acc: 0.8617 - val_loss: 1.4484 - val_acc: 0.7027\n",
      "Epoch 2661/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3683 - acc: 0.8667 - val_loss: 1.3251 - val_acc: 0.6857\n",
      "Epoch 2662/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3897 - acc: 0.8613 - val_loss: 1.7862 - val_acc: 0.6232\n",
      "Epoch 2663/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3830 - acc: 0.8675 - val_loss: 1.6034 - val_acc: 0.6446\n",
      "Epoch 2664/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3774 - acc: 0.8656 - val_loss: 1.1718 - val_acc: 0.7196\n",
      "Epoch 2665/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3792 - acc: 0.8649 - val_loss: 1.2970 - val_acc: 0.6750\n",
      "Epoch 2666/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3844 - acc: 0.8591 - val_loss: 1.4149 - val_acc: 0.6732\n",
      "Epoch 2667/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3666 - acc: 0.8638 - val_loss: 1.4979 - val_acc: 0.6562\n",
      "Epoch 2668/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3864 - acc: 0.8624 - val_loss: 1.3046 - val_acc: 0.6893\n",
      "Epoch 2669/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3736 - acc: 0.8646 - val_loss: 1.4236 - val_acc: 0.7170\n",
      "Epoch 2670/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3878 - acc: 0.8613 - val_loss: 1.4559 - val_acc: 0.6607\n",
      "Epoch 2671/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3869 - acc: 0.8615 - val_loss: 1.4616 - val_acc: 0.6839\n",
      "Epoch 2672/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3899 - acc: 0.8609 - val_loss: 1.7718 - val_acc: 0.6589\n",
      "Epoch 2673/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3894 - acc: 0.8639 - val_loss: 1.4873 - val_acc: 0.6866\n",
      "Epoch 2674/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3742 - acc: 0.8652 - val_loss: 1.3067 - val_acc: 0.7018\n",
      "Epoch 2675/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3609 - acc: 0.8685 - val_loss: 1.7420 - val_acc: 0.6402\n",
      "Epoch 2676/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4071 - acc: 0.8555 - val_loss: 1.2311 - val_acc: 0.7205\n",
      "Epoch 2677/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3744 - acc: 0.8643 - val_loss: 1.6535 - val_acc: 0.6625\n",
      "Epoch 2678/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3877 - acc: 0.8603 - val_loss: 1.4477 - val_acc: 0.6964\n",
      "Epoch 2679/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3910 - acc: 0.8593 - val_loss: 1.7605 - val_acc: 0.6455\n",
      "Epoch 2680/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3878 - acc: 0.8650 - val_loss: 1.6381 - val_acc: 0.6375\n",
      "Epoch 2681/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3774 - acc: 0.8669 - val_loss: 1.3590 - val_acc: 0.6946\n",
      "Epoch 2682/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3837 - acc: 0.8617 - val_loss: 1.3181 - val_acc: 0.6679\n",
      "Epoch 2683/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3725 - acc: 0.8685 - val_loss: 1.5114 - val_acc: 0.6652\n",
      "Epoch 2684/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3810 - acc: 0.8626 - val_loss: 2.0249 - val_acc: 0.6009\n",
      "Epoch 2685/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4109 - acc: 0.8597 - val_loss: 1.3981 - val_acc: 0.7009\n",
      "Epoch 2686/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3869 - acc: 0.8595 - val_loss: 1.2635 - val_acc: 0.6795\n",
      "Epoch 2687/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3783 - acc: 0.8647 - val_loss: 1.7531 - val_acc: 0.6437\n",
      "Epoch 2688/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3888 - acc: 0.8627 - val_loss: 1.5585 - val_acc: 0.6643\n",
      "Epoch 2689/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3598 - acc: 0.8707 - val_loss: 1.5095 - val_acc: 0.6580\n",
      "Epoch 2690/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3818 - acc: 0.8633 - val_loss: 1.7474 - val_acc: 0.6152\n",
      "Epoch 2691/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3916 - acc: 0.8601 - val_loss: 1.5831 - val_acc: 0.6339\n",
      "Epoch 2692/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.4178 - acc: 0.8525 - val_loss: 1.5343 - val_acc: 0.6536\n",
      "Epoch 2693/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3733 - acc: 0.8659 - val_loss: 1.6747 - val_acc: 0.6652\n",
      "Epoch 2694/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3861 - acc: 0.8638 - val_loss: 1.4066 - val_acc: 0.6509\n",
      "Epoch 2695/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3904 - acc: 0.8562 - val_loss: 1.2393 - val_acc: 0.6875\n",
      "Epoch 2696/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3761 - acc: 0.8655 - val_loss: 1.5513 - val_acc: 0.6661\n",
      "Epoch 2697/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3916 - acc: 0.8597 - val_loss: 1.5130 - val_acc: 0.7009\n",
      "Epoch 2698/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3759 - acc: 0.8646 - val_loss: 1.3589 - val_acc: 0.6875\n",
      "Epoch 2699/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3825 - acc: 0.8644 - val_loss: 1.3649 - val_acc: 0.7036\n",
      "Epoch 2700/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3741 - acc: 0.8651 - val_loss: 1.7767 - val_acc: 0.6402\n",
      "Epoch 2701/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4062 - acc: 0.8559 - val_loss: 1.7817 - val_acc: 0.6036\n",
      "Epoch 2702/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3899 - acc: 0.8593 - val_loss: 1.6477 - val_acc: 0.6545\n",
      "Epoch 2703/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3933 - acc: 0.8610 - val_loss: 1.9218 - val_acc: 0.6054\n",
      "Epoch 2704/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3789 - acc: 0.8679 - val_loss: 1.4589 - val_acc: 0.6804\n",
      "Epoch 2705/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3915 - acc: 0.8582 - val_loss: 1.5487 - val_acc: 0.6420\n",
      "Epoch 2706/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3934 - acc: 0.8591 - val_loss: 1.7388 - val_acc: 0.6732\n",
      "Epoch 2707/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3760 - acc: 0.8673 - val_loss: 1.5415 - val_acc: 0.6732\n",
      "Epoch 2708/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3954 - acc: 0.8603 - val_loss: 1.5735 - val_acc: 0.6420\n",
      "Epoch 2709/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3718 - acc: 0.8695 - val_loss: 1.3773 - val_acc: 0.6580\n",
      "Epoch 2710/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3962 - acc: 0.8575 - val_loss: 1.4543 - val_acc: 0.6884\n",
      "Epoch 2711/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3637 - acc: 0.8682 - val_loss: 1.4219 - val_acc: 0.6518\n",
      "Epoch 2712/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3969 - acc: 0.8578 - val_loss: 1.5868 - val_acc: 0.6661\n",
      "Epoch 2713/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3846 - acc: 0.8651 - val_loss: 1.2404 - val_acc: 0.7080\n",
      "Epoch 2714/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3739 - acc: 0.8647 - val_loss: 1.2828 - val_acc: 0.6786\n",
      "Epoch 2715/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3679 - acc: 0.8667 - val_loss: 1.3797 - val_acc: 0.6750\n",
      "Epoch 2716/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3764 - acc: 0.8661 - val_loss: 1.3250 - val_acc: 0.7018\n",
      "Epoch 2717/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3781 - acc: 0.8652 - val_loss: 1.7882 - val_acc: 0.6116\n",
      "Epoch 2718/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3925 - acc: 0.8607 - val_loss: 1.3239 - val_acc: 0.6848\n",
      "Epoch 2719/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3588 - acc: 0.8719 - val_loss: 1.3265 - val_acc: 0.6429\n",
      "Epoch 2720/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3768 - acc: 0.8603 - val_loss: 1.5928 - val_acc: 0.6473\n",
      "Epoch 2721/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3691 - acc: 0.8689 - val_loss: 1.5371 - val_acc: 0.6893\n",
      "Epoch 2722/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3978 - acc: 0.8638 - val_loss: 1.4666 - val_acc: 0.6732\n",
      "Epoch 2723/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3896 - acc: 0.8585 - val_loss: 1.4017 - val_acc: 0.6848\n",
      "Epoch 2724/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3770 - acc: 0.8661 - val_loss: 1.3404 - val_acc: 0.6946\n",
      "Epoch 2725/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3815 - acc: 0.8641 - val_loss: 1.4813 - val_acc: 0.6759\n",
      "Epoch 2726/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3741 - acc: 0.8643 - val_loss: 1.2998 - val_acc: 0.6786\n",
      "Epoch 2727/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3687 - acc: 0.8663 - val_loss: 1.9722 - val_acc: 0.6125\n",
      "Epoch 2728/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3802 - acc: 0.8650 - val_loss: 1.5319 - val_acc: 0.6688\n",
      "Epoch 2729/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3803 - acc: 0.8666 - val_loss: 1.5710 - val_acc: 0.6679\n",
      "Epoch 2730/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3826 - acc: 0.8623 - val_loss: 1.8037 - val_acc: 0.6125\n",
      "Epoch 2731/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3819 - acc: 0.8644 - val_loss: 1.6266 - val_acc: 0.6304\n",
      "Epoch 2732/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3976 - acc: 0.8565 - val_loss: 1.4457 - val_acc: 0.6580\n",
      "Epoch 2733/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3705 - acc: 0.8689 - val_loss: 1.4502 - val_acc: 0.6607\n",
      "Epoch 2734/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3978 - acc: 0.8535 - val_loss: 1.3860 - val_acc: 0.6589\n",
      "Epoch 2735/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3839 - acc: 0.8629 - val_loss: 1.4738 - val_acc: 0.6750\n",
      "Epoch 2736/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3733 - acc: 0.8669 - val_loss: 1.5655 - val_acc: 0.6500\n",
      "Epoch 2737/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3759 - acc: 0.8653 - val_loss: 1.4733 - val_acc: 0.6884\n",
      "Epoch 2738/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3823 - acc: 0.8615 - val_loss: 1.2982 - val_acc: 0.7179\n",
      "Epoch 2739/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3730 - acc: 0.8641 - val_loss: 1.4815 - val_acc: 0.6429\n",
      "Epoch 2740/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3855 - acc: 0.8619 - val_loss: 1.2474 - val_acc: 0.7134\n",
      "Epoch 2741/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.3589 - acc: 0.8665 - val_loss: 1.6457 - val_acc: 0.6464\n",
      "Epoch 2742/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3793 - acc: 0.8629 - val_loss: 1.2331 - val_acc: 0.7188\n",
      "Epoch 2743/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3638 - acc: 0.8659 - val_loss: 1.6015 - val_acc: 0.6545\n",
      "Epoch 2744/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3782 - acc: 0.8656 - val_loss: 1.6036 - val_acc: 0.6295\n",
      "Epoch 2745/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3728 - acc: 0.8701 - val_loss: 1.5764 - val_acc: 0.6571\n",
      "Epoch 2746/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3898 - acc: 0.8658 - val_loss: 1.6646 - val_acc: 0.6652\n",
      "Epoch 2747/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3686 - acc: 0.8679 - val_loss: 1.8028 - val_acc: 0.6062\n",
      "Epoch 2748/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3795 - acc: 0.8648 - val_loss: 1.3811 - val_acc: 0.6893\n",
      "Epoch 2749/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3626 - acc: 0.8677 - val_loss: 1.6016 - val_acc: 0.6536\n",
      "Epoch 2750/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3839 - acc: 0.8628 - val_loss: 1.3288 - val_acc: 0.6920\n",
      "Epoch 2751/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3689 - acc: 0.8673 - val_loss: 1.4639 - val_acc: 0.6830\n",
      "Epoch 2752/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3813 - acc: 0.8635 - val_loss: 1.4189 - val_acc: 0.6643\n",
      "Epoch 2753/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3725 - acc: 0.8672 - val_loss: 1.8468 - val_acc: 0.5946\n",
      "Epoch 2754/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3804 - acc: 0.8669 - val_loss: 1.4348 - val_acc: 0.6312\n",
      "Epoch 2755/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3820 - acc: 0.8612 - val_loss: 1.7598 - val_acc: 0.6107\n",
      "Epoch 2756/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3776 - acc: 0.8619 - val_loss: 1.4790 - val_acc: 0.6545\n",
      "Epoch 2757/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3849 - acc: 0.8639 - val_loss: 2.0569 - val_acc: 0.5795\n",
      "Epoch 2758/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4007 - acc: 0.8589 - val_loss: 1.5420 - val_acc: 0.6571\n",
      "Epoch 2759/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3590 - acc: 0.8737 - val_loss: 1.6406 - val_acc: 0.6714\n",
      "Epoch 2760/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3683 - acc: 0.8685 - val_loss: 1.5589 - val_acc: 0.6607\n",
      "Epoch 2761/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3922 - acc: 0.8600 - val_loss: 1.2231 - val_acc: 0.7152\n",
      "Epoch 2762/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3727 - acc: 0.8643 - val_loss: 1.3126 - val_acc: 0.6946\n",
      "Epoch 2763/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3626 - acc: 0.8696 - val_loss: 1.7280 - val_acc: 0.6187\n",
      "Epoch 2764/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.4060 - acc: 0.8586 - val_loss: 1.4267 - val_acc: 0.6705\n",
      "Epoch 2765/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3687 - acc: 0.8679 - val_loss: 1.7117 - val_acc: 0.6214\n",
      "Epoch 2766/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3849 - acc: 0.8655 - val_loss: 1.4955 - val_acc: 0.6339\n",
      "Epoch 2767/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3796 - acc: 0.8665 - val_loss: 2.0805 - val_acc: 0.5821\n",
      "Epoch 2768/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3915 - acc: 0.8645 - val_loss: 1.4260 - val_acc: 0.6446\n",
      "Epoch 2769/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3741 - acc: 0.8635 - val_loss: 1.3041 - val_acc: 0.6616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2770/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3772 - acc: 0.8651 - val_loss: 1.5895 - val_acc: 0.6536\n",
      "Epoch 2771/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3872 - acc: 0.8632 - val_loss: 1.4734 - val_acc: 0.6509\n",
      "Epoch 2772/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3612 - acc: 0.8683 - val_loss: 1.8141 - val_acc: 0.6018\n",
      "Epoch 2773/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3735 - acc: 0.8662 - val_loss: 1.4224 - val_acc: 0.7054\n",
      "Epoch 2774/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3724 - acc: 0.8657 - val_loss: 1.5244 - val_acc: 0.6812\n",
      "Epoch 2775/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3701 - acc: 0.8679 - val_loss: 1.3229 - val_acc: 0.6839\n",
      "Epoch 2776/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3738 - acc: 0.8633 - val_loss: 1.5961 - val_acc: 0.6902\n",
      "Epoch 2777/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3645 - acc: 0.8677 - val_loss: 2.0166 - val_acc: 0.6527\n",
      "Epoch 2778/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3884 - acc: 0.8636 - val_loss: 1.4912 - val_acc: 0.6813\n",
      "Epoch 2779/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3509 - acc: 0.8719 - val_loss: 1.5105 - val_acc: 0.6857\n",
      "Epoch 2780/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3727 - acc: 0.8675 - val_loss: 1.8234 - val_acc: 0.6402\n",
      "Epoch 2781/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3882 - acc: 0.8671 - val_loss: 1.8170 - val_acc: 0.6179\n",
      "Epoch 2782/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3854 - acc: 0.8617 - val_loss: 1.5301 - val_acc: 0.6714\n",
      "Epoch 2783/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3908 - acc: 0.8616 - val_loss: 1.5337 - val_acc: 0.6527\n",
      "Epoch 2784/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3765 - acc: 0.8627 - val_loss: 1.4911 - val_acc: 0.6723\n",
      "Epoch 2785/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3705 - acc: 0.8677 - val_loss: 1.4755 - val_acc: 0.6598\n",
      "Epoch 2786/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3886 - acc: 0.8643 - val_loss: 1.4466 - val_acc: 0.6607\n",
      "Epoch 2787/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3957 - acc: 0.8584 - val_loss: 1.4140 - val_acc: 0.6455\n",
      "Epoch 2788/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3600 - acc: 0.8705 - val_loss: 1.6082 - val_acc: 0.6125\n",
      "Epoch 2789/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3831 - acc: 0.8665 - val_loss: 1.7430 - val_acc: 0.6375\n",
      "Epoch 2790/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3743 - acc: 0.8673 - val_loss: 2.1668 - val_acc: 0.5750\n",
      "Epoch 2791/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.4020 - acc: 0.8599 - val_loss: 1.6900 - val_acc: 0.6429\n",
      "Epoch 2792/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3732 - acc: 0.8669 - val_loss: 1.4218 - val_acc: 0.6821\n",
      "Epoch 2793/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3863 - acc: 0.8635 - val_loss: 1.8239 - val_acc: 0.6054\n",
      "Epoch 2794/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3778 - acc: 0.8657 - val_loss: 1.5780 - val_acc: 0.6795\n",
      "Epoch 2795/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3757 - acc: 0.8624 - val_loss: 1.3654 - val_acc: 0.6777\n",
      "Epoch 2796/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3621 - acc: 0.8689 - val_loss: 1.5296 - val_acc: 0.6598\n",
      "Epoch 2797/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3599 - acc: 0.8705 - val_loss: 1.5744 - val_acc: 0.6518\n",
      "Epoch 2798/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3768 - acc: 0.8661 - val_loss: 1.4833 - val_acc: 0.6661\n",
      "Epoch 2799/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3676 - acc: 0.8679 - val_loss: 1.9274 - val_acc: 0.6143\n",
      "Epoch 2800/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3766 - acc: 0.8670 - val_loss: 1.3948 - val_acc: 0.6777\n",
      "Epoch 2801/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3829 - acc: 0.8599 - val_loss: 1.3958 - val_acc: 0.6893\n",
      "Epoch 2802/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3619 - acc: 0.8688 - val_loss: 1.5045 - val_acc: 0.6777\n",
      "Epoch 2803/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3643 - acc: 0.8667 - val_loss: 1.4632 - val_acc: 0.6661\n",
      "Epoch 2804/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3607 - acc: 0.8663 - val_loss: 1.2712 - val_acc: 0.7027\n",
      "Epoch 2805/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3633 - acc: 0.8684 - val_loss: 1.2415 - val_acc: 0.6875\n",
      "Epoch 2806/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3740 - acc: 0.8643 - val_loss: 1.5939 - val_acc: 0.6179\n",
      "Epoch 2807/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3759 - acc: 0.8697 - val_loss: 1.4612 - val_acc: 0.6705\n",
      "Epoch 2808/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3881 - acc: 0.8623 - val_loss: 1.7709 - val_acc: 0.6286\n",
      "Epoch 2809/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3755 - acc: 0.8674 - val_loss: 1.7405 - val_acc: 0.6652\n",
      "Epoch 2810/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3942 - acc: 0.8645 - val_loss: 1.4463 - val_acc: 0.6750\n",
      "Epoch 2811/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3645 - acc: 0.8686 - val_loss: 1.3536 - val_acc: 0.6875\n",
      "Epoch 2812/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3695 - acc: 0.8664 - val_loss: 1.6130 - val_acc: 0.6393\n",
      "Epoch 2813/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3917 - acc: 0.8640 - val_loss: 1.4340 - val_acc: 0.6670\n",
      "Epoch 2814/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3774 - acc: 0.8651 - val_loss: 1.5160 - val_acc: 0.6571\n",
      "Epoch 2815/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3646 - acc: 0.8670 - val_loss: 1.4076 - val_acc: 0.6786\n",
      "Epoch 2816/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3763 - acc: 0.8666 - val_loss: 1.5394 - val_acc: 0.6759\n",
      "Epoch 2817/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3795 - acc: 0.8642 - val_loss: 1.5344 - val_acc: 0.6759\n",
      "Epoch 2818/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3724 - acc: 0.8642 - val_loss: 1.3895 - val_acc: 0.6857\n",
      "Epoch 2819/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3735 - acc: 0.8663 - val_loss: 1.5454 - val_acc: 0.6866\n",
      "Epoch 2820/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3851 - acc: 0.8639 - val_loss: 1.3848 - val_acc: 0.6830\n",
      "Epoch 2821/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3593 - acc: 0.8696 - val_loss: 1.4161 - val_acc: 0.7054\n",
      "Epoch 2822/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3616 - acc: 0.8711 - val_loss: 1.6615 - val_acc: 0.6589\n",
      "Epoch 2823/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3740 - acc: 0.8695 - val_loss: 1.5545 - val_acc: 0.6598\n",
      "Epoch 2824/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3809 - acc: 0.8641 - val_loss: 1.3681 - val_acc: 0.6982\n",
      "Epoch 2825/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3576 - acc: 0.8719 - val_loss: 1.6636 - val_acc: 0.6652\n",
      "Epoch 2826/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3655 - acc: 0.8682 - val_loss: 1.5840 - val_acc: 0.6429\n",
      "Epoch 2827/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3627 - acc: 0.8696 - val_loss: 1.4823 - val_acc: 0.6652\n",
      "Epoch 2828/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3982 - acc: 0.8577 - val_loss: 1.3221 - val_acc: 0.7071\n",
      "Epoch 2829/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3554 - acc: 0.8695 - val_loss: 1.7867 - val_acc: 0.6321\n",
      "Epoch 2830/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3586 - acc: 0.8697 - val_loss: 1.5103 - val_acc: 0.6616\n",
      "Epoch 2831/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3806 - acc: 0.8668 - val_loss: 1.4353 - val_acc: 0.6795\n",
      "Epoch 2832/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3609 - acc: 0.8685 - val_loss: 1.8117 - val_acc: 0.6607\n",
      "Epoch 2833/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3629 - acc: 0.8719 - val_loss: 1.3644 - val_acc: 0.6714\n",
      "Epoch 2834/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3632 - acc: 0.8669 - val_loss: 1.2355 - val_acc: 0.6964\n",
      "Epoch 2835/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3521 - acc: 0.8739 - val_loss: 1.7158 - val_acc: 0.6268\n",
      "Epoch 2836/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.4127 - acc: 0.8585 - val_loss: 1.4756 - val_acc: 0.6616\n",
      "Epoch 2837/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3575 - acc: 0.8707 - val_loss: 1.7798 - val_acc: 0.6232\n",
      "Epoch 2838/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.4159 - acc: 0.8566 - val_loss: 1.7820 - val_acc: 0.6009\n",
      "Epoch 2839/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3776 - acc: 0.8658 - val_loss: 1.8799 - val_acc: 0.6268\n",
      "Epoch 2840/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3962 - acc: 0.8612 - val_loss: 1.3559 - val_acc: 0.6893\n",
      "Epoch 2841/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3590 - acc: 0.8723 - val_loss: 1.6290 - val_acc: 0.6554\n",
      "Epoch 2842/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3831 - acc: 0.8626 - val_loss: 1.2995 - val_acc: 0.7009\n",
      "Epoch 2843/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3663 - acc: 0.8683 - val_loss: 1.5874 - val_acc: 0.6589\n",
      "Epoch 2844/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3794 - acc: 0.8626 - val_loss: 1.5926 - val_acc: 0.6375\n",
      "Epoch 2845/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3884 - acc: 0.8645 - val_loss: 1.7269 - val_acc: 0.6170\n",
      "Epoch 2846/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3776 - acc: 0.8635 - val_loss: 1.3658 - val_acc: 0.6866\n",
      "Epoch 2847/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3913 - acc: 0.8636 - val_loss: 1.5112 - val_acc: 0.6625\n",
      "Epoch 2848/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3619 - acc: 0.8715 - val_loss: 1.5398 - val_acc: 0.6679\n",
      "Epoch 2849/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3576 - acc: 0.8750 - val_loss: 1.5373 - val_acc: 0.6812\n",
      "Epoch 2850/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3714 - acc: 0.8679 - val_loss: 1.5561 - val_acc: 0.6661\n",
      "Epoch 2851/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3704 - acc: 0.8694 - val_loss: 1.3475 - val_acc: 0.6973\n",
      "Epoch 2852/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3840 - acc: 0.8637 - val_loss: 1.4366 - val_acc: 0.7134\n",
      "Epoch 2853/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3806 - acc: 0.8633 - val_loss: 1.5731 - val_acc: 0.6714\n",
      "Epoch 2854/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3734 - acc: 0.8669 - val_loss: 1.6746 - val_acc: 0.6527\n",
      "Epoch 2855/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3753 - acc: 0.8691 - val_loss: 1.8100 - val_acc: 0.6625\n",
      "Epoch 2856/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3605 - acc: 0.8709 - val_loss: 1.3439 - val_acc: 0.7000\n",
      "Epoch 2857/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3535 - acc: 0.8728 - val_loss: 1.2928 - val_acc: 0.6991\n",
      "Epoch 2858/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3692 - acc: 0.8671 - val_loss: 1.3723 - val_acc: 0.6875\n",
      "Epoch 2859/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3549 - acc: 0.8729 - val_loss: 1.6788 - val_acc: 0.6875\n",
      "Epoch 2860/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3764 - acc: 0.8693 - val_loss: 1.1716 - val_acc: 0.7116\n",
      "Epoch 2861/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3479 - acc: 0.8722 - val_loss: 1.7658 - val_acc: 0.6304\n",
      "Epoch 2862/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3701 - acc: 0.8679 - val_loss: 1.4321 - val_acc: 0.6786\n",
      "Epoch 2863/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3619 - acc: 0.8708 - val_loss: 1.4596 - val_acc: 0.6527\n",
      "Epoch 2864/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3839 - acc: 0.8626 - val_loss: 1.3936 - val_acc: 0.6554\n",
      "Epoch 2865/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3581 - acc: 0.8713 - val_loss: 1.6130 - val_acc: 0.6527\n",
      "Epoch 2866/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3663 - acc: 0.8712 - val_loss: 1.9779 - val_acc: 0.6018\n",
      "Epoch 2867/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3741 - acc: 0.8689 - val_loss: 1.7523 - val_acc: 0.6804\n",
      "Epoch 2868/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3670 - acc: 0.8679 - val_loss: 1.3392 - val_acc: 0.6670\n",
      "Epoch 2869/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3780 - acc: 0.8643 - val_loss: 1.5110 - val_acc: 0.6518\n",
      "Epoch 2870/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3730 - acc: 0.8649 - val_loss: 1.4011 - val_acc: 0.6893\n",
      "Epoch 2871/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3576 - acc: 0.8683 - val_loss: 1.6340 - val_acc: 0.6589\n",
      "Epoch 2872/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3687 - acc: 0.8696 - val_loss: 1.5356 - val_acc: 0.6929\n",
      "Epoch 2873/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3702 - acc: 0.8659 - val_loss: 1.6020 - val_acc: 0.6482\n",
      "Epoch 2874/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3654 - acc: 0.8669 - val_loss: 1.5765 - val_acc: 0.6652\n",
      "Epoch 2875/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3716 - acc: 0.8667 - val_loss: 1.8634 - val_acc: 0.6295\n",
      "Epoch 2876/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3952 - acc: 0.8641 - val_loss: 1.3741 - val_acc: 0.6875\n",
      "Epoch 2877/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3640 - acc: 0.8683 - val_loss: 1.4401 - val_acc: 0.6661\n",
      "Epoch 2878/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3774 - acc: 0.8637 - val_loss: 2.0447 - val_acc: 0.6259\n",
      "Epoch 2879/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3901 - acc: 0.8639 - val_loss: 1.3274 - val_acc: 0.6812\n",
      "Epoch 2880/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3726 - acc: 0.8666 - val_loss: 1.5466 - val_acc: 0.6223\n",
      "Epoch 2881/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3696 - acc: 0.8675 - val_loss: 1.4724 - val_acc: 0.6554\n",
      "Epoch 2882/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3830 - acc: 0.8633 - val_loss: 1.5686 - val_acc: 0.6598\n",
      "Epoch 2883/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3847 - acc: 0.8603 - val_loss: 1.2521 - val_acc: 0.7143\n",
      "Epoch 2884/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3591 - acc: 0.8685 - val_loss: 1.5602 - val_acc: 0.6741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2885/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3652 - acc: 0.8691 - val_loss: 1.8767 - val_acc: 0.6714\n",
      "Epoch 2886/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3611 - acc: 0.8731 - val_loss: 1.9017 - val_acc: 0.6232\n",
      "Epoch 2887/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3782 - acc: 0.8661 - val_loss: 1.3720 - val_acc: 0.6982\n",
      "Epoch 2888/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3522 - acc: 0.8732 - val_loss: 1.3188 - val_acc: 0.6982\n",
      "Epoch 2889/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3684 - acc: 0.8681 - val_loss: 1.4715 - val_acc: 0.6518\n",
      "Epoch 2890/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3729 - acc: 0.8685 - val_loss: 1.6556 - val_acc: 0.6571\n",
      "Epoch 2891/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3703 - acc: 0.8707 - val_loss: 1.4985 - val_acc: 0.6848\n",
      "Epoch 2892/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3540 - acc: 0.8741 - val_loss: 1.7780 - val_acc: 0.6179\n",
      "Epoch 2893/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3826 - acc: 0.8687 - val_loss: 1.7593 - val_acc: 0.6714\n",
      "Epoch 2894/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3694 - acc: 0.8682 - val_loss: 1.8039 - val_acc: 0.6223\n",
      "Epoch 2895/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3766 - acc: 0.8653 - val_loss: 1.2629 - val_acc: 0.7098\n",
      "Epoch 2896/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3517 - acc: 0.8754 - val_loss: 2.0460 - val_acc: 0.6250\n",
      "Epoch 2897/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3889 - acc: 0.8665 - val_loss: 1.3768 - val_acc: 0.6955\n",
      "Epoch 2898/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3614 - acc: 0.8680 - val_loss: 2.0282 - val_acc: 0.6152\n",
      "Epoch 2899/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3947 - acc: 0.8675 - val_loss: 1.7623 - val_acc: 0.5982\n",
      "Epoch 2900/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3913 - acc: 0.8615 - val_loss: 1.4527 - val_acc: 0.6705\n",
      "Epoch 2901/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3466 - acc: 0.8753 - val_loss: 2.0423 - val_acc: 0.5938\n",
      "Epoch 2902/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3734 - acc: 0.8703 - val_loss: 1.4990 - val_acc: 0.6607\n",
      "Epoch 2903/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3724 - acc: 0.8635 - val_loss: 1.9275 - val_acc: 0.5937\n",
      "Epoch 2904/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3881 - acc: 0.8632 - val_loss: 1.3301 - val_acc: 0.6955\n",
      "Epoch 2905/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3587 - acc: 0.8695 - val_loss: 1.4983 - val_acc: 0.6857\n",
      "Epoch 2906/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3716 - acc: 0.8682 - val_loss: 1.6616 - val_acc: 0.6214\n",
      "Epoch 2907/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3754 - acc: 0.8664 - val_loss: 1.3519 - val_acc: 0.6973\n",
      "Epoch 2908/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3616 - acc: 0.8705 - val_loss: 1.3658 - val_acc: 0.7027\n",
      "Epoch 2909/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3596 - acc: 0.8697 - val_loss: 1.3779 - val_acc: 0.7063\n",
      "Epoch 2910/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3609 - acc: 0.8684 - val_loss: 1.4007 - val_acc: 0.6652\n",
      "Epoch 2911/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3603 - acc: 0.8665 - val_loss: 1.8406 - val_acc: 0.6125\n",
      "Epoch 2912/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3639 - acc: 0.8699 - val_loss: 1.5169 - val_acc: 0.6688\n",
      "Epoch 2913/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3755 - acc: 0.8638 - val_loss: 1.5631 - val_acc: 0.6705\n",
      "Epoch 2914/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3855 - acc: 0.8643 - val_loss: 1.5342 - val_acc: 0.6384\n",
      "Epoch 2915/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3577 - acc: 0.8710 - val_loss: 1.4472 - val_acc: 0.6813\n",
      "Epoch 2916/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3595 - acc: 0.8715 - val_loss: 2.0811 - val_acc: 0.5830\n",
      "Epoch 2917/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3925 - acc: 0.8663 - val_loss: 1.4833 - val_acc: 0.6723\n",
      "Epoch 2918/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3690 - acc: 0.8661 - val_loss: 1.6180 - val_acc: 0.6411\n",
      "Epoch 2919/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3692 - acc: 0.8704 - val_loss: 1.5108 - val_acc: 0.6848\n",
      "Epoch 2920/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3619 - acc: 0.8685 - val_loss: 1.7026 - val_acc: 0.6366\n",
      "Epoch 2921/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3839 - acc: 0.8645 - val_loss: 1.2501 - val_acc: 0.7018\n",
      "Epoch 2922/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3432 - acc: 0.8770 - val_loss: 1.6958 - val_acc: 0.6500\n",
      "Epoch 2923/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3760 - acc: 0.8654 - val_loss: 1.4942 - val_acc: 0.6661\n",
      "Epoch 2924/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3783 - acc: 0.8656 - val_loss: 1.2893 - val_acc: 0.6982\n",
      "Epoch 2925/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3519 - acc: 0.8736 - val_loss: 1.4971 - val_acc: 0.6902\n",
      "Epoch 2926/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3529 - acc: 0.8718 - val_loss: 1.6941 - val_acc: 0.6598\n",
      "Epoch 2927/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3624 - acc: 0.8709 - val_loss: 1.4563 - val_acc: 0.6830\n",
      "Epoch 2928/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3656 - acc: 0.8723 - val_loss: 1.4199 - val_acc: 0.6652\n",
      "Epoch 2929/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3854 - acc: 0.8635 - val_loss: 1.6846 - val_acc: 0.6491\n",
      "Epoch 2930/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3833 - acc: 0.8640 - val_loss: 1.4635 - val_acc: 0.6929\n",
      "Epoch 2931/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3562 - acc: 0.8727 - val_loss: 1.3840 - val_acc: 0.6804\n",
      "Epoch 2932/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3619 - acc: 0.8690 - val_loss: 1.4360 - val_acc: 0.6804\n",
      "Epoch 2933/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3649 - acc: 0.8678 - val_loss: 1.4640 - val_acc: 0.6616\n",
      "Epoch 2934/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3742 - acc: 0.8653 - val_loss: 1.3989 - val_acc: 0.6634\n",
      "Epoch 2935/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3749 - acc: 0.8669 - val_loss: 1.4652 - val_acc: 0.6857\n",
      "Epoch 2936/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3613 - acc: 0.8678 - val_loss: 1.4198 - val_acc: 0.6929\n",
      "Epoch 2937/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3519 - acc: 0.8732 - val_loss: 1.7143 - val_acc: 0.6491\n",
      "Epoch 2938/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3717 - acc: 0.8671 - val_loss: 1.5865 - val_acc: 0.6643\n",
      "Epoch 2939/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3605 - acc: 0.8709 - val_loss: 1.2635 - val_acc: 0.7286\n",
      "Epoch 2940/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3441 - acc: 0.8745 - val_loss: 1.4346 - val_acc: 0.6607\n",
      "Epoch 2941/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3667 - acc: 0.8673 - val_loss: 1.3721 - val_acc: 0.6705\n",
      "Epoch 2942/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3759 - acc: 0.8629 - val_loss: 1.4618 - val_acc: 0.6571\n",
      "Epoch 2943/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3554 - acc: 0.8763 - val_loss: 1.2069 - val_acc: 0.7259\n",
      "Epoch 2944/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3545 - acc: 0.8701 - val_loss: 1.9015 - val_acc: 0.6036\n",
      "Epoch 2945/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3789 - acc: 0.8648 - val_loss: 1.3570 - val_acc: 0.6902\n",
      "Epoch 2946/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3537 - acc: 0.8710 - val_loss: 1.4692 - val_acc: 0.6598\n",
      "Epoch 2947/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3863 - acc: 0.8618 - val_loss: 1.6183 - val_acc: 0.6509\n",
      "Epoch 2948/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3553 - acc: 0.8731 - val_loss: 1.5768 - val_acc: 0.6884\n",
      "Epoch 2949/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3608 - acc: 0.8712 - val_loss: 1.3754 - val_acc: 0.6804\n",
      "Epoch 2950/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3644 - acc: 0.8691 - val_loss: 1.5564 - val_acc: 0.6973\n",
      "Epoch 2951/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3510 - acc: 0.8727 - val_loss: 1.5187 - val_acc: 0.6679\n",
      "Epoch 2952/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3634 - acc: 0.8735 - val_loss: 1.2433 - val_acc: 0.7063\n",
      "Epoch 2953/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3532 - acc: 0.8722 - val_loss: 1.5394 - val_acc: 0.6893\n",
      "Epoch 2954/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3617 - acc: 0.8701 - val_loss: 1.6246 - val_acc: 0.6527\n",
      "Epoch 2955/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3622 - acc: 0.8705 - val_loss: 1.2665 - val_acc: 0.7009\n",
      "Epoch 2956/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3476 - acc: 0.8735 - val_loss: 1.5127 - val_acc: 0.6955\n",
      "Epoch 2957/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3580 - acc: 0.8713 - val_loss: 1.4504 - val_acc: 0.7009\n",
      "Epoch 2958/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3554 - acc: 0.8719 - val_loss: 2.0015 - val_acc: 0.5920\n",
      "Epoch 2959/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3696 - acc: 0.8701 - val_loss: 1.4821 - val_acc: 0.6786\n",
      "Epoch 2960/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3580 - acc: 0.8740 - val_loss: 1.5080 - val_acc: 0.6446\n",
      "Epoch 2961/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3630 - acc: 0.8704 - val_loss: 1.2985 - val_acc: 0.7312\n",
      "Epoch 2962/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3566 - acc: 0.8722 - val_loss: 1.3072 - val_acc: 0.6964\n",
      "Epoch 2963/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3412 - acc: 0.8766 - val_loss: 1.7439 - val_acc: 0.6411\n",
      "Epoch 2964/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3648 - acc: 0.8711 - val_loss: 1.8225 - val_acc: 0.6277\n",
      "Epoch 2965/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3732 - acc: 0.8687 - val_loss: 1.5707 - val_acc: 0.6786\n",
      "Epoch 2966/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3582 - acc: 0.8711 - val_loss: 2.2240 - val_acc: 0.6009\n",
      "Epoch 2967/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3785 - acc: 0.8682 - val_loss: 1.5664 - val_acc: 0.6687\n",
      "Epoch 2968/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3593 - acc: 0.8730 - val_loss: 1.6910 - val_acc: 0.6509\n",
      "Epoch 2969/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3610 - acc: 0.8746 - val_loss: 1.4251 - val_acc: 0.6714\n",
      "Epoch 2970/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3752 - acc: 0.8683 - val_loss: 1.3829 - val_acc: 0.6884\n",
      "Epoch 2971/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3551 - acc: 0.8710 - val_loss: 1.8926 - val_acc: 0.6152\n",
      "Epoch 2972/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3634 - acc: 0.8723 - val_loss: 1.2341 - val_acc: 0.6866\n",
      "Epoch 2973/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3556 - acc: 0.8726 - val_loss: 1.3028 - val_acc: 0.7063\n",
      "Epoch 2974/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3544 - acc: 0.8723 - val_loss: 2.0811 - val_acc: 0.6062\n",
      "Epoch 2975/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3822 - acc: 0.8679 - val_loss: 1.9791 - val_acc: 0.6170\n",
      "Epoch 2976/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3712 - acc: 0.8661 - val_loss: 1.2911 - val_acc: 0.7080\n",
      "Epoch 2977/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3565 - acc: 0.8700 - val_loss: 1.6467 - val_acc: 0.6268\n",
      "Epoch 2978/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3741 - acc: 0.8667 - val_loss: 1.5392 - val_acc: 0.6723\n",
      "Epoch 2979/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3845 - acc: 0.8657 - val_loss: 1.5136 - val_acc: 0.6723\n",
      "Epoch 2980/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3628 - acc: 0.8699 - val_loss: 1.2747 - val_acc: 0.7062\n",
      "Epoch 2981/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3537 - acc: 0.8719 - val_loss: 1.4356 - val_acc: 0.6438\n",
      "Epoch 2982/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3773 - acc: 0.8624 - val_loss: 1.8110 - val_acc: 0.6589\n",
      "Epoch 2983/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3825 - acc: 0.8682 - val_loss: 1.5212 - val_acc: 0.6464\n",
      "Epoch 2984/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3730 - acc: 0.8677 - val_loss: 1.4382 - val_acc: 0.6911\n",
      "Epoch 2985/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3594 - acc: 0.8712 - val_loss: 1.5061 - val_acc: 0.6946\n",
      "Epoch 2986/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3714 - acc: 0.8675 - val_loss: 1.5540 - val_acc: 0.6830\n",
      "Epoch 2987/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3605 - acc: 0.8704 - val_loss: 1.4765 - val_acc: 0.6580\n",
      "Epoch 2988/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3741 - acc: 0.8669 - val_loss: 1.4657 - val_acc: 0.6768\n",
      "Epoch 2989/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3582 - acc: 0.8711 - val_loss: 1.3672 - val_acc: 0.7143\n",
      "Epoch 2990/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3492 - acc: 0.8734 - val_loss: 1.6938 - val_acc: 0.6670\n",
      "Epoch 2991/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3571 - acc: 0.8733 - val_loss: 1.7767 - val_acc: 0.6446\n",
      "Epoch 2992/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3766 - acc: 0.8684 - val_loss: 1.3726 - val_acc: 0.6866\n",
      "Epoch 2993/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3480 - acc: 0.8744 - val_loss: 1.6928 - val_acc: 0.6661\n",
      "Epoch 2994/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3536 - acc: 0.8750 - val_loss: 1.4705 - val_acc: 0.6821\n",
      "Epoch 2995/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3618 - acc: 0.8749 - val_loss: 1.6959 - val_acc: 0.6286\n",
      "Epoch 2996/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3798 - acc: 0.8646 - val_loss: 1.4490 - val_acc: 0.6937\n",
      "Epoch 2997/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3451 - acc: 0.8747 - val_loss: 1.2463 - val_acc: 0.7062\n",
      "Epoch 2998/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3581 - acc: 0.8673 - val_loss: 1.5637 - val_acc: 0.6580\n",
      "Epoch 2999/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3560 - acc: 0.8691 - val_loss: 1.4615 - val_acc: 0.6768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3599 - acc: 0.8724 - val_loss: 1.4681 - val_acc: 0.6750\n",
      "Epoch 3001/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3485 - acc: 0.8744 - val_loss: 1.3657 - val_acc: 0.6723\n",
      "Epoch 3002/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3676 - acc: 0.8670 - val_loss: 2.5357 - val_acc: 0.5732\n",
      "Epoch 3003/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3907 - acc: 0.8672 - val_loss: 1.8855 - val_acc: 0.6330\n",
      "Epoch 3004/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3802 - acc: 0.8675 - val_loss: 1.5679 - val_acc: 0.6920\n",
      "Epoch 3005/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3682 - acc: 0.8701 - val_loss: 2.0753 - val_acc: 0.5884\n",
      "Epoch 3006/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3659 - acc: 0.8735 - val_loss: 1.2248 - val_acc: 0.7196\n",
      "Epoch 3007/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3507 - acc: 0.8711 - val_loss: 1.3382 - val_acc: 0.6920\n",
      "Epoch 3008/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3485 - acc: 0.8728 - val_loss: 1.5370 - val_acc: 0.6893\n",
      "Epoch 3009/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3639 - acc: 0.8697 - val_loss: 1.3445 - val_acc: 0.7152\n",
      "Epoch 3010/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3526 - acc: 0.8723 - val_loss: 1.3066 - val_acc: 0.7036\n",
      "Epoch 3011/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3433 - acc: 0.8780 - val_loss: 1.4434 - val_acc: 0.7116\n",
      "Epoch 3012/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3435 - acc: 0.8735 - val_loss: 1.3273 - val_acc: 0.6946\n",
      "Epoch 3013/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3594 - acc: 0.8699 - val_loss: 1.6511 - val_acc: 0.6170\n",
      "Epoch 3014/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3731 - acc: 0.8700 - val_loss: 1.4675 - val_acc: 0.7134\n",
      "Epoch 3015/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3444 - acc: 0.8743 - val_loss: 1.5711 - val_acc: 0.6821\n",
      "Epoch 3016/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3727 - acc: 0.8689 - val_loss: 1.5671 - val_acc: 0.6563\n",
      "Epoch 3017/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3618 - acc: 0.8675 - val_loss: 1.5952 - val_acc: 0.6518\n",
      "Epoch 3018/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3550 - acc: 0.8709 - val_loss: 1.9170 - val_acc: 0.6304\n",
      "Epoch 3019/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3504 - acc: 0.8721 - val_loss: 1.5503 - val_acc: 0.6348\n",
      "Epoch 3020/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3681 - acc: 0.8717 - val_loss: 1.4587 - val_acc: 0.6991\n",
      "Epoch 3021/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3445 - acc: 0.8755 - val_loss: 1.4051 - val_acc: 0.6920\n",
      "Epoch 3022/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3512 - acc: 0.8719 - val_loss: 1.6856 - val_acc: 0.6527\n",
      "Epoch 3023/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3608 - acc: 0.8694 - val_loss: 2.0395 - val_acc: 0.6116\n",
      "Epoch 3024/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3668 - acc: 0.8711 - val_loss: 1.5921 - val_acc: 0.6589\n",
      "Epoch 3025/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3680 - acc: 0.8712 - val_loss: 2.0308 - val_acc: 0.5955\n",
      "Epoch 3026/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3792 - acc: 0.8709 - val_loss: 1.6287 - val_acc: 0.6687\n",
      "Epoch 3027/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3617 - acc: 0.8730 - val_loss: 1.4635 - val_acc: 0.6848\n",
      "Epoch 3028/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3630 - acc: 0.8695 - val_loss: 1.3878 - val_acc: 0.6804\n",
      "Epoch 3029/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3539 - acc: 0.8712 - val_loss: 1.3797 - val_acc: 0.7027\n",
      "Epoch 3030/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3514 - acc: 0.8737 - val_loss: 1.5265 - val_acc: 0.6607\n",
      "Epoch 3031/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3652 - acc: 0.8695 - val_loss: 1.7309 - val_acc: 0.6491\n",
      "Epoch 3032/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3420 - acc: 0.8741 - val_loss: 1.5664 - val_acc: 0.6866\n",
      "Epoch 3033/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3604 - acc: 0.8707 - val_loss: 1.5139 - val_acc: 0.6866\n",
      "Epoch 3034/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3455 - acc: 0.8767 - val_loss: 1.6536 - val_acc: 0.6634\n",
      "Epoch 3035/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3625 - acc: 0.8691 - val_loss: 1.5619 - val_acc: 0.6732\n",
      "Epoch 3036/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3532 - acc: 0.8711 - val_loss: 1.5295 - val_acc: 0.6875\n",
      "Epoch 3037/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3420 - acc: 0.8756 - val_loss: 1.2753 - val_acc: 0.7125\n",
      "Epoch 3038/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3372 - acc: 0.8779 - val_loss: 1.7843 - val_acc: 0.6321\n",
      "Epoch 3039/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3662 - acc: 0.8664 - val_loss: 1.3390 - val_acc: 0.6786\n",
      "Epoch 3040/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3559 - acc: 0.8701 - val_loss: 1.6052 - val_acc: 0.6571\n",
      "Epoch 3041/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3541 - acc: 0.8771 - val_loss: 1.6456 - val_acc: 0.6268\n",
      "Epoch 3042/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3677 - acc: 0.8715 - val_loss: 1.4996 - val_acc: 0.6786\n",
      "Epoch 3043/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3681 - acc: 0.8693 - val_loss: 1.3638 - val_acc: 0.6982\n",
      "Epoch 3044/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3468 - acc: 0.8739 - val_loss: 1.4743 - val_acc: 0.6848\n",
      "Epoch 3045/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3414 - acc: 0.8746 - val_loss: 1.5110 - val_acc: 0.6813\n",
      "Epoch 3046/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3592 - acc: 0.8717 - val_loss: 1.6368 - val_acc: 0.6813\n",
      "Epoch 3047/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3682 - acc: 0.8690 - val_loss: 1.4886 - val_acc: 0.7054\n",
      "Epoch 3048/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3534 - acc: 0.8719 - val_loss: 1.7744 - val_acc: 0.6339\n",
      "Epoch 3049/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3670 - acc: 0.8666 - val_loss: 1.4800 - val_acc: 0.6902\n",
      "Epoch 3050/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3471 - acc: 0.8748 - val_loss: 1.4750 - val_acc: 0.6911\n",
      "Epoch 3051/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3683 - acc: 0.8688 - val_loss: 1.6180 - val_acc: 0.6839\n",
      "Epoch 3052/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3585 - acc: 0.8730 - val_loss: 1.9147 - val_acc: 0.5866\n",
      "Epoch 3053/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3819 - acc: 0.8655 - val_loss: 1.7822 - val_acc: 0.6446\n",
      "Epoch 3054/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3512 - acc: 0.8747 - val_loss: 1.2928 - val_acc: 0.6696\n",
      "Epoch 3055/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3692 - acc: 0.8651 - val_loss: 1.6932 - val_acc: 0.6446\n",
      "Epoch 3056/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3654 - acc: 0.8726 - val_loss: 1.5416 - val_acc: 0.6741\n",
      "Epoch 3057/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3506 - acc: 0.8767 - val_loss: 1.3832 - val_acc: 0.6911\n",
      "Epoch 3058/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3513 - acc: 0.8709 - val_loss: 1.3590 - val_acc: 0.7036\n",
      "Epoch 3059/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3468 - acc: 0.8737 - val_loss: 1.3284 - val_acc: 0.6893\n",
      "Epoch 3060/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3536 - acc: 0.8697 - val_loss: 1.6353 - val_acc: 0.6813\n",
      "Epoch 3061/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3555 - acc: 0.8737 - val_loss: 1.5207 - val_acc: 0.6830\n",
      "Epoch 3062/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3506 - acc: 0.8763 - val_loss: 2.0039 - val_acc: 0.5911\n",
      "Epoch 3063/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3916 - acc: 0.8655 - val_loss: 1.8135 - val_acc: 0.6205\n",
      "Epoch 3064/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3690 - acc: 0.8694 - val_loss: 1.7446 - val_acc: 0.6607\n",
      "Epoch 3065/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3448 - acc: 0.8771 - val_loss: 1.3942 - val_acc: 0.7125\n",
      "Epoch 3066/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3440 - acc: 0.8779 - val_loss: 1.6834 - val_acc: 0.6848\n",
      "Epoch 3067/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3703 - acc: 0.8696 - val_loss: 2.2515 - val_acc: 0.5741\n",
      "Epoch 3068/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3535 - acc: 0.8754 - val_loss: 1.4699 - val_acc: 0.6732\n",
      "Epoch 3069/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3653 - acc: 0.8704 - val_loss: 1.3807 - val_acc: 0.6884\n",
      "Epoch 3070/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3407 - acc: 0.8782 - val_loss: 1.3670 - val_acc: 0.7080\n",
      "Epoch 3071/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3569 - acc: 0.8699 - val_loss: 1.8721 - val_acc: 0.6679\n",
      "Epoch 3072/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3646 - acc: 0.8721 - val_loss: 1.5452 - val_acc: 0.6723\n",
      "Epoch 3073/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3601 - acc: 0.8706 - val_loss: 1.8007 - val_acc: 0.6286\n",
      "Epoch 3074/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3506 - acc: 0.8755 - val_loss: 1.5030 - val_acc: 0.6768\n",
      "Epoch 3075/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3598 - acc: 0.8715 - val_loss: 1.5378 - val_acc: 0.6893\n",
      "Epoch 3076/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3599 - acc: 0.8707 - val_loss: 1.4657 - val_acc: 0.7062\n",
      "Epoch 3077/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3572 - acc: 0.8666 - val_loss: 1.3471 - val_acc: 0.6911\n",
      "Epoch 3078/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3480 - acc: 0.8764 - val_loss: 1.8821 - val_acc: 0.6438\n",
      "Epoch 3079/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3632 - acc: 0.8763 - val_loss: 1.5424 - val_acc: 0.6527\n",
      "Epoch 3080/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3598 - acc: 0.8734 - val_loss: 2.0794 - val_acc: 0.5955\n",
      "Epoch 3081/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3603 - acc: 0.8753 - val_loss: 1.5726 - val_acc: 0.6732\n",
      "Epoch 3082/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3520 - acc: 0.8736 - val_loss: 1.3277 - val_acc: 0.7277\n",
      "Epoch 3083/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3500 - acc: 0.8746 - val_loss: 1.7466 - val_acc: 0.6491\n",
      "Epoch 3084/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3506 - acc: 0.8720 - val_loss: 1.3738 - val_acc: 0.6884\n",
      "Epoch 3085/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3773 - acc: 0.8657 - val_loss: 1.2832 - val_acc: 0.6964\n",
      "Epoch 3086/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3492 - acc: 0.8728 - val_loss: 1.6019 - val_acc: 0.6705\n",
      "Epoch 3087/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3477 - acc: 0.8721 - val_loss: 1.5700 - val_acc: 0.6795\n",
      "Epoch 3088/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3463 - acc: 0.8730 - val_loss: 1.8676 - val_acc: 0.6116\n",
      "Epoch 3089/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3829 - acc: 0.8619 - val_loss: 1.2921 - val_acc: 0.7241\n",
      "Epoch 3090/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3412 - acc: 0.8773 - val_loss: 1.4597 - val_acc: 0.7000\n",
      "Epoch 3091/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3584 - acc: 0.8707 - val_loss: 1.5685 - val_acc: 0.6830\n",
      "Epoch 3092/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3318 - acc: 0.8812 - val_loss: 1.9025 - val_acc: 0.6312\n",
      "Epoch 3093/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3484 - acc: 0.8767 - val_loss: 1.6633 - val_acc: 0.6768\n",
      "Epoch 3094/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3539 - acc: 0.8733 - val_loss: 1.4538 - val_acc: 0.6589\n",
      "Epoch 3095/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3464 - acc: 0.8793 - val_loss: 1.7305 - val_acc: 0.6062\n",
      "Epoch 3096/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3927 - acc: 0.8637 - val_loss: 1.5919 - val_acc: 0.6723\n",
      "Epoch 3097/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3537 - acc: 0.8746 - val_loss: 1.6429 - val_acc: 0.6723\n",
      "Epoch 3098/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3676 - acc: 0.8689 - val_loss: 1.4461 - val_acc: 0.6759\n",
      "Epoch 3099/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3479 - acc: 0.8760 - val_loss: 1.8163 - val_acc: 0.6473\n",
      "Epoch 3100/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3506 - acc: 0.8765 - val_loss: 1.4053 - val_acc: 0.7054\n",
      "Epoch 3101/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3457 - acc: 0.8770 - val_loss: 1.7053 - val_acc: 0.6652\n",
      "Epoch 3102/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3472 - acc: 0.8763 - val_loss: 2.0989 - val_acc: 0.6304\n",
      "Epoch 3103/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3650 - acc: 0.8730 - val_loss: 1.5589 - val_acc: 0.6616\n",
      "Epoch 3104/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3487 - acc: 0.8767 - val_loss: 1.5459 - val_acc: 0.6982\n",
      "Epoch 3105/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3557 - acc: 0.8745 - val_loss: 1.5123 - val_acc: 0.6866\n",
      "Epoch 3106/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3412 - acc: 0.8759 - val_loss: 1.7232 - val_acc: 0.6455\n",
      "Epoch 3107/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3548 - acc: 0.8746 - val_loss: 1.7017 - val_acc: 0.6955\n",
      "Epoch 3108/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3503 - acc: 0.8715 - val_loss: 1.3532 - val_acc: 0.7080\n",
      "Epoch 3109/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3617 - acc: 0.8697 - val_loss: 1.3989 - val_acc: 0.6536\n",
      "Epoch 3110/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3395 - acc: 0.8768 - val_loss: 1.5982 - val_acc: 0.6813\n",
      "Epoch 3111/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3548 - acc: 0.8726 - val_loss: 1.2305 - val_acc: 0.7295\n",
      "Epoch 3112/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3385 - acc: 0.8795 - val_loss: 1.4273 - val_acc: 0.6714\n",
      "Epoch 3113/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3496 - acc: 0.8715 - val_loss: 1.8182 - val_acc: 0.6589\n",
      "Epoch 3114/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3546 - acc: 0.8765 - val_loss: 1.8806 - val_acc: 0.6170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3115/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3628 - acc: 0.8715 - val_loss: 1.4469 - val_acc: 0.6893\n",
      "Epoch 3116/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3548 - acc: 0.8684 - val_loss: 1.4377 - val_acc: 0.7000\n",
      "Epoch 3117/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3499 - acc: 0.8735 - val_loss: 1.5576 - val_acc: 0.6723\n",
      "Epoch 3118/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3644 - acc: 0.8726 - val_loss: 1.6553 - val_acc: 0.6679\n",
      "Epoch 3119/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3704 - acc: 0.8681 - val_loss: 2.1597 - val_acc: 0.5759\n",
      "Epoch 3120/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3756 - acc: 0.8694 - val_loss: 1.5878 - val_acc: 0.6848\n",
      "Epoch 3121/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3585 - acc: 0.8696 - val_loss: 1.7193 - val_acc: 0.6473\n",
      "Epoch 3122/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3509 - acc: 0.8763 - val_loss: 1.2838 - val_acc: 0.6929\n",
      "Epoch 3123/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3476 - acc: 0.8736 - val_loss: 1.7404 - val_acc: 0.6420\n",
      "Epoch 3124/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3446 - acc: 0.8767 - val_loss: 1.8626 - val_acc: 0.6214\n",
      "Epoch 3125/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3873 - acc: 0.8647 - val_loss: 1.3250 - val_acc: 0.6938\n",
      "Epoch 3126/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3516 - acc: 0.8705 - val_loss: 1.2602 - val_acc: 0.7134\n",
      "Epoch 3127/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3335 - acc: 0.8765 - val_loss: 1.6213 - val_acc: 0.6607\n",
      "Epoch 3128/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3987 - acc: 0.8637 - val_loss: 1.5916 - val_acc: 0.6527\n",
      "Epoch 3129/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3403 - acc: 0.8791 - val_loss: 1.6147 - val_acc: 0.6679\n",
      "Epoch 3130/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3593 - acc: 0.8727 - val_loss: 2.0308 - val_acc: 0.5911\n",
      "Epoch 3131/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3727 - acc: 0.8689 - val_loss: 1.4999 - val_acc: 0.6687\n",
      "Epoch 3132/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3546 - acc: 0.8725 - val_loss: 1.3136 - val_acc: 0.7054\n",
      "Epoch 3133/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3475 - acc: 0.8747 - val_loss: 1.4024 - val_acc: 0.6964\n",
      "Epoch 3134/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3521 - acc: 0.8760 - val_loss: 2.3336 - val_acc: 0.6107\n",
      "Epoch 3135/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3681 - acc: 0.8717 - val_loss: 1.5482 - val_acc: 0.6607\n",
      "Epoch 3136/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3466 - acc: 0.8763 - val_loss: 1.4044 - val_acc: 0.6955\n",
      "Epoch 3137/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3527 - acc: 0.8733 - val_loss: 1.9300 - val_acc: 0.6429\n",
      "Epoch 3138/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3666 - acc: 0.8726 - val_loss: 1.5986 - val_acc: 0.6670\n",
      "Epoch 3139/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3559 - acc: 0.8709 - val_loss: 1.5135 - val_acc: 0.6696\n",
      "Epoch 3140/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3467 - acc: 0.8761 - val_loss: 1.2846 - val_acc: 0.6795\n",
      "Epoch 3141/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3710 - acc: 0.8646 - val_loss: 1.6736 - val_acc: 0.6518\n",
      "Epoch 3142/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3560 - acc: 0.8734 - val_loss: 1.3592 - val_acc: 0.6795\n",
      "Epoch 3143/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3503 - acc: 0.8741 - val_loss: 1.7359 - val_acc: 0.6187\n",
      "Epoch 3144/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3433 - acc: 0.8801 - val_loss: 1.3430 - val_acc: 0.7000\n",
      "Epoch 3145/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3330 - acc: 0.8804 - val_loss: 1.8276 - val_acc: 0.6580\n",
      "Epoch 3146/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3533 - acc: 0.8739 - val_loss: 1.9477 - val_acc: 0.5866\n",
      "Epoch 3147/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3774 - acc: 0.8703 - val_loss: 1.6852 - val_acc: 0.6625\n",
      "Epoch 3148/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3467 - acc: 0.8769 - val_loss: 1.7495 - val_acc: 0.6500\n",
      "Epoch 3149/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3535 - acc: 0.8741 - val_loss: 1.3284 - val_acc: 0.7214\n",
      "Epoch 3150/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3447 - acc: 0.8737 - val_loss: 1.5863 - val_acc: 0.6625\n",
      "Epoch 3151/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3468 - acc: 0.8745 - val_loss: 1.4118 - val_acc: 0.6884\n",
      "Epoch 3152/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3440 - acc: 0.8731 - val_loss: 1.7140 - val_acc: 0.6634\n",
      "Epoch 3153/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3478 - acc: 0.8749 - val_loss: 1.4166 - val_acc: 0.7161\n",
      "Epoch 3154/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3455 - acc: 0.8744 - val_loss: 1.6316 - val_acc: 0.6634\n",
      "Epoch 3155/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3612 - acc: 0.8712 - val_loss: 1.4970 - val_acc: 0.6920\n",
      "Epoch 3156/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3432 - acc: 0.8778 - val_loss: 1.4496 - val_acc: 0.6571\n",
      "Epoch 3157/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3579 - acc: 0.8737 - val_loss: 1.5986 - val_acc: 0.6643\n",
      "Epoch 3158/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3560 - acc: 0.8718 - val_loss: 1.3724 - val_acc: 0.7089\n",
      "Epoch 3159/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3458 - acc: 0.8769 - val_loss: 1.6298 - val_acc: 0.6741\n",
      "Epoch 3160/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3482 - acc: 0.8775 - val_loss: 1.6475 - val_acc: 0.6964\n",
      "Epoch 3161/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3413 - acc: 0.8793 - val_loss: 1.6488 - val_acc: 0.6929\n",
      "Epoch 3162/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3569 - acc: 0.8753 - val_loss: 1.9074 - val_acc: 0.6250\n",
      "Epoch 3163/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3681 - acc: 0.8731 - val_loss: 1.3013 - val_acc: 0.6955\n",
      "Epoch 3164/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3305 - acc: 0.8789 - val_loss: 1.3431 - val_acc: 0.6946\n",
      "Epoch 3165/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3623 - acc: 0.8705 - val_loss: 1.3478 - val_acc: 0.6964\n",
      "Epoch 3166/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3207 - acc: 0.8849 - val_loss: 1.5626 - val_acc: 0.6357\n",
      "Epoch 3167/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3622 - acc: 0.8699 - val_loss: 1.4800 - val_acc: 0.6848\n",
      "Epoch 3168/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3401 - acc: 0.8757 - val_loss: 1.4221 - val_acc: 0.7259\n",
      "Epoch 3169/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3364 - acc: 0.8785 - val_loss: 1.6247 - val_acc: 0.6411\n",
      "Epoch 3170/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3460 - acc: 0.8741 - val_loss: 1.6282 - val_acc: 0.6536\n",
      "Epoch 3171/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3482 - acc: 0.8755 - val_loss: 1.2895 - val_acc: 0.6920\n",
      "Epoch 3172/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3443 - acc: 0.8747 - val_loss: 1.5378 - val_acc: 0.6893\n",
      "Epoch 3173/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3486 - acc: 0.8751 - val_loss: 1.3730 - val_acc: 0.6679\n",
      "Epoch 3174/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3403 - acc: 0.8760 - val_loss: 1.7844 - val_acc: 0.6482\n",
      "Epoch 3175/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3591 - acc: 0.8740 - val_loss: 1.6919 - val_acc: 0.6509\n",
      "Epoch 3176/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3368 - acc: 0.8785 - val_loss: 1.6427 - val_acc: 0.6866\n",
      "Epoch 3177/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3536 - acc: 0.8754 - val_loss: 1.2755 - val_acc: 0.7286\n",
      "Epoch 3178/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3334 - acc: 0.8775 - val_loss: 1.6132 - val_acc: 0.6875\n",
      "Epoch 3179/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3622 - acc: 0.8693 - val_loss: 1.2198 - val_acc: 0.7313\n",
      "Epoch 3180/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3240 - acc: 0.8810 - val_loss: 1.5736 - val_acc: 0.6973\n",
      "Epoch 3181/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3451 - acc: 0.8727 - val_loss: 1.3317 - val_acc: 0.7116\n",
      "Epoch 3182/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3489 - acc: 0.8753 - val_loss: 1.7756 - val_acc: 0.6411\n",
      "Epoch 3183/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3529 - acc: 0.8766 - val_loss: 1.7212 - val_acc: 0.6205\n",
      "Epoch 3184/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3636 - acc: 0.8716 - val_loss: 1.9660 - val_acc: 0.6241\n",
      "Epoch 3185/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3606 - acc: 0.8768 - val_loss: 1.5655 - val_acc: 0.6714\n",
      "Epoch 3186/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3514 - acc: 0.8733 - val_loss: 1.4071 - val_acc: 0.7125\n",
      "Epoch 3187/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3369 - acc: 0.8764 - val_loss: 1.6780 - val_acc: 0.6732\n",
      "Epoch 3188/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3498 - acc: 0.8738 - val_loss: 1.4379 - val_acc: 0.7080\n",
      "Epoch 3189/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3338 - acc: 0.8801 - val_loss: 1.7163 - val_acc: 0.6625\n",
      "Epoch 3190/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3546 - acc: 0.8745 - val_loss: 1.4023 - val_acc: 0.6732\n",
      "Epoch 3191/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3437 - acc: 0.8783 - val_loss: 1.4652 - val_acc: 0.6866\n",
      "Epoch 3192/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3325 - acc: 0.8819 - val_loss: 1.3235 - val_acc: 0.7330\n",
      "Epoch 3193/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3404 - acc: 0.8745 - val_loss: 1.4471 - val_acc: 0.6929\n",
      "Epoch 3194/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3410 - acc: 0.8777 - val_loss: 1.6904 - val_acc: 0.6545\n",
      "Epoch 3195/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3618 - acc: 0.8719 - val_loss: 1.3241 - val_acc: 0.7062\n",
      "Epoch 3196/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3290 - acc: 0.8789 - val_loss: 1.5243 - val_acc: 0.6634\n",
      "Epoch 3197/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3500 - acc: 0.8733 - val_loss: 1.6020 - val_acc: 0.6714\n",
      "Epoch 3198/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3491 - acc: 0.8758 - val_loss: 1.4894 - val_acc: 0.7027\n",
      "Epoch 3199/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3544 - acc: 0.8749 - val_loss: 1.3233 - val_acc: 0.7179\n",
      "Epoch 3200/5000\n",
      "15008/15008 [==============================] - 1s 95us/step - loss: 0.3475 - acc: 0.8777 - val_loss: 1.4840 - val_acc: 0.6491\n",
      "Epoch 3201/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3598 - acc: 0.8717 - val_loss: 1.8639 - val_acc: 0.6152\n",
      "Epoch 3202/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3662 - acc: 0.8741 - val_loss: 1.5095 - val_acc: 0.6866\n",
      "Epoch 3203/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3437 - acc: 0.8776 - val_loss: 1.6115 - val_acc: 0.6768\n",
      "Epoch 3204/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3923 - acc: 0.8628 - val_loss: 1.4289 - val_acc: 0.6696\n",
      "Epoch 3205/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3507 - acc: 0.8754 - val_loss: 1.6610 - val_acc: 0.6402\n",
      "Epoch 3206/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3507 - acc: 0.8755 - val_loss: 1.7108 - val_acc: 0.6259\n",
      "Epoch 3207/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3433 - acc: 0.8775 - val_loss: 1.6009 - val_acc: 0.6696\n",
      "Epoch 3208/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3448 - acc: 0.8765 - val_loss: 1.5685 - val_acc: 0.6929\n",
      "Epoch 3209/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3484 - acc: 0.8737 - val_loss: 1.2679 - val_acc: 0.7268\n",
      "Epoch 3210/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3353 - acc: 0.8773 - val_loss: 1.3764 - val_acc: 0.7027\n",
      "Epoch 3211/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3363 - acc: 0.8781 - val_loss: 1.6026 - val_acc: 0.6571\n",
      "Epoch 3212/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3406 - acc: 0.8795 - val_loss: 1.6729 - val_acc: 0.6670\n",
      "Epoch 3213/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3642 - acc: 0.8725 - val_loss: 1.5799 - val_acc: 0.6670\n",
      "Epoch 3214/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3442 - acc: 0.8751 - val_loss: 1.7252 - val_acc: 0.6554\n",
      "Epoch 3215/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3611 - acc: 0.8713 - val_loss: 1.2501 - val_acc: 0.7375\n",
      "Epoch 3216/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3362 - acc: 0.8761 - val_loss: 1.8384 - val_acc: 0.6366\n",
      "Epoch 3217/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3677 - acc: 0.8705 - val_loss: 1.3566 - val_acc: 0.7018\n",
      "Epoch 3218/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3340 - acc: 0.8810 - val_loss: 1.4853 - val_acc: 0.6973\n",
      "Epoch 3219/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3425 - acc: 0.8774 - val_loss: 1.2176 - val_acc: 0.7241\n",
      "Epoch 3220/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3394 - acc: 0.8784 - val_loss: 1.4513 - val_acc: 0.6821\n",
      "Epoch 3221/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3450 - acc: 0.8759 - val_loss: 1.6385 - val_acc: 0.6750\n",
      "Epoch 3222/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3389 - acc: 0.8803 - val_loss: 1.5964 - val_acc: 0.6688\n",
      "Epoch 3223/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3322 - acc: 0.8803 - val_loss: 1.6284 - val_acc: 0.6964\n",
      "Epoch 3224/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3401 - acc: 0.8774 - val_loss: 1.3953 - val_acc: 0.6955\n",
      "Epoch 3225/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3385 - acc: 0.8786 - val_loss: 1.4343 - val_acc: 0.6786\n",
      "Epoch 3226/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3331 - acc: 0.8786 - val_loss: 1.7659 - val_acc: 0.6661\n",
      "Epoch 3227/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3487 - acc: 0.8766 - val_loss: 1.6115 - val_acc: 0.6723\n",
      "Epoch 3228/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3513 - acc: 0.8754 - val_loss: 1.6977 - val_acc: 0.6589\n",
      "Epoch 3229/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3398 - acc: 0.8791 - val_loss: 1.6713 - val_acc: 0.6732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3230/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3428 - acc: 0.8806 - val_loss: 1.3727 - val_acc: 0.7018\n",
      "Epoch 3231/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3379 - acc: 0.8800 - val_loss: 1.7377 - val_acc: 0.6384\n",
      "Epoch 3232/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3652 - acc: 0.8697 - val_loss: 1.4929 - val_acc: 0.6893\n",
      "Epoch 3233/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3372 - acc: 0.8806 - val_loss: 1.6958 - val_acc: 0.5884\n",
      "Epoch 3234/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3706 - acc: 0.8706 - val_loss: 1.6136 - val_acc: 0.6491\n",
      "Epoch 3235/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3491 - acc: 0.8749 - val_loss: 1.8501 - val_acc: 0.6438\n",
      "Epoch 3236/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3573 - acc: 0.8755 - val_loss: 1.3991 - val_acc: 0.7071\n",
      "Epoch 3237/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3500 - acc: 0.8753 - val_loss: 1.5912 - val_acc: 0.6571\n",
      "Epoch 3238/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3468 - acc: 0.8733 - val_loss: 1.5555 - val_acc: 0.6679\n",
      "Epoch 3239/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3443 - acc: 0.8767 - val_loss: 1.3368 - val_acc: 0.7125\n",
      "Epoch 3240/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3230 - acc: 0.8821 - val_loss: 1.4798 - val_acc: 0.6839\n",
      "Epoch 3241/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3599 - acc: 0.8689 - val_loss: 1.6144 - val_acc: 0.7062\n",
      "Epoch 3242/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3427 - acc: 0.8798 - val_loss: 1.5520 - val_acc: 0.6830\n",
      "Epoch 3243/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3527 - acc: 0.8713 - val_loss: 1.2445 - val_acc: 0.6982\n",
      "Epoch 3244/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3380 - acc: 0.8771 - val_loss: 1.5793 - val_acc: 0.6634\n",
      "Epoch 3245/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3583 - acc: 0.8719 - val_loss: 1.5499 - val_acc: 0.7214\n",
      "Epoch 3246/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3343 - acc: 0.8795 - val_loss: 1.8774 - val_acc: 0.6277\n",
      "Epoch 3247/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3663 - acc: 0.8753 - val_loss: 1.5831 - val_acc: 0.6741\n",
      "Epoch 3248/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3465 - acc: 0.8761 - val_loss: 1.5083 - val_acc: 0.6795\n",
      "Epoch 3249/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3560 - acc: 0.8713 - val_loss: 1.4017 - val_acc: 0.6875\n",
      "Epoch 3250/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3372 - acc: 0.8781 - val_loss: 1.4394 - val_acc: 0.6643\n",
      "Epoch 3251/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3434 - acc: 0.8751 - val_loss: 1.5693 - val_acc: 0.6393\n",
      "Epoch 3252/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3469 - acc: 0.8745 - val_loss: 1.3551 - val_acc: 0.6955\n",
      "Epoch 3253/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3370 - acc: 0.8811 - val_loss: 1.7347 - val_acc: 0.6232\n",
      "Epoch 3254/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3655 - acc: 0.8716 - val_loss: 1.8001 - val_acc: 0.6491\n",
      "Epoch 3255/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3557 - acc: 0.8759 - val_loss: 1.9346 - val_acc: 0.6250\n",
      "Epoch 3256/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3531 - acc: 0.8797 - val_loss: 1.3901 - val_acc: 0.6786\n",
      "Epoch 3257/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3518 - acc: 0.8733 - val_loss: 1.4686 - val_acc: 0.6848\n",
      "Epoch 3258/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3401 - acc: 0.8781 - val_loss: 1.3865 - val_acc: 0.7116\n",
      "Epoch 3259/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3342 - acc: 0.8793 - val_loss: 1.5209 - val_acc: 0.6714\n",
      "Epoch 3260/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3609 - acc: 0.8714 - val_loss: 1.5337 - val_acc: 0.7027\n",
      "Epoch 3261/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3493 - acc: 0.8750 - val_loss: 1.3838 - val_acc: 0.6955\n",
      "Epoch 3262/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3355 - acc: 0.8783 - val_loss: 1.9143 - val_acc: 0.6286\n",
      "Epoch 3263/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3509 - acc: 0.8747 - val_loss: 1.9903 - val_acc: 0.6429\n",
      "Epoch 3264/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3708 - acc: 0.8702 - val_loss: 1.9922 - val_acc: 0.6330\n",
      "Epoch 3265/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3644 - acc: 0.8728 - val_loss: 1.2877 - val_acc: 0.7330\n",
      "Epoch 3266/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3298 - acc: 0.8803 - val_loss: 1.5429 - val_acc: 0.7071\n",
      "Epoch 3267/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3399 - acc: 0.8787 - val_loss: 1.4752 - val_acc: 0.6598\n",
      "Epoch 3268/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3491 - acc: 0.8763 - val_loss: 1.5619 - val_acc: 0.6875\n",
      "Epoch 3269/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3355 - acc: 0.8785 - val_loss: 1.6687 - val_acc: 0.6848\n",
      "Epoch 3270/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3722 - acc: 0.8690 - val_loss: 1.7374 - val_acc: 0.6187\n",
      "Epoch 3271/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3407 - acc: 0.8805 - val_loss: 1.5689 - val_acc: 0.6804\n",
      "Epoch 3272/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3568 - acc: 0.8751 - val_loss: 1.5093 - val_acc: 0.6839\n",
      "Epoch 3273/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3396 - acc: 0.8776 - val_loss: 1.5292 - val_acc: 0.6920\n",
      "Epoch 3274/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3254 - acc: 0.8841 - val_loss: 1.4020 - val_acc: 0.6964\n",
      "Epoch 3275/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3548 - acc: 0.8705 - val_loss: 1.4492 - val_acc: 0.6902\n",
      "Epoch 3276/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3435 - acc: 0.8767 - val_loss: 1.4715 - val_acc: 0.6545\n",
      "Epoch 3277/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3482 - acc: 0.8719 - val_loss: 1.6917 - val_acc: 0.6527\n",
      "Epoch 3278/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3396 - acc: 0.8798 - val_loss: 1.3762 - val_acc: 0.7089\n",
      "Epoch 3279/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3375 - acc: 0.8778 - val_loss: 1.5106 - val_acc: 0.6821\n",
      "Epoch 3280/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3368 - acc: 0.8796 - val_loss: 1.6201 - val_acc: 0.6759\n",
      "Epoch 3281/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3401 - acc: 0.8801 - val_loss: 1.7336 - val_acc: 0.6518\n",
      "Epoch 3282/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3297 - acc: 0.8844 - val_loss: 1.4704 - val_acc: 0.7116\n",
      "Epoch 3283/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3357 - acc: 0.8771 - val_loss: 1.2736 - val_acc: 0.7125\n",
      "Epoch 3284/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3457 - acc: 0.8750 - val_loss: 1.4253 - val_acc: 0.7179\n",
      "Epoch 3285/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3344 - acc: 0.8785 - val_loss: 1.4956 - val_acc: 0.6777\n",
      "Epoch 3286/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3313 - acc: 0.8806 - val_loss: 1.4967 - val_acc: 0.6804\n",
      "Epoch 3287/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3440 - acc: 0.8769 - val_loss: 1.7054 - val_acc: 0.6723\n",
      "Epoch 3288/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3445 - acc: 0.8756 - val_loss: 1.3693 - val_acc: 0.7125\n",
      "Epoch 3289/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3344 - acc: 0.8808 - val_loss: 1.7099 - val_acc: 0.6741\n",
      "Epoch 3290/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3422 - acc: 0.8790 - val_loss: 1.7702 - val_acc: 0.6286\n",
      "Epoch 3291/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3463 - acc: 0.8755 - val_loss: 1.4699 - val_acc: 0.7036\n",
      "Epoch 3292/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3385 - acc: 0.8787 - val_loss: 1.5898 - val_acc: 0.6696\n",
      "Epoch 3293/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3404 - acc: 0.8750 - val_loss: 1.6911 - val_acc: 0.6518\n",
      "Epoch 3294/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3362 - acc: 0.8793 - val_loss: 1.3235 - val_acc: 0.6911\n",
      "Epoch 3295/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3339 - acc: 0.8766 - val_loss: 1.5303 - val_acc: 0.6991\n",
      "Epoch 3296/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3350 - acc: 0.8803 - val_loss: 1.8984 - val_acc: 0.6473\n",
      "Epoch 3297/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3458 - acc: 0.8766 - val_loss: 1.4706 - val_acc: 0.6866\n",
      "Epoch 3298/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3561 - acc: 0.8709 - val_loss: 1.2605 - val_acc: 0.7223\n",
      "Epoch 3299/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3272 - acc: 0.8787 - val_loss: 1.6940 - val_acc: 0.6536\n",
      "Epoch 3300/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3562 - acc: 0.8747 - val_loss: 1.6058 - val_acc: 0.6500\n",
      "Epoch 3301/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3528 - acc: 0.8772 - val_loss: 1.4154 - val_acc: 0.7134\n",
      "Epoch 3302/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3382 - acc: 0.8794 - val_loss: 2.1596 - val_acc: 0.6170\n",
      "Epoch 3303/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3519 - acc: 0.8761 - val_loss: 1.4381 - val_acc: 0.6554\n",
      "Epoch 3304/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.3448 - acc: 0.8807 - val_loss: 1.3268 - val_acc: 0.7205\n",
      "Epoch 3305/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3298 - acc: 0.8819 - val_loss: 1.6862 - val_acc: 0.6696\n",
      "Epoch 3306/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3238 - acc: 0.8813 - val_loss: 1.5422 - val_acc: 0.6955\n",
      "Epoch 3307/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3523 - acc: 0.8739 - val_loss: 1.8110 - val_acc: 0.6464\n",
      "Epoch 3308/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3359 - acc: 0.8805 - val_loss: 1.3670 - val_acc: 0.7250\n",
      "Epoch 3309/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3339 - acc: 0.8765 - val_loss: 1.5353 - val_acc: 0.6848\n",
      "Epoch 3310/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3280 - acc: 0.8834 - val_loss: 1.5077 - val_acc: 0.6777\n",
      "Epoch 3311/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3282 - acc: 0.8811 - val_loss: 1.6497 - val_acc: 0.6804\n",
      "Epoch 3312/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3426 - acc: 0.8766 - val_loss: 1.9138 - val_acc: 0.6455\n",
      "Epoch 3313/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3364 - acc: 0.8781 - val_loss: 1.7040 - val_acc: 0.6768\n",
      "Epoch 3314/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3531 - acc: 0.8781 - val_loss: 1.5716 - val_acc: 0.6732\n",
      "Epoch 3315/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3345 - acc: 0.8787 - val_loss: 1.4987 - val_acc: 0.6839\n",
      "Epoch 3316/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3259 - acc: 0.8807 - val_loss: 1.4173 - val_acc: 0.6973\n",
      "Epoch 3317/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3307 - acc: 0.8798 - val_loss: 1.3944 - val_acc: 0.6929\n",
      "Epoch 3318/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3396 - acc: 0.8774 - val_loss: 1.4340 - val_acc: 0.6768\n",
      "Epoch 3319/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3467 - acc: 0.8755 - val_loss: 1.6963 - val_acc: 0.6759\n",
      "Epoch 3320/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3388 - acc: 0.8793 - val_loss: 1.5167 - val_acc: 0.6964\n",
      "Epoch 3321/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3407 - acc: 0.8798 - val_loss: 1.7828 - val_acc: 0.6589\n",
      "Epoch 3322/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3348 - acc: 0.8779 - val_loss: 1.4026 - val_acc: 0.6937\n",
      "Epoch 3323/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3225 - acc: 0.8823 - val_loss: 1.8987 - val_acc: 0.6134\n",
      "Epoch 3324/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3630 - acc: 0.8733 - val_loss: 1.8862 - val_acc: 0.6598\n",
      "Epoch 3325/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3426 - acc: 0.8796 - val_loss: 1.4888 - val_acc: 0.6955\n",
      "Epoch 3326/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3310 - acc: 0.8797 - val_loss: 1.9116 - val_acc: 0.6491\n",
      "Epoch 3327/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3463 - acc: 0.8795 - val_loss: 1.8859 - val_acc: 0.6688\n",
      "Epoch 3328/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3430 - acc: 0.8740 - val_loss: 1.5072 - val_acc: 0.7080\n",
      "Epoch 3329/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3363 - acc: 0.8797 - val_loss: 1.5966 - val_acc: 0.6598\n",
      "Epoch 3330/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3520 - acc: 0.8748 - val_loss: 1.8071 - val_acc: 0.6866\n",
      "Epoch 3331/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3384 - acc: 0.8807 - val_loss: 1.7442 - val_acc: 0.6509\n",
      "Epoch 3332/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3351 - acc: 0.8801 - val_loss: 1.7707 - val_acc: 0.6027\n",
      "Epoch 3333/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3505 - acc: 0.8723 - val_loss: 1.5281 - val_acc: 0.6536\n",
      "Epoch 3334/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3570 - acc: 0.8740 - val_loss: 1.4738 - val_acc: 0.7232\n",
      "Epoch 3335/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3339 - acc: 0.8781 - val_loss: 1.5249 - val_acc: 0.6946\n",
      "Epoch 3336/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3434 - acc: 0.8739 - val_loss: 1.4671 - val_acc: 0.6893\n",
      "Epoch 3337/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3281 - acc: 0.8827 - val_loss: 1.8388 - val_acc: 0.6455\n",
      "Epoch 3338/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3410 - acc: 0.8807 - val_loss: 1.7894 - val_acc: 0.6429\n",
      "Epoch 3339/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3381 - acc: 0.8813 - val_loss: 1.4849 - val_acc: 0.6786\n",
      "Epoch 3340/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3492 - acc: 0.8751 - val_loss: 1.6223 - val_acc: 0.6304\n",
      "Epoch 3341/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3347 - acc: 0.8786 - val_loss: 1.3760 - val_acc: 0.6750\n",
      "Epoch 3342/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3374 - acc: 0.8789 - val_loss: 1.4476 - val_acc: 0.7205\n",
      "Epoch 3343/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3286 - acc: 0.8795 - val_loss: 1.5294 - val_acc: 0.6652\n",
      "Epoch 3344/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3348 - acc: 0.8801 - val_loss: 2.1531 - val_acc: 0.6179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3345/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3350 - acc: 0.8815 - val_loss: 1.6561 - val_acc: 0.6643\n",
      "Epoch 3346/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3385 - acc: 0.8829 - val_loss: 1.8796 - val_acc: 0.6464\n",
      "Epoch 3347/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3400 - acc: 0.8816 - val_loss: 1.5035 - val_acc: 0.7054\n",
      "Epoch 3348/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3233 - acc: 0.8829 - val_loss: 1.6447 - val_acc: 0.6759\n",
      "Epoch 3349/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3412 - acc: 0.8787 - val_loss: 1.5614 - val_acc: 0.6714\n",
      "Epoch 3350/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3279 - acc: 0.8803 - val_loss: 2.0374 - val_acc: 0.6062\n",
      "Epoch 3351/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3393 - acc: 0.8797 - val_loss: 1.7557 - val_acc: 0.6473\n",
      "Epoch 3352/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3527 - acc: 0.8759 - val_loss: 1.5431 - val_acc: 0.6732\n",
      "Epoch 3353/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3223 - acc: 0.8813 - val_loss: 1.9119 - val_acc: 0.6393\n",
      "Epoch 3354/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3523 - acc: 0.8738 - val_loss: 1.6344 - val_acc: 0.6634\n",
      "Epoch 3355/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3299 - acc: 0.8822 - val_loss: 1.6274 - val_acc: 0.6625\n",
      "Epoch 3356/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3330 - acc: 0.8824 - val_loss: 1.5106 - val_acc: 0.6848\n",
      "Epoch 3357/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3468 - acc: 0.8776 - val_loss: 1.2280 - val_acc: 0.7116\n",
      "Epoch 3358/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3279 - acc: 0.8823 - val_loss: 1.9937 - val_acc: 0.6571\n",
      "Epoch 3359/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3607 - acc: 0.8737 - val_loss: 1.5261 - val_acc: 0.6821\n",
      "Epoch 3360/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3188 - acc: 0.8867 - val_loss: 1.6278 - val_acc: 0.6750\n",
      "Epoch 3361/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3394 - acc: 0.8797 - val_loss: 1.5697 - val_acc: 0.6857\n",
      "Epoch 3362/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3400 - acc: 0.8785 - val_loss: 1.7364 - val_acc: 0.6598\n",
      "Epoch 3363/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3386 - acc: 0.8793 - val_loss: 1.6255 - val_acc: 0.6893\n",
      "Epoch 3364/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3401 - acc: 0.8765 - val_loss: 1.3862 - val_acc: 0.6893\n",
      "Epoch 3365/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3288 - acc: 0.8826 - val_loss: 1.5131 - val_acc: 0.6857\n",
      "Epoch 3366/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3341 - acc: 0.8781 - val_loss: 1.6025 - val_acc: 0.6554\n",
      "Epoch 3367/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3371 - acc: 0.8790 - val_loss: 1.3674 - val_acc: 0.7107\n",
      "Epoch 3368/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3459 - acc: 0.8725 - val_loss: 1.7932 - val_acc: 0.6509\n",
      "Epoch 3369/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3689 - acc: 0.8765 - val_loss: 1.7938 - val_acc: 0.6393\n",
      "Epoch 3370/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3364 - acc: 0.8802 - val_loss: 1.5870 - val_acc: 0.6723\n",
      "Epoch 3371/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3358 - acc: 0.8797 - val_loss: 1.9919 - val_acc: 0.6304\n",
      "Epoch 3372/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3548 - acc: 0.8747 - val_loss: 1.6616 - val_acc: 0.7018\n",
      "Epoch 3373/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3376 - acc: 0.8793 - val_loss: 1.6355 - val_acc: 0.6696\n",
      "Epoch 3374/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3473 - acc: 0.8776 - val_loss: 1.5032 - val_acc: 0.6786\n",
      "Epoch 3375/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3460 - acc: 0.8761 - val_loss: 1.5958 - val_acc: 0.6768\n",
      "Epoch 3376/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3466 - acc: 0.8779 - val_loss: 2.0988 - val_acc: 0.6089\n",
      "Epoch 3377/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3573 - acc: 0.8789 - val_loss: 2.1893 - val_acc: 0.6393\n",
      "Epoch 3378/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3482 - acc: 0.8784 - val_loss: 1.3336 - val_acc: 0.7143\n",
      "Epoch 3379/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3374 - acc: 0.8764 - val_loss: 2.0505 - val_acc: 0.6268\n",
      "Epoch 3380/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3435 - acc: 0.8793 - val_loss: 1.5978 - val_acc: 0.7018\n",
      "Epoch 3381/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3377 - acc: 0.8785 - val_loss: 1.4717 - val_acc: 0.7161\n",
      "Epoch 3382/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3380 - acc: 0.8811 - val_loss: 1.4257 - val_acc: 0.7116\n",
      "Epoch 3383/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3284 - acc: 0.8815 - val_loss: 1.9711 - val_acc: 0.6411\n",
      "Epoch 3384/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3469 - acc: 0.8753 - val_loss: 1.3873 - val_acc: 0.6946\n",
      "Epoch 3385/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3180 - acc: 0.8848 - val_loss: 1.4837 - val_acc: 0.6955\n",
      "Epoch 3386/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3254 - acc: 0.8798 - val_loss: 1.6876 - val_acc: 0.6652\n",
      "Epoch 3387/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3424 - acc: 0.8768 - val_loss: 1.6004 - val_acc: 0.6759\n",
      "Epoch 3388/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3588 - acc: 0.8743 - val_loss: 1.3800 - val_acc: 0.7179\n",
      "Epoch 3389/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3271 - acc: 0.8804 - val_loss: 1.8525 - val_acc: 0.6232\n",
      "Epoch 3390/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3425 - acc: 0.8758 - val_loss: 1.3025 - val_acc: 0.7089\n",
      "Epoch 3391/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3275 - acc: 0.8805 - val_loss: 1.6057 - val_acc: 0.6982\n",
      "Epoch 3392/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3371 - acc: 0.8811 - val_loss: 1.5644 - val_acc: 0.7080\n",
      "Epoch 3393/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3459 - acc: 0.8771 - val_loss: 1.8481 - val_acc: 0.6545\n",
      "Epoch 3394/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3444 - acc: 0.8776 - val_loss: 1.5822 - val_acc: 0.6795\n",
      "Epoch 3395/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3506 - acc: 0.8771 - val_loss: 1.4736 - val_acc: 0.7125\n",
      "Epoch 3396/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3185 - acc: 0.8827 - val_loss: 1.4833 - val_acc: 0.6929\n",
      "Epoch 3397/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3423 - acc: 0.8792 - val_loss: 1.4525 - val_acc: 0.7045\n",
      "Epoch 3398/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3166 - acc: 0.8841 - val_loss: 1.8492 - val_acc: 0.6152\n",
      "Epoch 3399/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3579 - acc: 0.8753 - val_loss: 1.5955 - val_acc: 0.6795\n",
      "Epoch 3400/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3282 - acc: 0.8805 - val_loss: 1.5055 - val_acc: 0.6911\n",
      "Epoch 3401/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3191 - acc: 0.8847 - val_loss: 1.7254 - val_acc: 0.6536\n",
      "Epoch 3402/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3560 - acc: 0.8763 - val_loss: 1.6415 - val_acc: 0.6616\n",
      "Epoch 3403/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3387 - acc: 0.8795 - val_loss: 1.5743 - val_acc: 0.6795\n",
      "Epoch 3404/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3212 - acc: 0.8839 - val_loss: 1.7239 - val_acc: 0.6616\n",
      "Epoch 3405/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3571 - acc: 0.8735 - val_loss: 1.7619 - val_acc: 0.6420\n",
      "Epoch 3406/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3327 - acc: 0.8819 - val_loss: 1.6851 - val_acc: 0.6625\n",
      "Epoch 3407/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3424 - acc: 0.8791 - val_loss: 1.5121 - val_acc: 0.6366\n",
      "Epoch 3408/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3557 - acc: 0.8709 - val_loss: 1.5640 - val_acc: 0.6705\n",
      "Epoch 3409/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3350 - acc: 0.8822 - val_loss: 1.5133 - val_acc: 0.6661\n",
      "Epoch 3410/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3389 - acc: 0.8795 - val_loss: 1.3856 - val_acc: 0.6929\n",
      "Epoch 3411/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3211 - acc: 0.8833 - val_loss: 1.5346 - val_acc: 0.6732\n",
      "Epoch 3412/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3293 - acc: 0.8813 - val_loss: 1.6289 - val_acc: 0.6955\n",
      "Epoch 3413/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3292 - acc: 0.8777 - val_loss: 1.7714 - val_acc: 0.6545\n",
      "Epoch 3414/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3252 - acc: 0.8831 - val_loss: 1.6570 - val_acc: 0.6473\n",
      "Epoch 3415/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3425 - acc: 0.8749 - val_loss: 1.7841 - val_acc: 0.6759\n",
      "Epoch 3416/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3665 - acc: 0.8723 - val_loss: 2.3091 - val_acc: 0.6223\n",
      "Epoch 3417/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3524 - acc: 0.8802 - val_loss: 1.4917 - val_acc: 0.6866\n",
      "Epoch 3418/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3267 - acc: 0.8825 - val_loss: 1.5902 - val_acc: 0.6795\n",
      "Epoch 3419/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3383 - acc: 0.8776 - val_loss: 1.5040 - val_acc: 0.7000\n",
      "Epoch 3420/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3380 - acc: 0.8803 - val_loss: 1.7110 - val_acc: 0.6652\n",
      "Epoch 3421/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3227 - acc: 0.8863 - val_loss: 1.7244 - val_acc: 0.6661\n",
      "Epoch 3422/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3452 - acc: 0.8779 - val_loss: 1.5126 - val_acc: 0.6821\n",
      "Epoch 3423/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3285 - acc: 0.8843 - val_loss: 2.0816 - val_acc: 0.6473\n",
      "Epoch 3424/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3459 - acc: 0.8800 - val_loss: 1.5983 - val_acc: 0.6571\n",
      "Epoch 3425/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3515 - acc: 0.8757 - val_loss: 1.9071 - val_acc: 0.6491\n",
      "Epoch 3426/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3327 - acc: 0.8793 - val_loss: 1.4015 - val_acc: 0.7054\n",
      "Epoch 3427/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3359 - acc: 0.8804 - val_loss: 1.7864 - val_acc: 0.6339\n",
      "Epoch 3428/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3377 - acc: 0.8766 - val_loss: 1.4917 - val_acc: 0.7134\n",
      "Epoch 3429/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3224 - acc: 0.8834 - val_loss: 2.1954 - val_acc: 0.6205\n",
      "Epoch 3430/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3566 - acc: 0.8773 - val_loss: 1.4761 - val_acc: 0.6857\n",
      "Epoch 3431/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3283 - acc: 0.8819 - val_loss: 1.6387 - val_acc: 0.6688\n",
      "Epoch 3432/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3363 - acc: 0.8804 - val_loss: 1.5268 - val_acc: 0.6920\n",
      "Epoch 3433/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3278 - acc: 0.8816 - val_loss: 1.6942 - val_acc: 0.6277\n",
      "Epoch 3434/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3526 - acc: 0.8774 - val_loss: 1.2410 - val_acc: 0.7304\n",
      "Epoch 3435/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3202 - acc: 0.8809 - val_loss: 1.3547 - val_acc: 0.6902\n",
      "Epoch 3436/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3461 - acc: 0.8757 - val_loss: 1.7123 - val_acc: 0.6920\n",
      "Epoch 3437/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3466 - acc: 0.8762 - val_loss: 1.3836 - val_acc: 0.7152\n",
      "Epoch 3438/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3204 - acc: 0.8841 - val_loss: 1.4364 - val_acc: 0.7188\n",
      "Epoch 3439/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3254 - acc: 0.8829 - val_loss: 1.7275 - val_acc: 0.6795\n",
      "Epoch 3440/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3432 - acc: 0.8776 - val_loss: 1.4716 - val_acc: 0.6875\n",
      "Epoch 3441/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3412 - acc: 0.8768 - val_loss: 1.3572 - val_acc: 0.6902\n",
      "Epoch 3442/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3365 - acc: 0.8786 - val_loss: 1.5386 - val_acc: 0.6821\n",
      "Epoch 3443/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3401 - acc: 0.8741 - val_loss: 1.6455 - val_acc: 0.6714\n",
      "Epoch 3444/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3223 - acc: 0.8807 - val_loss: 2.2489 - val_acc: 0.6339\n",
      "Epoch 3445/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3430 - acc: 0.8811 - val_loss: 1.2943 - val_acc: 0.7063\n",
      "Epoch 3446/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3186 - acc: 0.8837 - val_loss: 1.5190 - val_acc: 0.6955\n",
      "Epoch 3447/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3308 - acc: 0.8793 - val_loss: 1.8578 - val_acc: 0.6375\n",
      "Epoch 3448/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3488 - acc: 0.8761 - val_loss: 1.4175 - val_acc: 0.7330\n",
      "Epoch 3449/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3389 - acc: 0.8764 - val_loss: 1.6581 - val_acc: 0.6607\n",
      "Epoch 3450/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3359 - acc: 0.8807 - val_loss: 1.5809 - val_acc: 0.6607\n",
      "Epoch 3451/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3309 - acc: 0.8822 - val_loss: 1.6696 - val_acc: 0.6491\n",
      "Epoch 3452/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3592 - acc: 0.8751 - val_loss: 1.8780 - val_acc: 0.6518\n",
      "Epoch 3453/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3388 - acc: 0.8780 - val_loss: 1.8128 - val_acc: 0.6473\n",
      "Epoch 3454/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3428 - acc: 0.8790 - val_loss: 1.3637 - val_acc: 0.7027\n",
      "Epoch 3455/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3354 - acc: 0.8769 - val_loss: 1.4994 - val_acc: 0.7045\n",
      "Epoch 3456/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3348 - acc: 0.8795 - val_loss: 1.9175 - val_acc: 0.6348\n",
      "Epoch 3457/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3461 - acc: 0.8786 - val_loss: 1.4434 - val_acc: 0.7036\n",
      "Epoch 3458/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3190 - acc: 0.8849 - val_loss: 1.6230 - val_acc: 0.7054\n",
      "Epoch 3459/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3260 - acc: 0.8801 - val_loss: 1.7913 - val_acc: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3460/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3321 - acc: 0.8797 - val_loss: 1.7958 - val_acc: 0.6348\n",
      "Epoch 3461/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3436 - acc: 0.8787 - val_loss: 1.5825 - val_acc: 0.6839\n",
      "Epoch 3462/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3234 - acc: 0.8846 - val_loss: 1.3296 - val_acc: 0.7170\n",
      "Epoch 3463/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3251 - acc: 0.8826 - val_loss: 1.9217 - val_acc: 0.6491\n",
      "Epoch 3464/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3568 - acc: 0.8766 - val_loss: 2.1021 - val_acc: 0.6196\n",
      "Epoch 3465/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3432 - acc: 0.8812 - val_loss: 1.6571 - val_acc: 0.6857\n",
      "Epoch 3466/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3309 - acc: 0.8802 - val_loss: 1.9218 - val_acc: 0.6527\n",
      "Epoch 3467/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3516 - acc: 0.8768 - val_loss: 1.4933 - val_acc: 0.6893\n",
      "Epoch 3468/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3280 - acc: 0.8862 - val_loss: 1.4855 - val_acc: 0.6982\n",
      "Epoch 3469/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3395 - acc: 0.8801 - val_loss: 1.7749 - val_acc: 0.6473\n",
      "Epoch 3470/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3424 - acc: 0.8756 - val_loss: 1.4176 - val_acc: 0.7018\n",
      "Epoch 3471/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3160 - acc: 0.8862 - val_loss: 1.4349 - val_acc: 0.6973\n",
      "Epoch 3472/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3197 - acc: 0.8879 - val_loss: 1.3203 - val_acc: 0.6973\n",
      "Epoch 3473/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3172 - acc: 0.8840 - val_loss: 1.4523 - val_acc: 0.7080\n",
      "Epoch 3474/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3250 - acc: 0.8815 - val_loss: 1.8571 - val_acc: 0.6286\n",
      "Epoch 3475/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3541 - acc: 0.8787 - val_loss: 1.7989 - val_acc: 0.6429\n",
      "Epoch 3476/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3449 - acc: 0.8806 - val_loss: 2.0189 - val_acc: 0.6286\n",
      "Epoch 3477/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3482 - acc: 0.8771 - val_loss: 1.7118 - val_acc: 0.6839\n",
      "Epoch 3478/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3488 - acc: 0.8774 - val_loss: 2.2170 - val_acc: 0.5920\n",
      "Epoch 3479/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3393 - acc: 0.8831 - val_loss: 1.3310 - val_acc: 0.7214\n",
      "Epoch 3480/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3311 - acc: 0.8828 - val_loss: 1.5740 - val_acc: 0.6607\n",
      "Epoch 3481/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3096 - acc: 0.8889 - val_loss: 1.5785 - val_acc: 0.7000\n",
      "Epoch 3482/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3277 - acc: 0.8803 - val_loss: 1.7247 - val_acc: 0.6491\n",
      "Epoch 3483/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3564 - acc: 0.8727 - val_loss: 1.4599 - val_acc: 0.7098\n",
      "Epoch 3484/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3287 - acc: 0.8768 - val_loss: 1.8399 - val_acc: 0.6446\n",
      "Epoch 3485/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3358 - acc: 0.8811 - val_loss: 1.6865 - val_acc: 0.6938\n",
      "Epoch 3486/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3339 - acc: 0.8824 - val_loss: 1.3407 - val_acc: 0.7179\n",
      "Epoch 3487/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3389 - acc: 0.8773 - val_loss: 1.3314 - val_acc: 0.6937\n",
      "Epoch 3488/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3155 - acc: 0.8850 - val_loss: 1.8610 - val_acc: 0.6330\n",
      "Epoch 3489/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3423 - acc: 0.8789 - val_loss: 1.8308 - val_acc: 0.6295\n",
      "Epoch 3490/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3588 - acc: 0.8753 - val_loss: 1.3217 - val_acc: 0.7098\n",
      "Epoch 3491/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3357 - acc: 0.8819 - val_loss: 1.5232 - val_acc: 0.7062\n",
      "Epoch 3492/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3193 - acc: 0.8852 - val_loss: 1.4963 - val_acc: 0.7152\n",
      "Epoch 3493/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3290 - acc: 0.8823 - val_loss: 2.3263 - val_acc: 0.6089\n",
      "Epoch 3494/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3548 - acc: 0.8752 - val_loss: 1.9699 - val_acc: 0.6687\n",
      "Epoch 3495/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3311 - acc: 0.8810 - val_loss: 1.3822 - val_acc: 0.6884\n",
      "Epoch 3496/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3323 - acc: 0.8825 - val_loss: 1.6642 - val_acc: 0.6643\n",
      "Epoch 3497/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3231 - acc: 0.8831 - val_loss: 1.5448 - val_acc: 0.6821\n",
      "Epoch 3498/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3391 - acc: 0.8782 - val_loss: 1.4395 - val_acc: 0.7009\n",
      "Epoch 3499/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3197 - acc: 0.8801 - val_loss: 1.8073 - val_acc: 0.6723\n",
      "Epoch 3500/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3483 - acc: 0.8812 - val_loss: 1.5479 - val_acc: 0.6652\n",
      "Epoch 3501/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3197 - acc: 0.8849 - val_loss: 1.3565 - val_acc: 0.6955\n",
      "Epoch 3502/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3533 - acc: 0.8705 - val_loss: 1.3282 - val_acc: 0.6732\n",
      "Epoch 3503/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3397 - acc: 0.8765 - val_loss: 2.1255 - val_acc: 0.6170\n",
      "Epoch 3504/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3516 - acc: 0.8798 - val_loss: 1.4694 - val_acc: 0.6795\n",
      "Epoch 3505/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3243 - acc: 0.8806 - val_loss: 1.5022 - val_acc: 0.7009\n",
      "Epoch 3506/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3457 - acc: 0.8749 - val_loss: 1.3662 - val_acc: 0.7018\n",
      "Epoch 3507/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3113 - acc: 0.8865 - val_loss: 1.5388 - val_acc: 0.6777\n",
      "Epoch 3508/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3305 - acc: 0.8829 - val_loss: 1.9093 - val_acc: 0.6813\n",
      "Epoch 3509/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3344 - acc: 0.8813 - val_loss: 1.5347 - val_acc: 0.6866\n",
      "Epoch 3510/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8862 - val_loss: 1.7167 - val_acc: 0.6429\n",
      "Epoch 3511/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3353 - acc: 0.8791 - val_loss: 1.4590 - val_acc: 0.6723\n",
      "Epoch 3512/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3393 - acc: 0.8757 - val_loss: 1.4330 - val_acc: 0.7152\n",
      "Epoch 3513/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3279 - acc: 0.8841 - val_loss: 1.5915 - val_acc: 0.6821\n",
      "Epoch 3514/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3350 - acc: 0.8803 - val_loss: 2.2253 - val_acc: 0.5946\n",
      "Epoch 3515/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3610 - acc: 0.8775 - val_loss: 2.0509 - val_acc: 0.6384\n",
      "Epoch 3516/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3310 - acc: 0.8873 - val_loss: 1.5103 - val_acc: 0.7107\n",
      "Epoch 3517/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3432 - acc: 0.8798 - val_loss: 1.4728 - val_acc: 0.7009\n",
      "Epoch 3518/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3234 - acc: 0.8825 - val_loss: 1.3681 - val_acc: 0.7089\n",
      "Epoch 3519/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3259 - acc: 0.8809 - val_loss: 1.6233 - val_acc: 0.6643\n",
      "Epoch 3520/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3322 - acc: 0.8814 - val_loss: 1.5661 - val_acc: 0.6893\n",
      "Epoch 3521/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3179 - acc: 0.8863 - val_loss: 1.6293 - val_acc: 0.6616\n",
      "Epoch 3522/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3380 - acc: 0.8804 - val_loss: 2.0190 - val_acc: 0.6429\n",
      "Epoch 3523/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3423 - acc: 0.8789 - val_loss: 2.2081 - val_acc: 0.6134\n",
      "Epoch 3524/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3461 - acc: 0.8760 - val_loss: 1.7248 - val_acc: 0.6598\n",
      "Epoch 3525/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3371 - acc: 0.8813 - val_loss: 2.1358 - val_acc: 0.6143\n",
      "Epoch 3526/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3375 - acc: 0.8800 - val_loss: 1.8252 - val_acc: 0.6777\n",
      "Epoch 3527/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3262 - acc: 0.8822 - val_loss: 1.5671 - val_acc: 0.6813\n",
      "Epoch 3528/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3334 - acc: 0.8815 - val_loss: 1.6093 - val_acc: 0.6634\n",
      "Epoch 3529/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3252 - acc: 0.8805 - val_loss: 1.3785 - val_acc: 0.6991\n",
      "Epoch 3530/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3187 - acc: 0.8843 - val_loss: 1.9766 - val_acc: 0.6402\n",
      "Epoch 3531/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3278 - acc: 0.8828 - val_loss: 1.7310 - val_acc: 0.6884\n",
      "Epoch 3532/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3174 - acc: 0.8848 - val_loss: 1.7350 - val_acc: 0.6571\n",
      "Epoch 3533/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3374 - acc: 0.8795 - val_loss: 1.5818 - val_acc: 0.6571\n",
      "Epoch 3534/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3296 - acc: 0.8826 - val_loss: 2.4778 - val_acc: 0.5750\n",
      "Epoch 3535/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3339 - acc: 0.8879 - val_loss: 1.4236 - val_acc: 0.6946\n",
      "Epoch 3536/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3298 - acc: 0.8793 - val_loss: 1.9615 - val_acc: 0.6652\n",
      "Epoch 3537/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3242 - acc: 0.8871 - val_loss: 1.7890 - val_acc: 0.6688\n",
      "Epoch 3538/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3490 - acc: 0.8799 - val_loss: 1.5640 - val_acc: 0.6768\n",
      "Epoch 3539/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3565 - acc: 0.8745 - val_loss: 1.6834 - val_acc: 0.6839\n",
      "Epoch 3540/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3123 - acc: 0.8848 - val_loss: 1.4133 - val_acc: 0.7009\n",
      "Epoch 3541/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3174 - acc: 0.8860 - val_loss: 1.9103 - val_acc: 0.6464\n",
      "Epoch 3542/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3384 - acc: 0.8785 - val_loss: 1.7133 - val_acc: 0.6723\n",
      "Epoch 3543/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3270 - acc: 0.8821 - val_loss: 1.4734 - val_acc: 0.7045\n",
      "Epoch 3544/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3182 - acc: 0.8854 - val_loss: 1.3985 - val_acc: 0.7161\n",
      "Epoch 3545/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3173 - acc: 0.8843 - val_loss: 1.6732 - val_acc: 0.6580\n",
      "Epoch 3546/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3263 - acc: 0.8857 - val_loss: 1.6933 - val_acc: 0.6893\n",
      "Epoch 3547/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3316 - acc: 0.8789 - val_loss: 1.5649 - val_acc: 0.6857\n",
      "Epoch 3548/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3230 - acc: 0.8839 - val_loss: 1.9939 - val_acc: 0.6125\n",
      "Epoch 3549/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3459 - acc: 0.8793 - val_loss: 1.8309 - val_acc: 0.6295\n",
      "Epoch 3550/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3281 - acc: 0.8850 - val_loss: 1.3598 - val_acc: 0.7071\n",
      "Epoch 3551/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3168 - acc: 0.8823 - val_loss: 1.8357 - val_acc: 0.6768\n",
      "Epoch 3552/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3388 - acc: 0.8765 - val_loss: 1.7244 - val_acc: 0.6652\n",
      "Epoch 3553/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3286 - acc: 0.8849 - val_loss: 2.0509 - val_acc: 0.6312\n",
      "Epoch 3554/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3406 - acc: 0.8756 - val_loss: 1.5645 - val_acc: 0.6893\n",
      "Epoch 3555/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3359 - acc: 0.8781 - val_loss: 1.4820 - val_acc: 0.6527\n",
      "Epoch 3556/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3435 - acc: 0.8767 - val_loss: 1.6755 - val_acc: 0.6786\n",
      "Epoch 3557/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3304 - acc: 0.8797 - val_loss: 1.9698 - val_acc: 0.6304\n",
      "Epoch 3558/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3250 - acc: 0.8844 - val_loss: 1.6148 - val_acc: 0.6839\n",
      "Epoch 3559/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3197 - acc: 0.8845 - val_loss: 1.5725 - val_acc: 0.6839\n",
      "Epoch 3560/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3248 - acc: 0.8831 - val_loss: 1.5668 - val_acc: 0.6955\n",
      "Epoch 3561/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3326 - acc: 0.8814 - val_loss: 1.9922 - val_acc: 0.6170\n",
      "Epoch 3562/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3416 - acc: 0.8804 - val_loss: 1.6488 - val_acc: 0.6839\n",
      "Epoch 3563/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3337 - acc: 0.8826 - val_loss: 1.5424 - val_acc: 0.6339\n",
      "Epoch 3564/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3459 - acc: 0.8767 - val_loss: 1.7852 - val_acc: 0.6732\n",
      "Epoch 3565/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3203 - acc: 0.8881 - val_loss: 1.2662 - val_acc: 0.7205\n",
      "Epoch 3566/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3253 - acc: 0.8853 - val_loss: 1.9937 - val_acc: 0.6661\n",
      "Epoch 3567/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3408 - acc: 0.8825 - val_loss: 1.9685 - val_acc: 0.6152\n",
      "Epoch 3568/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3531 - acc: 0.8773 - val_loss: 1.7300 - val_acc: 0.6696\n",
      "Epoch 3569/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3394 - acc: 0.8803 - val_loss: 1.7709 - val_acc: 0.6625\n",
      "Epoch 3570/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3529 - acc: 0.8781 - val_loss: 1.8272 - val_acc: 0.6420\n",
      "Epoch 3571/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3444 - acc: 0.8766 - val_loss: 1.2857 - val_acc: 0.7143\n",
      "Epoch 3572/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3169 - acc: 0.8857 - val_loss: 1.8614 - val_acc: 0.6482\n",
      "Epoch 3573/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3193 - acc: 0.8843 - val_loss: 1.5299 - val_acc: 0.6795\n",
      "Epoch 3574/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3463 - acc: 0.8787 - val_loss: 1.7113 - val_acc: 0.6402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3575/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3362 - acc: 0.8828 - val_loss: 1.4962 - val_acc: 0.7009\n",
      "Epoch 3576/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3293 - acc: 0.8803 - val_loss: 2.1039 - val_acc: 0.6187\n",
      "Epoch 3577/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3296 - acc: 0.8851 - val_loss: 2.0314 - val_acc: 0.6473\n",
      "Epoch 3578/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3436 - acc: 0.8783 - val_loss: 1.4451 - val_acc: 0.7071\n",
      "Epoch 3579/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3184 - acc: 0.8835 - val_loss: 1.8117 - val_acc: 0.6643\n",
      "Epoch 3580/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3363 - acc: 0.8822 - val_loss: 2.2325 - val_acc: 0.6420\n",
      "Epoch 3581/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3479 - acc: 0.8846 - val_loss: 1.4294 - val_acc: 0.7063\n",
      "Epoch 3582/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3345 - acc: 0.8807 - val_loss: 1.5149 - val_acc: 0.6759\n",
      "Epoch 3583/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3250 - acc: 0.8814 - val_loss: 1.2971 - val_acc: 0.7179\n",
      "Epoch 3584/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3078 - acc: 0.8893 - val_loss: 1.8531 - val_acc: 0.6625\n",
      "Epoch 3585/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3235 - acc: 0.8851 - val_loss: 1.3303 - val_acc: 0.7098\n",
      "Epoch 3586/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3174 - acc: 0.8851 - val_loss: 1.3593 - val_acc: 0.7321\n",
      "Epoch 3587/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3253 - acc: 0.8830 - val_loss: 1.4865 - val_acc: 0.6937\n",
      "Epoch 3588/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3163 - acc: 0.8866 - val_loss: 1.6894 - val_acc: 0.6571\n",
      "Epoch 3589/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3231 - acc: 0.8836 - val_loss: 1.7263 - val_acc: 0.6893\n",
      "Epoch 3590/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3304 - acc: 0.8811 - val_loss: 1.4327 - val_acc: 0.7107\n",
      "Epoch 3591/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3177 - acc: 0.8874 - val_loss: 1.6760 - val_acc: 0.6821\n",
      "Epoch 3592/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3188 - acc: 0.8854 - val_loss: 1.6804 - val_acc: 0.6786\n",
      "Epoch 3593/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3195 - acc: 0.8862 - val_loss: 1.5053 - val_acc: 0.6875\n",
      "Epoch 3594/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3263 - acc: 0.8839 - val_loss: 1.7524 - val_acc: 0.6321\n",
      "Epoch 3595/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3419 - acc: 0.8785 - val_loss: 1.5292 - val_acc: 0.6696\n",
      "Epoch 3596/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3230 - acc: 0.8845 - val_loss: 1.5243 - val_acc: 0.7205\n",
      "Epoch 3597/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3172 - acc: 0.8889 - val_loss: 1.5343 - val_acc: 0.6813\n",
      "Epoch 3598/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3425 - acc: 0.8779 - val_loss: 1.7934 - val_acc: 0.6196\n",
      "Epoch 3599/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3388 - acc: 0.8807 - val_loss: 1.5992 - val_acc: 0.6929\n",
      "Epoch 3600/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3108 - acc: 0.8863 - val_loss: 1.6857 - val_acc: 0.6768\n",
      "Epoch 3601/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3237 - acc: 0.8823 - val_loss: 1.6590 - val_acc: 0.6670\n",
      "Epoch 3602/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3370 - acc: 0.8798 - val_loss: 1.5278 - val_acc: 0.6848\n",
      "Epoch 3603/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8856 - val_loss: 1.9635 - val_acc: 0.6214\n",
      "Epoch 3604/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3307 - acc: 0.8841 - val_loss: 1.3064 - val_acc: 0.7268\n",
      "Epoch 3605/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3212 - acc: 0.8828 - val_loss: 1.5083 - val_acc: 0.6884\n",
      "Epoch 3606/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3181 - acc: 0.8858 - val_loss: 1.9371 - val_acc: 0.6473\n",
      "Epoch 3607/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3399 - acc: 0.8854 - val_loss: 1.7024 - val_acc: 0.6812\n",
      "Epoch 3608/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3278 - acc: 0.8845 - val_loss: 1.6709 - val_acc: 0.6616\n",
      "Epoch 3609/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3401 - acc: 0.8777 - val_loss: 1.5030 - val_acc: 0.6839\n",
      "Epoch 3610/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3135 - acc: 0.8865 - val_loss: 1.4135 - val_acc: 0.7241\n",
      "Epoch 3611/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3155 - acc: 0.8839 - val_loss: 1.8606 - val_acc: 0.6339\n",
      "Epoch 3612/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3331 - acc: 0.8809 - val_loss: 1.6872 - val_acc: 0.6357\n",
      "Epoch 3613/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3210 - acc: 0.8848 - val_loss: 1.4066 - val_acc: 0.6964\n",
      "Epoch 3614/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3214 - acc: 0.8839 - val_loss: 1.8308 - val_acc: 0.6661\n",
      "Epoch 3615/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3094 - acc: 0.8880 - val_loss: 1.7883 - val_acc: 0.6402\n",
      "Epoch 3616/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3533 - acc: 0.8788 - val_loss: 1.4170 - val_acc: 0.7312\n",
      "Epoch 3617/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3253 - acc: 0.8826 - val_loss: 1.6799 - val_acc: 0.6705\n",
      "Epoch 3618/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3127 - acc: 0.8882 - val_loss: 1.6523 - val_acc: 0.6848\n",
      "Epoch 3619/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3171 - acc: 0.8862 - val_loss: 1.2935 - val_acc: 0.7223\n",
      "Epoch 3620/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3240 - acc: 0.8836 - val_loss: 1.3551 - val_acc: 0.7250\n",
      "Epoch 3621/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3169 - acc: 0.8850 - val_loss: 1.7045 - val_acc: 0.6759\n",
      "Epoch 3622/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3198 - acc: 0.8853 - val_loss: 1.5413 - val_acc: 0.6911\n",
      "Epoch 3623/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3111 - acc: 0.8851 - val_loss: 1.9391 - val_acc: 0.6286\n",
      "Epoch 3624/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3324 - acc: 0.8798 - val_loss: 1.4135 - val_acc: 0.7125\n",
      "Epoch 3625/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3443 - acc: 0.8790 - val_loss: 1.4992 - val_acc: 0.7009\n",
      "Epoch 3626/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3157 - acc: 0.8835 - val_loss: 2.2744 - val_acc: 0.5839\n",
      "Epoch 3627/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3554 - acc: 0.8769 - val_loss: 1.3984 - val_acc: 0.7098\n",
      "Epoch 3628/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3169 - acc: 0.8845 - val_loss: 1.7149 - val_acc: 0.6652\n",
      "Epoch 3629/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3330 - acc: 0.8821 - val_loss: 1.9843 - val_acc: 0.6411\n",
      "Epoch 3630/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3422 - acc: 0.8824 - val_loss: 1.7491 - val_acc: 0.6857\n",
      "Epoch 3631/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3219 - acc: 0.8827 - val_loss: 1.8133 - val_acc: 0.6473\n",
      "Epoch 3632/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3196 - acc: 0.8858 - val_loss: 1.6466 - val_acc: 0.6625\n",
      "Epoch 3633/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3268 - acc: 0.8861 - val_loss: 1.5362 - val_acc: 0.6723\n",
      "Epoch 3634/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3216 - acc: 0.8836 - val_loss: 1.6627 - val_acc: 0.6679\n",
      "Epoch 3635/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3371 - acc: 0.8814 - val_loss: 1.2712 - val_acc: 0.7268\n",
      "Epoch 3636/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3087 - acc: 0.8873 - val_loss: 1.6616 - val_acc: 0.6750\n",
      "Epoch 3637/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3218 - acc: 0.8819 - val_loss: 1.2879 - val_acc: 0.6973\n",
      "Epoch 3638/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3138 - acc: 0.8851 - val_loss: 1.8314 - val_acc: 0.6527\n",
      "Epoch 3639/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3318 - acc: 0.8842 - val_loss: 1.6293 - val_acc: 0.6875\n",
      "Epoch 3640/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3372 - acc: 0.8777 - val_loss: 1.3458 - val_acc: 0.7250\n",
      "Epoch 3641/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2958 - acc: 0.8933 - val_loss: 1.9174 - val_acc: 0.6616\n",
      "Epoch 3642/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3255 - acc: 0.8841 - val_loss: 1.9489 - val_acc: 0.6634\n",
      "Epoch 3643/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3377 - acc: 0.8814 - val_loss: 1.6950 - val_acc: 0.6616\n",
      "Epoch 3644/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3090 - acc: 0.8891 - val_loss: 1.9033 - val_acc: 0.6187\n",
      "Epoch 3645/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3486 - acc: 0.8797 - val_loss: 1.5198 - val_acc: 0.7071\n",
      "Epoch 3646/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3264 - acc: 0.8829 - val_loss: 1.3157 - val_acc: 0.7250\n",
      "Epoch 3647/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3063 - acc: 0.8903 - val_loss: 1.7136 - val_acc: 0.6750\n",
      "Epoch 3648/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3312 - acc: 0.8829 - val_loss: 1.5205 - val_acc: 0.7152\n",
      "Epoch 3649/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3255 - acc: 0.8820 - val_loss: 1.6473 - val_acc: 0.6589\n",
      "Epoch 3650/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3341 - acc: 0.8843 - val_loss: 1.3582 - val_acc: 0.7214\n",
      "Epoch 3651/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3208 - acc: 0.8823 - val_loss: 1.3905 - val_acc: 0.7375\n",
      "Epoch 3652/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3237 - acc: 0.8832 - val_loss: 1.2769 - val_acc: 0.7205\n",
      "Epoch 3653/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3123 - acc: 0.8859 - val_loss: 1.4938 - val_acc: 0.6911\n",
      "Epoch 3654/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3210 - acc: 0.8877 - val_loss: 1.6450 - val_acc: 0.6804\n",
      "Epoch 3655/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3326 - acc: 0.8792 - val_loss: 1.5220 - val_acc: 0.6857\n",
      "Epoch 3656/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3158 - acc: 0.8850 - val_loss: 1.7781 - val_acc: 0.6446\n",
      "Epoch 3657/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3268 - acc: 0.8854 - val_loss: 1.4784 - val_acc: 0.6938\n",
      "Epoch 3658/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3386 - acc: 0.8807 - val_loss: 1.5144 - val_acc: 0.6982\n",
      "Epoch 3659/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3122 - acc: 0.8881 - val_loss: 1.5114 - val_acc: 0.7098\n",
      "Epoch 3660/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3209 - acc: 0.8812 - val_loss: 2.0376 - val_acc: 0.6277\n",
      "Epoch 3661/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3621 - acc: 0.8769 - val_loss: 1.5693 - val_acc: 0.6946\n",
      "Epoch 3662/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3176 - acc: 0.8843 - val_loss: 1.4280 - val_acc: 0.7188\n",
      "Epoch 3663/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3162 - acc: 0.8843 - val_loss: 1.4763 - val_acc: 0.7000\n",
      "Epoch 3664/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3174 - acc: 0.8851 - val_loss: 1.5102 - val_acc: 0.6866\n",
      "Epoch 3665/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3232 - acc: 0.8842 - val_loss: 1.4586 - val_acc: 0.7107\n",
      "Epoch 3666/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3288 - acc: 0.8841 - val_loss: 1.6253 - val_acc: 0.6741\n",
      "Epoch 3667/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3173 - acc: 0.8875 - val_loss: 2.1278 - val_acc: 0.6268\n",
      "Epoch 3668/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3570 - acc: 0.8810 - val_loss: 1.3346 - val_acc: 0.7214\n",
      "Epoch 3669/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3196 - acc: 0.8839 - val_loss: 1.9137 - val_acc: 0.6759\n",
      "Epoch 3670/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3160 - acc: 0.8867 - val_loss: 1.6147 - val_acc: 0.6598\n",
      "Epoch 3671/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3153 - acc: 0.8875 - val_loss: 1.5719 - val_acc: 0.6911\n",
      "Epoch 3672/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3145 - acc: 0.8868 - val_loss: 1.7043 - val_acc: 0.6812\n",
      "Epoch 3673/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3312 - acc: 0.8850 - val_loss: 1.5129 - val_acc: 0.7143\n",
      "Epoch 3674/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3154 - acc: 0.8875 - val_loss: 1.6876 - val_acc: 0.6545\n",
      "Epoch 3675/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3324 - acc: 0.8784 - val_loss: 1.4459 - val_acc: 0.7107\n",
      "Epoch 3676/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3146 - acc: 0.8843 - val_loss: 1.7081 - val_acc: 0.6429\n",
      "Epoch 3677/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3244 - acc: 0.8831 - val_loss: 1.9192 - val_acc: 0.6196\n",
      "Epoch 3678/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3163 - acc: 0.8833 - val_loss: 2.0157 - val_acc: 0.6375\n",
      "Epoch 3679/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3526 - acc: 0.8786 - val_loss: 1.6993 - val_acc: 0.6607\n",
      "Epoch 3680/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3134 - acc: 0.8890 - val_loss: 1.6199 - val_acc: 0.6795\n",
      "Epoch 3681/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3237 - acc: 0.8826 - val_loss: 1.5435 - val_acc: 0.6830\n",
      "Epoch 3682/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3101 - acc: 0.8878 - val_loss: 1.9483 - val_acc: 0.6679\n",
      "Epoch 3683/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3244 - acc: 0.8836 - val_loss: 1.4918 - val_acc: 0.7089\n",
      "Epoch 3684/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3210 - acc: 0.8855 - val_loss: 1.5585 - val_acc: 0.6687\n",
      "Epoch 3685/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3356 - acc: 0.8837 - val_loss: 1.4923 - val_acc: 0.7214\n",
      "Epoch 3686/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3034 - acc: 0.8898 - val_loss: 1.7113 - val_acc: 0.6571\n",
      "Epoch 3687/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3444 - acc: 0.8765 - val_loss: 1.1969 - val_acc: 0.7286\n",
      "Epoch 3688/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3038 - acc: 0.8896 - val_loss: 1.5043 - val_acc: 0.6893\n",
      "Epoch 3689/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3194 - acc: 0.8862 - val_loss: 1.5030 - val_acc: 0.7054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3690/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3259 - acc: 0.8813 - val_loss: 1.6966 - val_acc: 0.6875\n",
      "Epoch 3691/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3187 - acc: 0.8863 - val_loss: 1.3856 - val_acc: 0.7000\n",
      "Epoch 3692/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3378 - acc: 0.8793 - val_loss: 1.5188 - val_acc: 0.6866\n",
      "Epoch 3693/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3047 - acc: 0.8887 - val_loss: 1.6688 - val_acc: 0.6625\n",
      "Epoch 3694/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3226 - acc: 0.8847 - val_loss: 1.3558 - val_acc: 0.7152\n",
      "Epoch 3695/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3230 - acc: 0.8844 - val_loss: 1.5244 - val_acc: 0.6741\n",
      "Epoch 3696/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3133 - acc: 0.8839 - val_loss: 1.7656 - val_acc: 0.6705\n",
      "Epoch 3697/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3255 - acc: 0.8833 - val_loss: 1.6802 - val_acc: 0.6804\n",
      "Epoch 3698/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3232 - acc: 0.8850 - val_loss: 1.4699 - val_acc: 0.7089\n",
      "Epoch 3699/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3031 - acc: 0.8892 - val_loss: 1.3913 - val_acc: 0.7071\n",
      "Epoch 3700/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3142 - acc: 0.8871 - val_loss: 1.3786 - val_acc: 0.7054\n",
      "Epoch 3701/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3314 - acc: 0.8829 - val_loss: 1.8285 - val_acc: 0.6652\n",
      "Epoch 3702/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3450 - acc: 0.8801 - val_loss: 1.4032 - val_acc: 0.7080\n",
      "Epoch 3703/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3215 - acc: 0.8827 - val_loss: 1.5261 - val_acc: 0.6750\n",
      "Epoch 3704/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3246 - acc: 0.8819 - val_loss: 1.8654 - val_acc: 0.6920\n",
      "Epoch 3705/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3120 - acc: 0.8867 - val_loss: 1.9136 - val_acc: 0.6571\n",
      "Epoch 3706/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3359 - acc: 0.8845 - val_loss: 1.6006 - val_acc: 0.7045\n",
      "Epoch 3707/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3084 - acc: 0.8873 - val_loss: 1.3555 - val_acc: 0.7045\n",
      "Epoch 3708/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3105 - acc: 0.8882 - val_loss: 1.9824 - val_acc: 0.6589\n",
      "Epoch 3709/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3471 - acc: 0.8807 - val_loss: 1.5766 - val_acc: 0.7000\n",
      "Epoch 3710/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3304 - acc: 0.8834 - val_loss: 1.5915 - val_acc: 0.6661\n",
      "Epoch 3711/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3148 - acc: 0.8839 - val_loss: 1.4207 - val_acc: 0.6839\n",
      "Epoch 3712/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3309 - acc: 0.8795 - val_loss: 1.7093 - val_acc: 0.6634\n",
      "Epoch 3713/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3214 - acc: 0.8845 - val_loss: 1.4801 - val_acc: 0.6884\n",
      "Epoch 3714/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3193 - acc: 0.8820 - val_loss: 1.9405 - val_acc: 0.6554\n",
      "Epoch 3715/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3235 - acc: 0.8855 - val_loss: 1.3776 - val_acc: 0.7125\n",
      "Epoch 3716/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3117 - acc: 0.8891 - val_loss: 1.4119 - val_acc: 0.7250\n",
      "Epoch 3717/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3066 - acc: 0.8906 - val_loss: 1.5462 - val_acc: 0.6982\n",
      "Epoch 3718/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3239 - acc: 0.8849 - val_loss: 1.9826 - val_acc: 0.6223\n",
      "Epoch 3719/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3399 - acc: 0.8818 - val_loss: 1.4957 - val_acc: 0.7000\n",
      "Epoch 3720/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3099 - acc: 0.8888 - val_loss: 1.4179 - val_acc: 0.7089\n",
      "Epoch 3721/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3153 - acc: 0.8836 - val_loss: 1.4423 - val_acc: 0.6875\n",
      "Epoch 3722/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3312 - acc: 0.8798 - val_loss: 1.6481 - val_acc: 0.6920\n",
      "Epoch 3723/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3290 - acc: 0.8849 - val_loss: 1.7999 - val_acc: 0.6518\n",
      "Epoch 3724/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3255 - acc: 0.8810 - val_loss: 1.4947 - val_acc: 0.6911\n",
      "Epoch 3725/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3282 - acc: 0.8839 - val_loss: 2.1238 - val_acc: 0.5804\n",
      "Epoch 3726/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3520 - acc: 0.8791 - val_loss: 1.6374 - val_acc: 0.6839\n",
      "Epoch 3727/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3083 - acc: 0.8889 - val_loss: 1.2887 - val_acc: 0.7411\n",
      "Epoch 3728/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3123 - acc: 0.8872 - val_loss: 1.7152 - val_acc: 0.6670\n",
      "Epoch 3729/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3212 - acc: 0.8863 - val_loss: 1.7155 - val_acc: 0.6768\n",
      "Epoch 3730/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3176 - acc: 0.8875 - val_loss: 1.6790 - val_acc: 0.6786\n",
      "Epoch 3731/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3057 - acc: 0.8873 - val_loss: 1.8517 - val_acc: 0.6134\n",
      "Epoch 3732/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3339 - acc: 0.8824 - val_loss: 1.8108 - val_acc: 0.6616\n",
      "Epoch 3733/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3142 - acc: 0.8860 - val_loss: 1.9757 - val_acc: 0.6589\n",
      "Epoch 3734/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3414 - acc: 0.8857 - val_loss: 1.4604 - val_acc: 0.7250\n",
      "Epoch 3735/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3169 - acc: 0.8851 - val_loss: 1.7045 - val_acc: 0.6723\n",
      "Epoch 3736/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3056 - acc: 0.8869 - val_loss: 1.6404 - val_acc: 0.6866\n",
      "Epoch 3737/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3234 - acc: 0.8878 - val_loss: 1.9076 - val_acc: 0.6679\n",
      "Epoch 3738/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3354 - acc: 0.8813 - val_loss: 1.5174 - val_acc: 0.6902\n",
      "Epoch 3739/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3271 - acc: 0.8837 - val_loss: 1.4708 - val_acc: 0.7196\n",
      "Epoch 3740/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3180 - acc: 0.8852 - val_loss: 1.6715 - val_acc: 0.6777\n",
      "Epoch 3741/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3332 - acc: 0.8845 - val_loss: 1.8930 - val_acc: 0.6741\n",
      "Epoch 3742/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3273 - acc: 0.8848 - val_loss: 1.7084 - val_acc: 0.6759\n",
      "Epoch 3743/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3207 - acc: 0.8859 - val_loss: 1.5886 - val_acc: 0.6884\n",
      "Epoch 3744/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3197 - acc: 0.8841 - val_loss: 1.3025 - val_acc: 0.7170\n",
      "Epoch 3745/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3069 - acc: 0.8882 - val_loss: 1.7680 - val_acc: 0.6821\n",
      "Epoch 3746/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8872 - val_loss: 1.7406 - val_acc: 0.6580\n",
      "Epoch 3747/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3287 - acc: 0.8799 - val_loss: 1.2982 - val_acc: 0.7098\n",
      "Epoch 3748/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3134 - acc: 0.8848 - val_loss: 1.4443 - val_acc: 0.7036\n",
      "Epoch 3749/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3039 - acc: 0.8892 - val_loss: 1.7319 - val_acc: 0.6929\n",
      "Epoch 3750/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3105 - acc: 0.8888 - val_loss: 1.4781 - val_acc: 0.7009\n",
      "Epoch 3751/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3154 - acc: 0.8882 - val_loss: 2.2095 - val_acc: 0.6214\n",
      "Epoch 3752/5000\n",
      "15008/15008 [==============================] - 1s 82us/step - loss: 0.3410 - acc: 0.8837 - val_loss: 2.0444 - val_acc: 0.6580\n",
      "Epoch 3753/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3372 - acc: 0.8830 - val_loss: 1.6361 - val_acc: 0.6964\n",
      "Epoch 3754/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3335 - acc: 0.8901 - val_loss: 1.5085 - val_acc: 0.6911\n",
      "Epoch 3755/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3302 - acc: 0.8827 - val_loss: 1.6459 - val_acc: 0.6509\n",
      "Epoch 3756/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3170 - acc: 0.8879 - val_loss: 1.7193 - val_acc: 0.6786\n",
      "Epoch 3757/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3227 - acc: 0.8810 - val_loss: 1.6512 - val_acc: 0.6848\n",
      "Epoch 3758/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3098 - acc: 0.8861 - val_loss: 1.4567 - val_acc: 0.6527\n",
      "Epoch 3759/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3354 - acc: 0.8759 - val_loss: 1.7570 - val_acc: 0.6759\n",
      "Epoch 3760/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3202 - acc: 0.8854 - val_loss: 1.8736 - val_acc: 0.6304\n",
      "Epoch 3761/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3348 - acc: 0.8844 - val_loss: 1.5231 - val_acc: 0.6732\n",
      "Epoch 3762/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3107 - acc: 0.8867 - val_loss: 1.8090 - val_acc: 0.6625\n",
      "Epoch 3763/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3188 - acc: 0.8857 - val_loss: 1.7409 - val_acc: 0.6768\n",
      "Epoch 3764/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3043 - acc: 0.8901 - val_loss: 1.5801 - val_acc: 0.6705\n",
      "Epoch 3765/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3201 - acc: 0.8841 - val_loss: 2.5509 - val_acc: 0.5795\n",
      "Epoch 3766/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3399 - acc: 0.8860 - val_loss: 1.3334 - val_acc: 0.7250\n",
      "Epoch 3767/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3042 - acc: 0.8891 - val_loss: 1.6032 - val_acc: 0.6973\n",
      "Epoch 3768/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3200 - acc: 0.8850 - val_loss: 1.4489 - val_acc: 0.6955\n",
      "Epoch 3769/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3355 - acc: 0.8812 - val_loss: 1.7574 - val_acc: 0.6321\n",
      "Epoch 3770/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3251 - acc: 0.8850 - val_loss: 1.4365 - val_acc: 0.6920\n",
      "Epoch 3771/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2979 - acc: 0.8919 - val_loss: 1.6668 - val_acc: 0.6973\n",
      "Epoch 3772/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3133 - acc: 0.8866 - val_loss: 1.5923 - val_acc: 0.6911\n",
      "Epoch 3773/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3349 - acc: 0.8819 - val_loss: 1.5031 - val_acc: 0.6848\n",
      "Epoch 3774/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3338 - acc: 0.8767 - val_loss: 1.3686 - val_acc: 0.7125\n",
      "Epoch 3775/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3062 - acc: 0.8905 - val_loss: 1.6135 - val_acc: 0.6750\n",
      "Epoch 3776/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3345 - acc: 0.8811 - val_loss: 1.5230 - val_acc: 0.6830\n",
      "Epoch 3777/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3113 - acc: 0.8879 - val_loss: 1.7263 - val_acc: 0.6393\n",
      "Epoch 3778/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3181 - acc: 0.8860 - val_loss: 1.5699 - val_acc: 0.7027\n",
      "Epoch 3779/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3108 - acc: 0.8875 - val_loss: 1.9167 - val_acc: 0.6205\n",
      "Epoch 3780/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3289 - acc: 0.8814 - val_loss: 1.6777 - val_acc: 0.6866\n",
      "Epoch 3781/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3142 - acc: 0.8867 - val_loss: 1.3428 - val_acc: 0.7304\n",
      "Epoch 3782/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3044 - acc: 0.8906 - val_loss: 1.9943 - val_acc: 0.6491\n",
      "Epoch 3783/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3231 - acc: 0.8848 - val_loss: 1.3618 - val_acc: 0.7152\n",
      "Epoch 3784/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3095 - acc: 0.8890 - val_loss: 1.5871 - val_acc: 0.6714\n",
      "Epoch 3785/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3019 - acc: 0.8888 - val_loss: 1.5542 - val_acc: 0.6902\n",
      "Epoch 3786/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3177 - acc: 0.8867 - val_loss: 1.6026 - val_acc: 0.6482\n",
      "Epoch 3787/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3309 - acc: 0.8807 - val_loss: 1.5935 - val_acc: 0.6768\n",
      "Epoch 3788/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3128 - acc: 0.8847 - val_loss: 1.7061 - val_acc: 0.6384\n",
      "Epoch 3789/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3226 - acc: 0.8869 - val_loss: 1.7402 - val_acc: 0.6750\n",
      "Epoch 3790/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3097 - acc: 0.8869 - val_loss: 1.7326 - val_acc: 0.6777\n",
      "Epoch 3791/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3160 - acc: 0.8863 - val_loss: 1.5428 - val_acc: 0.6750\n",
      "Epoch 3792/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3294 - acc: 0.8788 - val_loss: 1.7606 - val_acc: 0.6598\n",
      "Epoch 3793/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3255 - acc: 0.8799 - val_loss: 1.4563 - val_acc: 0.7214\n",
      "Epoch 3794/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2995 - acc: 0.8895 - val_loss: 1.6452 - val_acc: 0.6607\n",
      "Epoch 3795/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3425 - acc: 0.8779 - val_loss: 1.5610 - val_acc: 0.6795\n",
      "Epoch 3796/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3290 - acc: 0.8814 - val_loss: 1.5246 - val_acc: 0.6839\n",
      "Epoch 3797/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3099 - acc: 0.8913 - val_loss: 1.7672 - val_acc: 0.6920\n",
      "Epoch 3798/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3098 - acc: 0.8890 - val_loss: 1.4678 - val_acc: 0.6911\n",
      "Epoch 3799/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3240 - acc: 0.8827 - val_loss: 1.3180 - val_acc: 0.7223\n",
      "Epoch 3800/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3079 - acc: 0.8885 - val_loss: 1.6502 - val_acc: 0.6759\n",
      "Epoch 3801/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3223 - acc: 0.8856 - val_loss: 1.6386 - val_acc: 0.6982\n",
      "Epoch 3802/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3041 - acc: 0.8894 - val_loss: 1.7301 - val_acc: 0.6661\n",
      "Epoch 3803/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3156 - acc: 0.8868 - val_loss: 1.8070 - val_acc: 0.6732\n",
      "Epoch 3804/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3526 - acc: 0.8799 - val_loss: 1.7408 - val_acc: 0.6893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3805/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3412 - acc: 0.8823 - val_loss: 1.5245 - val_acc: 0.6938\n",
      "Epoch 3806/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3004 - acc: 0.8927 - val_loss: 1.9897 - val_acc: 0.6366\n",
      "Epoch 3807/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3319 - acc: 0.8815 - val_loss: 1.3312 - val_acc: 0.7116\n",
      "Epoch 3808/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3085 - acc: 0.8886 - val_loss: 1.7077 - val_acc: 0.6902\n",
      "Epoch 3809/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3216 - acc: 0.8833 - val_loss: 1.3776 - val_acc: 0.7098\n",
      "Epoch 3810/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3050 - acc: 0.8894 - val_loss: 1.4787 - val_acc: 0.6911\n",
      "Epoch 3811/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3230 - acc: 0.8848 - val_loss: 2.1110 - val_acc: 0.6384\n",
      "Epoch 3812/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3229 - acc: 0.8870 - val_loss: 2.1288 - val_acc: 0.6196\n",
      "Epoch 3813/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3305 - acc: 0.8806 - val_loss: 1.3560 - val_acc: 0.7250\n",
      "Epoch 3814/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3064 - acc: 0.8901 - val_loss: 2.0927 - val_acc: 0.6661\n",
      "Epoch 3815/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3313 - acc: 0.8788 - val_loss: 1.4115 - val_acc: 0.7107\n",
      "Epoch 3816/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3021 - acc: 0.8897 - val_loss: 1.4659 - val_acc: 0.6759\n",
      "Epoch 3817/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3157 - acc: 0.8839 - val_loss: 1.7169 - val_acc: 0.6982\n",
      "Epoch 3818/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3172 - acc: 0.8847 - val_loss: 1.4311 - val_acc: 0.7107\n",
      "Epoch 3819/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3093 - acc: 0.8887 - val_loss: 1.7891 - val_acc: 0.6268\n",
      "Epoch 3820/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3314 - acc: 0.8857 - val_loss: 1.5922 - val_acc: 0.7054\n",
      "Epoch 3821/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3223 - acc: 0.8853 - val_loss: 1.8669 - val_acc: 0.6455\n",
      "Epoch 3822/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3223 - acc: 0.8849 - val_loss: 1.5176 - val_acc: 0.6723\n",
      "Epoch 3823/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3188 - acc: 0.8844 - val_loss: 1.4170 - val_acc: 0.6714\n",
      "Epoch 3824/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3035 - acc: 0.8893 - val_loss: 1.5025 - val_acc: 0.6973\n",
      "Epoch 3825/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3110 - acc: 0.8891 - val_loss: 1.4739 - val_acc: 0.7241\n",
      "Epoch 3826/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3219 - acc: 0.8861 - val_loss: 1.5447 - val_acc: 0.7241\n",
      "Epoch 3827/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3181 - acc: 0.8855 - val_loss: 1.4124 - val_acc: 0.7295\n",
      "Epoch 3828/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2899 - acc: 0.8951 - val_loss: 1.4176 - val_acc: 0.7045\n",
      "Epoch 3829/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2967 - acc: 0.8921 - val_loss: 1.4816 - val_acc: 0.6964\n",
      "Epoch 3830/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3128 - acc: 0.8864 - val_loss: 1.4940 - val_acc: 0.6875\n",
      "Epoch 3831/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2982 - acc: 0.8935 - val_loss: 1.7223 - val_acc: 0.6964\n",
      "Epoch 3832/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3404 - acc: 0.8809 - val_loss: 1.3838 - val_acc: 0.7196\n",
      "Epoch 3833/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2953 - acc: 0.8930 - val_loss: 2.0181 - val_acc: 0.6446\n",
      "Epoch 3834/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3314 - acc: 0.8855 - val_loss: 1.6149 - val_acc: 0.6929\n",
      "Epoch 3835/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3043 - acc: 0.8899 - val_loss: 1.7538 - val_acc: 0.6705\n",
      "Epoch 3836/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3115 - acc: 0.8889 - val_loss: 2.0579 - val_acc: 0.6509\n",
      "Epoch 3837/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3114 - acc: 0.8899 - val_loss: 1.9736 - val_acc: 0.6473\n",
      "Epoch 3838/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3386 - acc: 0.8817 - val_loss: 1.5892 - val_acc: 0.7018\n",
      "Epoch 3839/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3160 - acc: 0.8880 - val_loss: 1.4666 - val_acc: 0.6937\n",
      "Epoch 3840/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2962 - acc: 0.8943 - val_loss: 1.6552 - val_acc: 0.6795\n",
      "Epoch 3841/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3106 - acc: 0.8921 - val_loss: 1.8101 - val_acc: 0.6741\n",
      "Epoch 3842/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3163 - acc: 0.8837 - val_loss: 1.8437 - val_acc: 0.6411\n",
      "Epoch 3843/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3272 - acc: 0.8827 - val_loss: 1.8535 - val_acc: 0.6580\n",
      "Epoch 3844/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3220 - acc: 0.8867 - val_loss: 1.3812 - val_acc: 0.7259\n",
      "Epoch 3845/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3135 - acc: 0.8817 - val_loss: 1.4973 - val_acc: 0.6982\n",
      "Epoch 3846/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3090 - acc: 0.8879 - val_loss: 1.7137 - val_acc: 0.6937\n",
      "Epoch 3847/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3208 - acc: 0.8865 - val_loss: 1.7319 - val_acc: 0.6741\n",
      "Epoch 3848/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3234 - acc: 0.8852 - val_loss: 1.3925 - val_acc: 0.7304\n",
      "Epoch 3849/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3047 - acc: 0.8899 - val_loss: 1.7405 - val_acc: 0.6518\n",
      "Epoch 3850/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3163 - acc: 0.8851 - val_loss: 1.7109 - val_acc: 0.6795\n",
      "Epoch 3851/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3143 - acc: 0.8885 - val_loss: 1.5363 - val_acc: 0.6946\n",
      "Epoch 3852/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3123 - acc: 0.8844 - val_loss: 1.5859 - val_acc: 0.6857\n",
      "Epoch 3853/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3047 - acc: 0.8904 - val_loss: 1.8220 - val_acc: 0.6777\n",
      "Epoch 3854/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3113 - acc: 0.8871 - val_loss: 1.5792 - val_acc: 0.6598\n",
      "Epoch 3855/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3303 - acc: 0.8828 - val_loss: 2.1284 - val_acc: 0.6339\n",
      "Epoch 3856/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3188 - acc: 0.8861 - val_loss: 1.4338 - val_acc: 0.6982\n",
      "Epoch 3857/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3054 - acc: 0.8883 - val_loss: 2.0694 - val_acc: 0.6357\n",
      "Epoch 3858/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3166 - acc: 0.8892 - val_loss: 1.5818 - val_acc: 0.6813\n",
      "Epoch 3859/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3089 - acc: 0.8875 - val_loss: 1.5895 - val_acc: 0.6777\n",
      "Epoch 3860/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3124 - acc: 0.8896 - val_loss: 1.7409 - val_acc: 0.6830\n",
      "Epoch 3861/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3337 - acc: 0.8862 - val_loss: 1.4455 - val_acc: 0.7000\n",
      "Epoch 3862/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3130 - acc: 0.8895 - val_loss: 1.5000 - val_acc: 0.6875\n",
      "Epoch 3863/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3275 - acc: 0.8810 - val_loss: 1.4240 - val_acc: 0.7107\n",
      "Epoch 3864/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3109 - acc: 0.8857 - val_loss: 1.8151 - val_acc: 0.6679\n",
      "Epoch 3865/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3127 - acc: 0.8879 - val_loss: 1.4225 - val_acc: 0.7232\n",
      "Epoch 3866/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2947 - acc: 0.8949 - val_loss: 2.5377 - val_acc: 0.5804\n",
      "Epoch 3867/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3384 - acc: 0.8851 - val_loss: 1.6000 - val_acc: 0.6982\n",
      "Epoch 3868/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3079 - acc: 0.8877 - val_loss: 1.6806 - val_acc: 0.6911\n",
      "Epoch 3869/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3149 - acc: 0.8859 - val_loss: 1.9354 - val_acc: 0.6598\n",
      "Epoch 3870/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3239 - acc: 0.8853 - val_loss: 1.6548 - val_acc: 0.6929\n",
      "Epoch 3871/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3099 - acc: 0.8913 - val_loss: 1.5838 - val_acc: 0.6911\n",
      "Epoch 3872/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2976 - acc: 0.8951 - val_loss: 1.3770 - val_acc: 0.7027\n",
      "Epoch 3873/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3169 - acc: 0.8853 - val_loss: 1.4914 - val_acc: 0.6741\n",
      "Epoch 3874/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3179 - acc: 0.8877 - val_loss: 2.1312 - val_acc: 0.6286\n",
      "Epoch 3875/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3210 - acc: 0.8902 - val_loss: 2.2562 - val_acc: 0.6143\n",
      "Epoch 3876/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3230 - acc: 0.8862 - val_loss: 1.8475 - val_acc: 0.6804\n",
      "Epoch 3877/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2965 - acc: 0.8909 - val_loss: 1.4653 - val_acc: 0.7071\n",
      "Epoch 3878/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3033 - acc: 0.8884 - val_loss: 1.8697 - val_acc: 0.6482\n",
      "Epoch 3879/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3222 - acc: 0.8851 - val_loss: 1.4953 - val_acc: 0.7080\n",
      "Epoch 3880/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3008 - acc: 0.8895 - val_loss: 1.5201 - val_acc: 0.7080\n",
      "Epoch 3881/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3129 - acc: 0.8897 - val_loss: 1.6979 - val_acc: 0.6830\n",
      "Epoch 3882/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3256 - acc: 0.8846 - val_loss: 1.4211 - val_acc: 0.7125\n",
      "Epoch 3883/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2863 - acc: 0.8939 - val_loss: 1.4652 - val_acc: 0.7000\n",
      "Epoch 3884/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3070 - acc: 0.8894 - val_loss: 2.0662 - val_acc: 0.6491\n",
      "Epoch 3885/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3227 - acc: 0.8869 - val_loss: 1.3328 - val_acc: 0.7080\n",
      "Epoch 3886/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3023 - acc: 0.8894 - val_loss: 1.4164 - val_acc: 0.7134\n",
      "Epoch 3887/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2987 - acc: 0.8912 - val_loss: 1.4467 - val_acc: 0.6848\n",
      "Epoch 3888/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3268 - acc: 0.8837 - val_loss: 1.6265 - val_acc: 0.7018\n",
      "Epoch 3889/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3081 - acc: 0.8887 - val_loss: 1.6093 - val_acc: 0.6911\n",
      "Epoch 3890/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3177 - acc: 0.8860 - val_loss: 1.7677 - val_acc: 0.6679\n",
      "Epoch 3891/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3171 - acc: 0.8883 - val_loss: 1.4889 - val_acc: 0.6973\n",
      "Epoch 3892/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2970 - acc: 0.8914 - val_loss: 1.5557 - val_acc: 0.6973\n",
      "Epoch 3893/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3194 - acc: 0.8840 - val_loss: 1.5113 - val_acc: 0.6768\n",
      "Epoch 3894/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3137 - acc: 0.8856 - val_loss: 1.4131 - val_acc: 0.7125\n",
      "Epoch 3895/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2941 - acc: 0.8922 - val_loss: 2.1673 - val_acc: 0.6214\n",
      "Epoch 3896/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3391 - acc: 0.8843 - val_loss: 1.8541 - val_acc: 0.6562\n",
      "Epoch 3897/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3247 - acc: 0.8825 - val_loss: 1.8413 - val_acc: 0.6312\n",
      "Epoch 3898/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3272 - acc: 0.8865 - val_loss: 1.7252 - val_acc: 0.6741\n",
      "Epoch 3899/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3139 - acc: 0.8897 - val_loss: 1.8385 - val_acc: 0.6429\n",
      "Epoch 3900/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3141 - acc: 0.8890 - val_loss: 1.8434 - val_acc: 0.6857\n",
      "Epoch 3901/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3189 - acc: 0.8860 - val_loss: 2.0398 - val_acc: 0.6750\n",
      "Epoch 3902/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3163 - acc: 0.8869 - val_loss: 1.5907 - val_acc: 0.6839\n",
      "Epoch 3903/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3080 - acc: 0.8907 - val_loss: 1.3913 - val_acc: 0.7312\n",
      "Epoch 3904/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2990 - acc: 0.8917 - val_loss: 1.8605 - val_acc: 0.6839\n",
      "Epoch 3905/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3192 - acc: 0.8856 - val_loss: 1.3619 - val_acc: 0.7268\n",
      "Epoch 3906/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2958 - acc: 0.8925 - val_loss: 1.7148 - val_acc: 0.6991\n",
      "Epoch 3907/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3187 - acc: 0.8905 - val_loss: 2.0036 - val_acc: 0.6545\n",
      "Epoch 3908/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3281 - acc: 0.8835 - val_loss: 1.4806 - val_acc: 0.7000\n",
      "Epoch 3909/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3144 - acc: 0.8867 - val_loss: 1.6374 - val_acc: 0.6580\n",
      "Epoch 3910/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3053 - acc: 0.8891 - val_loss: 2.0205 - val_acc: 0.6652\n",
      "Epoch 3911/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3349 - acc: 0.8859 - val_loss: 1.9441 - val_acc: 0.6527\n",
      "Epoch 3912/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3392 - acc: 0.8855 - val_loss: 1.6463 - val_acc: 0.6821\n",
      "Epoch 3913/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3133 - acc: 0.8929 - val_loss: 1.9326 - val_acc: 0.6687\n",
      "Epoch 3914/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3335 - acc: 0.8864 - val_loss: 1.9345 - val_acc: 0.6652\n",
      "Epoch 3915/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3112 - acc: 0.8905 - val_loss: 1.5728 - val_acc: 0.7054\n",
      "Epoch 3916/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3083 - acc: 0.8872 - val_loss: 1.9385 - val_acc: 0.6509\n",
      "Epoch 3917/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3206 - acc: 0.8877 - val_loss: 1.3590 - val_acc: 0.7304\n",
      "Epoch 3918/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2886 - acc: 0.8941 - val_loss: 1.8815 - val_acc: 0.6768\n",
      "Epoch 3919/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3182 - acc: 0.8867 - val_loss: 1.7934 - val_acc: 0.6616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3920/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3214 - acc: 0.8879 - val_loss: 1.4618 - val_acc: 0.7232\n",
      "Epoch 3921/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3035 - acc: 0.8902 - val_loss: 1.5535 - val_acc: 0.6973\n",
      "Epoch 3922/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2995 - acc: 0.8905 - val_loss: 1.4674 - val_acc: 0.6973\n",
      "Epoch 3923/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3002 - acc: 0.8897 - val_loss: 1.5907 - val_acc: 0.6893\n",
      "Epoch 3924/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3025 - acc: 0.8913 - val_loss: 2.0388 - val_acc: 0.6464\n",
      "Epoch 3925/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3290 - acc: 0.8881 - val_loss: 1.4395 - val_acc: 0.7080\n",
      "Epoch 3926/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3125 - acc: 0.8868 - val_loss: 1.6398 - val_acc: 0.6768\n",
      "Epoch 3927/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3012 - acc: 0.8895 - val_loss: 1.6830 - val_acc: 0.6687\n",
      "Epoch 3928/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3040 - acc: 0.8895 - val_loss: 1.4622 - val_acc: 0.7018\n",
      "Epoch 3929/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3100 - acc: 0.8877 - val_loss: 1.8661 - val_acc: 0.6607\n",
      "Epoch 3930/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3416 - acc: 0.8827 - val_loss: 1.4663 - val_acc: 0.6893\n",
      "Epoch 3931/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3022 - acc: 0.8922 - val_loss: 1.5362 - val_acc: 0.7009\n",
      "Epoch 3932/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3026 - acc: 0.8895 - val_loss: 1.6927 - val_acc: 0.6946\n",
      "Epoch 3933/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3198 - acc: 0.8870 - val_loss: 1.8102 - val_acc: 0.6509\n",
      "Epoch 3934/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3052 - acc: 0.8887 - val_loss: 1.4038 - val_acc: 0.7232\n",
      "Epoch 3935/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3064 - acc: 0.8897 - val_loss: 1.6261 - val_acc: 0.6893\n",
      "Epoch 3936/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3195 - acc: 0.8826 - val_loss: 1.5147 - val_acc: 0.7071\n",
      "Epoch 3937/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2986 - acc: 0.8927 - val_loss: 1.2956 - val_acc: 0.7107\n",
      "Epoch 3938/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3001 - acc: 0.8905 - val_loss: 1.4969 - val_acc: 0.6991\n",
      "Epoch 3939/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3027 - acc: 0.8876 - val_loss: 1.9553 - val_acc: 0.6402\n",
      "Epoch 3940/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3532 - acc: 0.8805 - val_loss: 1.4296 - val_acc: 0.6902\n",
      "Epoch 3941/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2878 - acc: 0.8960 - val_loss: 1.6311 - val_acc: 0.6866\n",
      "Epoch 3942/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3103 - acc: 0.8901 - val_loss: 1.5636 - val_acc: 0.7018\n",
      "Epoch 3943/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3007 - acc: 0.8892 - val_loss: 1.8563 - val_acc: 0.6616\n",
      "Epoch 3944/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3181 - acc: 0.8888 - val_loss: 1.6949 - val_acc: 0.6893\n",
      "Epoch 3945/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2935 - acc: 0.8936 - val_loss: 1.6003 - val_acc: 0.6839\n",
      "Epoch 3946/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3120 - acc: 0.8921 - val_loss: 1.7926 - val_acc: 0.6643\n",
      "Epoch 3947/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3347 - acc: 0.8821 - val_loss: 1.4391 - val_acc: 0.7250\n",
      "Epoch 3948/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3022 - acc: 0.8927 - val_loss: 1.4205 - val_acc: 0.6759\n",
      "Epoch 3949/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3125 - acc: 0.8880 - val_loss: 1.7815 - val_acc: 0.6348\n",
      "Epoch 3950/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3251 - acc: 0.8845 - val_loss: 1.9950 - val_acc: 0.6045\n",
      "Epoch 3951/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3218 - acc: 0.8854 - val_loss: 1.9687 - val_acc: 0.6634\n",
      "Epoch 3952/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3104 - acc: 0.8906 - val_loss: 1.7530 - val_acc: 0.6929\n",
      "Epoch 3953/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2938 - acc: 0.8965 - val_loss: 1.8328 - val_acc: 0.6848\n",
      "Epoch 3954/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3118 - acc: 0.8909 - val_loss: 1.4858 - val_acc: 0.6973\n",
      "Epoch 3955/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3071 - acc: 0.8887 - val_loss: 1.8800 - val_acc: 0.6438\n",
      "Epoch 3956/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3229 - acc: 0.8839 - val_loss: 1.4678 - val_acc: 0.7054\n",
      "Epoch 3957/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3099 - acc: 0.8893 - val_loss: 1.5779 - val_acc: 0.6741\n",
      "Epoch 3958/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8880 - val_loss: 1.7641 - val_acc: 0.6616\n",
      "Epoch 3959/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3117 - acc: 0.8861 - val_loss: 2.2023 - val_acc: 0.6277\n",
      "Epoch 3960/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3496 - acc: 0.8798 - val_loss: 1.4890 - val_acc: 0.6696\n",
      "Epoch 3961/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3211 - acc: 0.8865 - val_loss: 1.3510 - val_acc: 0.7000\n",
      "Epoch 3962/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3021 - acc: 0.8914 - val_loss: 1.4849 - val_acc: 0.6964\n",
      "Epoch 3963/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3030 - acc: 0.8909 - val_loss: 1.9176 - val_acc: 0.6438\n",
      "Epoch 3964/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3350 - acc: 0.8856 - val_loss: 1.8293 - val_acc: 0.6607\n",
      "Epoch 3965/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3162 - acc: 0.8876 - val_loss: 2.3328 - val_acc: 0.5937\n",
      "Epoch 3966/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3252 - acc: 0.8868 - val_loss: 1.9056 - val_acc: 0.6482\n",
      "Epoch 3967/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3050 - acc: 0.8912 - val_loss: 1.6247 - val_acc: 0.6848\n",
      "Epoch 3968/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3181 - acc: 0.8885 - val_loss: 1.8966 - val_acc: 0.6571\n",
      "Epoch 3969/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3153 - acc: 0.8899 - val_loss: 1.9102 - val_acc: 0.6411\n",
      "Epoch 3970/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3184 - acc: 0.8874 - val_loss: 1.4632 - val_acc: 0.7170\n",
      "Epoch 3971/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3059 - acc: 0.8897 - val_loss: 1.8395 - val_acc: 0.6750\n",
      "Epoch 3972/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3071 - acc: 0.8913 - val_loss: 1.5427 - val_acc: 0.6946\n",
      "Epoch 3973/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3089 - acc: 0.8869 - val_loss: 1.8412 - val_acc: 0.6696\n",
      "Epoch 3974/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3077 - acc: 0.8912 - val_loss: 1.5231 - val_acc: 0.6964\n",
      "Epoch 3975/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3039 - acc: 0.8895 - val_loss: 1.4598 - val_acc: 0.6571\n",
      "Epoch 3976/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3485 - acc: 0.8779 - val_loss: 1.5021 - val_acc: 0.6911\n",
      "Epoch 3977/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3071 - acc: 0.8899 - val_loss: 1.5861 - val_acc: 0.6848\n",
      "Epoch 3978/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3240 - acc: 0.8814 - val_loss: 1.7605 - val_acc: 0.6652\n",
      "Epoch 3979/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3199 - acc: 0.8864 - val_loss: 1.7745 - val_acc: 0.6491\n",
      "Epoch 3980/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3158 - acc: 0.8862 - val_loss: 1.8535 - val_acc: 0.6536\n",
      "Epoch 3981/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3217 - acc: 0.8881 - val_loss: 1.5399 - val_acc: 0.6679\n",
      "Epoch 3982/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3330 - acc: 0.8857 - val_loss: 2.2701 - val_acc: 0.6250\n",
      "Epoch 3983/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3236 - acc: 0.8873 - val_loss: 1.6777 - val_acc: 0.7018\n",
      "Epoch 3984/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3236 - acc: 0.8868 - val_loss: 1.7046 - val_acc: 0.6875\n",
      "Epoch 3985/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3013 - acc: 0.8927 - val_loss: 1.5863 - val_acc: 0.6991\n",
      "Epoch 3986/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3028 - acc: 0.8885 - val_loss: 1.6787 - val_acc: 0.6786\n",
      "Epoch 3987/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3110 - acc: 0.8902 - val_loss: 1.7361 - val_acc: 0.6750\n",
      "Epoch 3988/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3011 - acc: 0.8953 - val_loss: 1.5765 - val_acc: 0.6741\n",
      "Epoch 3989/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3029 - acc: 0.8895 - val_loss: 1.9278 - val_acc: 0.6554\n",
      "Epoch 3990/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3122 - acc: 0.8905 - val_loss: 1.5644 - val_acc: 0.6866\n",
      "Epoch 3991/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3069 - acc: 0.8886 - val_loss: 1.3666 - val_acc: 0.7304\n",
      "Epoch 3992/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3004 - acc: 0.8918 - val_loss: 1.4526 - val_acc: 0.6902\n",
      "Epoch 3993/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3205 - acc: 0.8859 - val_loss: 1.5874 - val_acc: 0.6911\n",
      "Epoch 3994/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3172 - acc: 0.8853 - val_loss: 1.5511 - val_acc: 0.7134\n",
      "Epoch 3995/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2984 - acc: 0.8943 - val_loss: 1.7217 - val_acc: 0.6687\n",
      "Epoch 3996/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3104 - acc: 0.8911 - val_loss: 2.0921 - val_acc: 0.6259\n",
      "Epoch 3997/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3337 - acc: 0.8873 - val_loss: 3.4064 - val_acc: 0.5473\n",
      "Epoch 3998/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3665 - acc: 0.8879 - val_loss: 1.6307 - val_acc: 0.6866\n",
      "Epoch 3999/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3009 - acc: 0.8919 - val_loss: 1.6812 - val_acc: 0.6687\n",
      "Epoch 4000/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3291 - acc: 0.8853 - val_loss: 1.7042 - val_acc: 0.6955\n",
      "Epoch 4001/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3091 - acc: 0.8885 - val_loss: 1.5459 - val_acc: 0.6884\n",
      "Epoch 4002/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2964 - acc: 0.8961 - val_loss: 1.5685 - val_acc: 0.6982\n",
      "Epoch 4003/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2923 - acc: 0.8930 - val_loss: 1.6591 - val_acc: 0.7054\n",
      "Epoch 4004/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3105 - acc: 0.8890 - val_loss: 1.5425 - val_acc: 0.7098\n",
      "Epoch 4005/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3119 - acc: 0.8901 - val_loss: 1.6160 - val_acc: 0.7143\n",
      "Epoch 4006/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3009 - acc: 0.8921 - val_loss: 1.9362 - val_acc: 0.6312\n",
      "Epoch 4007/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3157 - acc: 0.8851 - val_loss: 1.8591 - val_acc: 0.6902\n",
      "Epoch 4008/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3029 - acc: 0.8925 - val_loss: 1.7663 - val_acc: 0.6812\n",
      "Epoch 4009/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3537 - acc: 0.8770 - val_loss: 1.6870 - val_acc: 0.6857\n",
      "Epoch 4010/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2840 - acc: 0.8995 - val_loss: 2.0910 - val_acc: 0.6321\n",
      "Epoch 4011/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3333 - acc: 0.8849 - val_loss: 1.4893 - val_acc: 0.7116\n",
      "Epoch 4012/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3031 - acc: 0.8885 - val_loss: 1.5410 - val_acc: 0.6866\n",
      "Epoch 4013/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2950 - acc: 0.8923 - val_loss: 1.4395 - val_acc: 0.6911\n",
      "Epoch 4014/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2981 - acc: 0.8896 - val_loss: 2.0121 - val_acc: 0.6536\n",
      "Epoch 4015/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3349 - acc: 0.8827 - val_loss: 1.6435 - val_acc: 0.7089\n",
      "Epoch 4016/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3150 - acc: 0.8875 - val_loss: 1.8221 - val_acc: 0.6920\n",
      "Epoch 4017/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3299 - acc: 0.8835 - val_loss: 1.6756 - val_acc: 0.7009\n",
      "Epoch 4018/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3006 - acc: 0.8911 - val_loss: 1.6868 - val_acc: 0.6920\n",
      "Epoch 4019/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3499 - acc: 0.8803 - val_loss: 1.4756 - val_acc: 0.7116\n",
      "Epoch 4020/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3075 - acc: 0.8888 - val_loss: 1.5317 - val_acc: 0.6973\n",
      "Epoch 4021/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3148 - acc: 0.8852 - val_loss: 1.9291 - val_acc: 0.6616\n",
      "Epoch 4022/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3340 - acc: 0.8840 - val_loss: 1.4012 - val_acc: 0.7348\n",
      "Epoch 4023/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2847 - acc: 0.8965 - val_loss: 1.9616 - val_acc: 0.6420\n",
      "Epoch 4024/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3357 - acc: 0.8851 - val_loss: 1.8069 - val_acc: 0.6482\n",
      "Epoch 4025/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3040 - acc: 0.8894 - val_loss: 1.4164 - val_acc: 0.6929\n",
      "Epoch 4026/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2969 - acc: 0.8911 - val_loss: 1.8393 - val_acc: 0.6777\n",
      "Epoch 4027/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3017 - acc: 0.8932 - val_loss: 1.5661 - val_acc: 0.6982\n",
      "Epoch 4028/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3081 - acc: 0.8887 - val_loss: 1.4985 - val_acc: 0.7152\n",
      "Epoch 4029/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2970 - acc: 0.8917 - val_loss: 1.6274 - val_acc: 0.6946\n",
      "Epoch 4030/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3033 - acc: 0.8937 - val_loss: 1.7740 - val_acc: 0.6750\n",
      "Epoch 4031/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3069 - acc: 0.8916 - val_loss: 1.6664 - val_acc: 0.6696\n",
      "Epoch 4032/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3160 - acc: 0.8887 - val_loss: 1.4093 - val_acc: 0.6830\n",
      "Epoch 4033/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3064 - acc: 0.8896 - val_loss: 1.8332 - val_acc: 0.6893\n",
      "Epoch 4034/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3022 - acc: 0.8924 - val_loss: 1.3208 - val_acc: 0.7161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4035/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3058 - acc: 0.8926 - val_loss: 2.2085 - val_acc: 0.6286\n",
      "Epoch 4036/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3231 - acc: 0.8889 - val_loss: 1.4755 - val_acc: 0.6991\n",
      "Epoch 4037/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2963 - acc: 0.8909 - val_loss: 1.4601 - val_acc: 0.7134\n",
      "Epoch 4038/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2992 - acc: 0.8924 - val_loss: 1.7615 - val_acc: 0.6759\n",
      "Epoch 4039/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3109 - acc: 0.8875 - val_loss: 1.5800 - val_acc: 0.6786\n",
      "Epoch 4040/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3124 - acc: 0.8876 - val_loss: 1.7012 - val_acc: 0.6491\n",
      "Epoch 4041/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3045 - acc: 0.8940 - val_loss: 1.6473 - val_acc: 0.7018\n",
      "Epoch 4042/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2969 - acc: 0.8959 - val_loss: 1.4479 - val_acc: 0.7232\n",
      "Epoch 4043/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3071 - acc: 0.8877 - val_loss: 1.3844 - val_acc: 0.7134\n",
      "Epoch 4044/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2881 - acc: 0.8945 - val_loss: 1.8196 - val_acc: 0.6688\n",
      "Epoch 4045/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3183 - acc: 0.8879 - val_loss: 1.5158 - val_acc: 0.6973\n",
      "Epoch 4046/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3080 - acc: 0.8891 - val_loss: 1.8018 - val_acc: 0.6804\n",
      "Epoch 4047/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3237 - acc: 0.8859 - val_loss: 2.0387 - val_acc: 0.6179\n",
      "Epoch 4048/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3157 - acc: 0.8897 - val_loss: 1.6431 - val_acc: 0.6911\n",
      "Epoch 4049/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3277 - acc: 0.8827 - val_loss: 1.5066 - val_acc: 0.6884\n",
      "Epoch 4050/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2910 - acc: 0.8974 - val_loss: 2.1009 - val_acc: 0.6571\n",
      "Epoch 4051/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3255 - acc: 0.8881 - val_loss: 1.4088 - val_acc: 0.7045\n",
      "Epoch 4052/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3086 - acc: 0.8894 - val_loss: 1.7567 - val_acc: 0.6366\n",
      "Epoch 4053/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2951 - acc: 0.8907 - val_loss: 1.7953 - val_acc: 0.6634\n",
      "Epoch 4054/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3099 - acc: 0.8903 - val_loss: 1.9995 - val_acc: 0.6429\n",
      "Epoch 4055/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3462 - acc: 0.8830 - val_loss: 1.6939 - val_acc: 0.6786\n",
      "Epoch 4056/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3040 - acc: 0.8923 - val_loss: 1.5794 - val_acc: 0.7027\n",
      "Epoch 4057/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2943 - acc: 0.8929 - val_loss: 1.3622 - val_acc: 0.6804\n",
      "Epoch 4058/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3074 - acc: 0.8889 - val_loss: 1.4834 - val_acc: 0.6982\n",
      "Epoch 4059/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2916 - acc: 0.8945 - val_loss: 1.7735 - val_acc: 0.6679\n",
      "Epoch 4060/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3158 - acc: 0.8898 - val_loss: 1.9314 - val_acc: 0.6473\n",
      "Epoch 4061/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3243 - acc: 0.8863 - val_loss: 2.1732 - val_acc: 0.6071\n",
      "Epoch 4062/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3284 - acc: 0.8865 - val_loss: 1.4194 - val_acc: 0.7134\n",
      "Epoch 4063/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2933 - acc: 0.8931 - val_loss: 1.5671 - val_acc: 0.6937\n",
      "Epoch 4064/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3043 - acc: 0.8911 - val_loss: 1.6821 - val_acc: 0.6893\n",
      "Epoch 4065/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3076 - acc: 0.8879 - val_loss: 1.4591 - val_acc: 0.7232\n",
      "Epoch 4066/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2947 - acc: 0.8926 - val_loss: 1.7089 - val_acc: 0.6848\n",
      "Epoch 4067/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3033 - acc: 0.8907 - val_loss: 1.5425 - val_acc: 0.6982\n",
      "Epoch 4068/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3042 - acc: 0.8899 - val_loss: 1.7350 - val_acc: 0.6563\n",
      "Epoch 4069/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3168 - acc: 0.8897 - val_loss: 1.9142 - val_acc: 0.6634\n",
      "Epoch 4070/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3056 - acc: 0.8941 - val_loss: 1.6971 - val_acc: 0.6848\n",
      "Epoch 4071/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3066 - acc: 0.8891 - val_loss: 1.5870 - val_acc: 0.6464\n",
      "Epoch 4072/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3064 - acc: 0.8889 - val_loss: 1.6272 - val_acc: 0.6920\n",
      "Epoch 4073/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2966 - acc: 0.8944 - val_loss: 1.4504 - val_acc: 0.7214\n",
      "Epoch 4074/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2885 - acc: 0.8939 - val_loss: 1.6810 - val_acc: 0.6813\n",
      "Epoch 4075/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3101 - acc: 0.8903 - val_loss: 1.7537 - val_acc: 0.6607\n",
      "Epoch 4076/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2994 - acc: 0.8929 - val_loss: 1.9911 - val_acc: 0.6491\n",
      "Epoch 4077/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3141 - acc: 0.8889 - val_loss: 1.5586 - val_acc: 0.7152\n",
      "Epoch 4078/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2912 - acc: 0.8913 - val_loss: 1.7100 - val_acc: 0.6938\n",
      "Epoch 4079/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3017 - acc: 0.8906 - val_loss: 1.4390 - val_acc: 0.7277\n",
      "Epoch 4080/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3134 - acc: 0.8871 - val_loss: 1.5952 - val_acc: 0.6812\n",
      "Epoch 4081/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2901 - acc: 0.8946 - val_loss: 1.7634 - val_acc: 0.6705\n",
      "Epoch 4082/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3175 - acc: 0.8871 - val_loss: 1.5104 - val_acc: 0.7071\n",
      "Epoch 4083/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2937 - acc: 0.8925 - val_loss: 1.5948 - val_acc: 0.6866\n",
      "Epoch 4084/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3035 - acc: 0.8897 - val_loss: 1.7709 - val_acc: 0.6732\n",
      "Epoch 4085/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3049 - acc: 0.8931 - val_loss: 1.4340 - val_acc: 0.7161\n",
      "Epoch 4086/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3047 - acc: 0.8859 - val_loss: 1.5833 - val_acc: 0.6821\n",
      "Epoch 4087/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3003 - acc: 0.8925 - val_loss: 1.6085 - val_acc: 0.6911\n",
      "Epoch 4088/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3084 - acc: 0.8887 - val_loss: 1.6143 - val_acc: 0.6821\n",
      "Epoch 4089/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2891 - acc: 0.8934 - val_loss: 2.2310 - val_acc: 0.6268\n",
      "Epoch 4090/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3370 - acc: 0.8875 - val_loss: 1.7123 - val_acc: 0.6786\n",
      "Epoch 4091/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3081 - acc: 0.8931 - val_loss: 1.7647 - val_acc: 0.6804\n",
      "Epoch 4092/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3091 - acc: 0.8887 - val_loss: 1.6081 - val_acc: 0.6982\n",
      "Epoch 4093/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2974 - acc: 0.8960 - val_loss: 1.4658 - val_acc: 0.7188\n",
      "Epoch 4094/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3025 - acc: 0.8931 - val_loss: 1.6803 - val_acc: 0.6839\n",
      "Epoch 4095/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2963 - acc: 0.8912 - val_loss: 2.0827 - val_acc: 0.6384\n",
      "Epoch 4096/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3199 - acc: 0.8900 - val_loss: 1.4533 - val_acc: 0.7107\n",
      "Epoch 4097/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3088 - acc: 0.8873 - val_loss: 1.4420 - val_acc: 0.7063\n",
      "Epoch 4098/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2852 - acc: 0.8947 - val_loss: 1.4777 - val_acc: 0.7018\n",
      "Epoch 4099/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3077 - acc: 0.8920 - val_loss: 1.4431 - val_acc: 0.7036\n",
      "Epoch 4100/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2887 - acc: 0.8937 - val_loss: 1.8928 - val_acc: 0.6634\n",
      "Epoch 4101/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3153 - acc: 0.8910 - val_loss: 1.7687 - val_acc: 0.6938\n",
      "Epoch 4102/5000\n",
      "15008/15008 [==============================] - 1s 82us/step - loss: 0.3107 - acc: 0.8920 - val_loss: 2.1240 - val_acc: 0.6304\n",
      "Epoch 4103/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3123 - acc: 0.8903 - val_loss: 2.1664 - val_acc: 0.6214\n",
      "Epoch 4104/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3355 - acc: 0.8913 - val_loss: 1.7032 - val_acc: 0.6741\n",
      "Epoch 4105/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3118 - acc: 0.8922 - val_loss: 1.5687 - val_acc: 0.6866\n",
      "Epoch 4106/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2985 - acc: 0.8945 - val_loss: 1.6180 - val_acc: 0.7098\n",
      "Epoch 4107/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2864 - acc: 0.8941 - val_loss: 1.6470 - val_acc: 0.6929\n",
      "Epoch 4108/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3151 - acc: 0.8891 - val_loss: 1.4825 - val_acc: 0.7054\n",
      "Epoch 4109/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2891 - acc: 0.8975 - val_loss: 1.6353 - val_acc: 0.6634\n",
      "Epoch 4110/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 1.3962 - val_acc: 0.7027\n",
      "Epoch 4111/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2996 - acc: 0.8923 - val_loss: 2.0851 - val_acc: 0.6393\n",
      "Epoch 4112/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3099 - acc: 0.8899 - val_loss: 1.9409 - val_acc: 0.6429\n",
      "Epoch 4113/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3198 - acc: 0.8879 - val_loss: 1.8070 - val_acc: 0.6795\n",
      "Epoch 4114/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3086 - acc: 0.8859 - val_loss: 1.7821 - val_acc: 0.7000\n",
      "Epoch 4115/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3189 - acc: 0.8861 - val_loss: 1.3748 - val_acc: 0.7045\n",
      "Epoch 4116/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2938 - acc: 0.8925 - val_loss: 1.5687 - val_acc: 0.7054\n",
      "Epoch 4117/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3128 - acc: 0.8859 - val_loss: 1.6561 - val_acc: 0.6670\n",
      "Epoch 4118/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3219 - acc: 0.8855 - val_loss: 1.5110 - val_acc: 0.6955\n",
      "Epoch 4119/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2969 - acc: 0.8935 - val_loss: 1.9293 - val_acc: 0.6804\n",
      "Epoch 4120/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2982 - acc: 0.8916 - val_loss: 1.5159 - val_acc: 0.7152\n",
      "Epoch 4121/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2980 - acc: 0.8927 - val_loss: 2.0992 - val_acc: 0.6223\n",
      "Epoch 4122/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3277 - acc: 0.8886 - val_loss: 1.9210 - val_acc: 0.6661\n",
      "Epoch 4123/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2967 - acc: 0.8940 - val_loss: 1.7794 - val_acc: 0.6991\n",
      "Epoch 4124/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3095 - acc: 0.8910 - val_loss: 1.8074 - val_acc: 0.6643\n",
      "Epoch 4125/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2892 - acc: 0.8976 - val_loss: 2.0185 - val_acc: 0.6518\n",
      "Epoch 4126/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3272 - acc: 0.8884 - val_loss: 1.6053 - val_acc: 0.6893\n",
      "Epoch 4127/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3066 - acc: 0.8899 - val_loss: 1.3994 - val_acc: 0.7089\n",
      "Epoch 4128/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2956 - acc: 0.8897 - val_loss: 2.0133 - val_acc: 0.6312\n",
      "Epoch 4129/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2881 - acc: 0.8963 - val_loss: 1.5634 - val_acc: 0.7062\n",
      "Epoch 4130/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3055 - acc: 0.8925 - val_loss: 1.8118 - val_acc: 0.6804\n",
      "Epoch 4131/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3249 - acc: 0.8902 - val_loss: 2.0907 - val_acc: 0.6509\n",
      "Epoch 4132/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3068 - acc: 0.8964 - val_loss: 1.4534 - val_acc: 0.7205\n",
      "Epoch 4133/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2827 - acc: 0.8987 - val_loss: 1.7871 - val_acc: 0.6670\n",
      "Epoch 4134/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2996 - acc: 0.8938 - val_loss: 1.5541 - val_acc: 0.6687\n",
      "Epoch 4135/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3172 - acc: 0.8887 - val_loss: 1.5741 - val_acc: 0.7027\n",
      "Epoch 4136/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2941 - acc: 0.8975 - val_loss: 1.5413 - val_acc: 0.7250\n",
      "Epoch 4137/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2929 - acc: 0.8945 - val_loss: 2.0512 - val_acc: 0.6536\n",
      "Epoch 4138/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3164 - acc: 0.8885 - val_loss: 1.4554 - val_acc: 0.7045\n",
      "Epoch 4139/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3002 - acc: 0.8924 - val_loss: 1.2934 - val_acc: 0.7098\n",
      "Epoch 4140/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2904 - acc: 0.8957 - val_loss: 1.9040 - val_acc: 0.6313\n",
      "Epoch 4141/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3162 - acc: 0.8921 - val_loss: 1.5196 - val_acc: 0.7125\n",
      "Epoch 4142/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3045 - acc: 0.8924 - val_loss: 1.5729 - val_acc: 0.7179\n",
      "Epoch 4143/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2842 - acc: 0.8967 - val_loss: 1.6569 - val_acc: 0.7089\n",
      "Epoch 4144/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3040 - acc: 0.8915 - val_loss: 1.8426 - val_acc: 0.6670\n",
      "Epoch 4145/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3003 - acc: 0.8913 - val_loss: 1.4229 - val_acc: 0.7232\n",
      "Epoch 4146/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2901 - acc: 0.8958 - val_loss: 1.5597 - val_acc: 0.7179\n",
      "Epoch 4147/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3050 - acc: 0.8903 - val_loss: 2.0435 - val_acc: 0.6330\n",
      "Epoch 4148/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3144 - acc: 0.8911 - val_loss: 1.9984 - val_acc: 0.6607\n",
      "Epoch 4149/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3177 - acc: 0.8901 - val_loss: 1.5516 - val_acc: 0.7009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4150/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2851 - acc: 0.8986 - val_loss: 1.6953 - val_acc: 0.6804\n",
      "Epoch 4151/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2774 - acc: 0.8994 - val_loss: 2.3575 - val_acc: 0.6170\n",
      "Epoch 4152/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3392 - acc: 0.8875 - val_loss: 1.5333 - val_acc: 0.6982\n",
      "Epoch 4153/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2981 - acc: 0.8909 - val_loss: 1.6422 - val_acc: 0.6982\n",
      "Epoch 4154/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3001 - acc: 0.8909 - val_loss: 1.6466 - val_acc: 0.6938\n",
      "Epoch 4155/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3058 - acc: 0.8927 - val_loss: 1.9433 - val_acc: 0.6723\n",
      "Epoch 4156/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3147 - acc: 0.8901 - val_loss: 1.8107 - val_acc: 0.6321\n",
      "Epoch 4157/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3001 - acc: 0.8913 - val_loss: 1.5952 - val_acc: 0.6759\n",
      "Epoch 4158/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3104 - acc: 0.8915 - val_loss: 1.5526 - val_acc: 0.7098\n",
      "Epoch 4159/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2858 - acc: 0.8954 - val_loss: 1.5650 - val_acc: 0.6634\n",
      "Epoch 4160/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3227 - acc: 0.8843 - val_loss: 2.0874 - val_acc: 0.6116\n",
      "Epoch 4161/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3075 - acc: 0.8911 - val_loss: 1.5160 - val_acc: 0.7000\n",
      "Epoch 4162/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2881 - acc: 0.8959 - val_loss: 1.4166 - val_acc: 0.7152\n",
      "Epoch 4163/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2850 - acc: 0.8971 - val_loss: 1.7288 - val_acc: 0.6518\n",
      "Epoch 4164/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3066 - acc: 0.8914 - val_loss: 1.3842 - val_acc: 0.7161\n",
      "Epoch 4165/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2913 - acc: 0.8944 - val_loss: 1.9373 - val_acc: 0.6750\n",
      "Epoch 4166/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3053 - acc: 0.8948 - val_loss: 1.7246 - val_acc: 0.7071\n",
      "Epoch 4167/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2949 - acc: 0.8939 - val_loss: 1.6016 - val_acc: 0.7259\n",
      "Epoch 4168/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3129 - acc: 0.8861 - val_loss: 1.4952 - val_acc: 0.7107\n",
      "Epoch 4169/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2824 - acc: 0.8981 - val_loss: 1.6100 - val_acc: 0.6741\n",
      "Epoch 4170/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3045 - acc: 0.8889 - val_loss: 1.5349 - val_acc: 0.7134\n",
      "Epoch 4171/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2988 - acc: 0.8918 - val_loss: 1.3839 - val_acc: 0.7187\n",
      "Epoch 4172/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2835 - acc: 0.8970 - val_loss: 1.9701 - val_acc: 0.6134\n",
      "Epoch 4173/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3380 - acc: 0.8854 - val_loss: 1.5504 - val_acc: 0.7286\n",
      "Epoch 4174/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2854 - acc: 0.8953 - val_loss: 1.6520 - val_acc: 0.6884\n",
      "Epoch 4175/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3101 - acc: 0.8877 - val_loss: 1.5997 - val_acc: 0.7107\n",
      "Epoch 4176/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2892 - acc: 0.8955 - val_loss: 2.3552 - val_acc: 0.6009\n",
      "Epoch 4177/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3281 - acc: 0.8899 - val_loss: 1.4991 - val_acc: 0.7027\n",
      "Epoch 4178/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2890 - acc: 0.8941 - val_loss: 1.8334 - val_acc: 0.6679\n",
      "Epoch 4179/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2865 - acc: 0.8963 - val_loss: 1.5754 - val_acc: 0.7080\n",
      "Epoch 4180/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2993 - acc: 0.8923 - val_loss: 2.0186 - val_acc: 0.6446\n",
      "Epoch 4181/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3160 - acc: 0.8875 - val_loss: 2.0949 - val_acc: 0.6438\n",
      "Epoch 4182/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3082 - acc: 0.8939 - val_loss: 1.7042 - val_acc: 0.6920\n",
      "Epoch 4183/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3061 - acc: 0.8906 - val_loss: 1.6497 - val_acc: 0.6866\n",
      "Epoch 4184/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3116 - acc: 0.8874 - val_loss: 1.4843 - val_acc: 0.6991\n",
      "Epoch 4185/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2947 - acc: 0.8935 - val_loss: 1.9149 - val_acc: 0.6652\n",
      "Epoch 4186/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3132 - acc: 0.8901 - val_loss: 1.5769 - val_acc: 0.6768\n",
      "Epoch 4187/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3115 - acc: 0.8884 - val_loss: 1.4667 - val_acc: 0.6866\n",
      "Epoch 4188/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3180 - acc: 0.8839 - val_loss: 1.6114 - val_acc: 0.6527\n",
      "Epoch 4189/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3162 - acc: 0.8875 - val_loss: 1.6836 - val_acc: 0.6750\n",
      "Epoch 4190/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2988 - acc: 0.8931 - val_loss: 1.7698 - val_acc: 0.6464\n",
      "Epoch 4191/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3136 - acc: 0.8872 - val_loss: 1.8063 - val_acc: 0.6554\n",
      "Epoch 4192/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3015 - acc: 0.8917 - val_loss: 1.5397 - val_acc: 0.7080\n",
      "Epoch 4193/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3002 - acc: 0.8893 - val_loss: 1.7090 - val_acc: 0.6946\n",
      "Epoch 4194/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2890 - acc: 0.8959 - val_loss: 1.5711 - val_acc: 0.6991\n",
      "Epoch 4195/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3067 - acc: 0.8905 - val_loss: 1.6989 - val_acc: 0.7009\n",
      "Epoch 4196/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3000 - acc: 0.8904 - val_loss: 1.5544 - val_acc: 0.7000\n",
      "Epoch 4197/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2976 - acc: 0.8929 - val_loss: 2.0785 - val_acc: 0.6420\n",
      "Epoch 4198/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3060 - acc: 0.8928 - val_loss: 1.7087 - val_acc: 0.6741\n",
      "Epoch 4199/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3080 - acc: 0.8906 - val_loss: 1.6548 - val_acc: 0.6911\n",
      "Epoch 4200/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2885 - acc: 0.8985 - val_loss: 1.6985 - val_acc: 0.6795\n",
      "Epoch 4201/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3057 - acc: 0.8907 - val_loss: 1.8241 - val_acc: 0.6688\n",
      "Epoch 4202/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2936 - acc: 0.8956 - val_loss: 1.5627 - val_acc: 0.6955\n",
      "Epoch 4203/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3116 - acc: 0.8897 - val_loss: 1.7751 - val_acc: 0.6750\n",
      "Epoch 4204/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3137 - acc: 0.8891 - val_loss: 1.7158 - val_acc: 0.6473\n",
      "Epoch 4205/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2927 - acc: 0.8937 - val_loss: 1.6540 - val_acc: 0.6670\n",
      "Epoch 4206/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3104 - acc: 0.8897 - val_loss: 1.6519 - val_acc: 0.7089\n",
      "Epoch 4207/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2784 - acc: 0.8981 - val_loss: 1.6087 - val_acc: 0.6955\n",
      "Epoch 4208/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3141 - acc: 0.8895 - val_loss: 1.7613 - val_acc: 0.6661\n",
      "Epoch 4209/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3089 - acc: 0.8919 - val_loss: 1.5284 - val_acc: 0.7071\n",
      "Epoch 4210/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2980 - acc: 0.8943 - val_loss: 1.7653 - val_acc: 0.6875\n",
      "Epoch 4211/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3131 - acc: 0.8906 - val_loss: 1.7689 - val_acc: 0.6812\n",
      "Epoch 4212/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2819 - acc: 0.8972 - val_loss: 1.5579 - val_acc: 0.6937\n",
      "Epoch 4213/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2945 - acc: 0.8936 - val_loss: 2.0014 - val_acc: 0.6643\n",
      "Epoch 4214/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3131 - acc: 0.8920 - val_loss: 1.5938 - val_acc: 0.6848\n",
      "Epoch 4215/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2990 - acc: 0.8922 - val_loss: 1.5735 - val_acc: 0.6839\n",
      "Epoch 4216/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2856 - acc: 0.8973 - val_loss: 1.4561 - val_acc: 0.7268\n",
      "Epoch 4217/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2875 - acc: 0.8955 - val_loss: 1.5029 - val_acc: 0.7250\n",
      "Epoch 4218/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2962 - acc: 0.8946 - val_loss: 1.6508 - val_acc: 0.6911\n",
      "Epoch 4219/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3193 - acc: 0.8885 - val_loss: 2.0550 - val_acc: 0.6768\n",
      "Epoch 4220/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3009 - acc: 0.8958 - val_loss: 1.4676 - val_acc: 0.7143\n",
      "Epoch 4221/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3047 - acc: 0.8900 - val_loss: 1.3741 - val_acc: 0.7125\n",
      "Epoch 4222/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3120 - acc: 0.8899 - val_loss: 1.3955 - val_acc: 0.7277\n",
      "Epoch 4223/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2764 - acc: 0.8992 - val_loss: 1.6820 - val_acc: 0.6848\n",
      "Epoch 4224/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3037 - acc: 0.8885 - val_loss: 1.7532 - val_acc: 0.6937\n",
      "Epoch 4225/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2961 - acc: 0.8934 - val_loss: 1.6186 - val_acc: 0.6911\n",
      "Epoch 4226/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3184 - acc: 0.8831 - val_loss: 1.4699 - val_acc: 0.7143\n",
      "Epoch 4227/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2944 - acc: 0.8935 - val_loss: 1.7744 - val_acc: 0.6821\n",
      "Epoch 4228/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2918 - acc: 0.8936 - val_loss: 1.4391 - val_acc: 0.7268\n",
      "Epoch 4229/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2871 - acc: 0.8950 - val_loss: 1.5384 - val_acc: 0.6982\n",
      "Epoch 4230/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2907 - acc: 0.8933 - val_loss: 1.5492 - val_acc: 0.7098\n",
      "Epoch 4231/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2952 - acc: 0.8954 - val_loss: 1.6755 - val_acc: 0.6634\n",
      "Epoch 4232/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3095 - acc: 0.8919 - val_loss: 1.4316 - val_acc: 0.7134\n",
      "Epoch 4233/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2957 - acc: 0.8963 - val_loss: 1.6481 - val_acc: 0.6902\n",
      "Epoch 4234/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2928 - acc: 0.8949 - val_loss: 1.4762 - val_acc: 0.6705\n",
      "Epoch 4235/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3062 - acc: 0.8891 - val_loss: 1.9219 - val_acc: 0.6295\n",
      "Epoch 4236/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3060 - acc: 0.8921 - val_loss: 1.7248 - val_acc: 0.6875\n",
      "Epoch 4237/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2866 - acc: 0.8960 - val_loss: 1.6317 - val_acc: 0.6732\n",
      "Epoch 4238/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2901 - acc: 0.8961 - val_loss: 2.1610 - val_acc: 0.6268\n",
      "Epoch 4239/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3221 - acc: 0.8893 - val_loss: 2.0440 - val_acc: 0.6527\n",
      "Epoch 4240/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2959 - acc: 0.8923 - val_loss: 1.9553 - val_acc: 0.6402\n",
      "Epoch 4241/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3130 - acc: 0.8875 - val_loss: 1.8715 - val_acc: 0.6312\n",
      "Epoch 4242/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2966 - acc: 0.8947 - val_loss: 1.8697 - val_acc: 0.6384\n",
      "Epoch 4243/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3304 - acc: 0.8827 - val_loss: 1.5563 - val_acc: 0.6741\n",
      "Epoch 4244/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3059 - acc: 0.8911 - val_loss: 1.6224 - val_acc: 0.6920\n",
      "Epoch 4245/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.3014 - acc: 0.8919 - val_loss: 1.4809 - val_acc: 0.7196\n",
      "Epoch 4246/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2931 - acc: 0.8945 - val_loss: 1.7780 - val_acc: 0.6804\n",
      "Epoch 4247/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2972 - acc: 0.8921 - val_loss: 1.9017 - val_acc: 0.6902\n",
      "Epoch 4248/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3007 - acc: 0.8929 - val_loss: 1.8451 - val_acc: 0.6830\n",
      "Epoch 4249/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3027 - acc: 0.8951 - val_loss: 1.5851 - val_acc: 0.6964\n",
      "Epoch 4250/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2919 - acc: 0.8961 - val_loss: 1.8303 - val_acc: 0.6857\n",
      "Epoch 4251/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2871 - acc: 0.8989 - val_loss: 1.7664 - val_acc: 0.6902\n",
      "Epoch 4252/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3080 - acc: 0.8918 - val_loss: 1.7093 - val_acc: 0.7018\n",
      "Epoch 4253/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3085 - acc: 0.8906 - val_loss: 1.5662 - val_acc: 0.6723\n",
      "Epoch 4254/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2963 - acc: 0.8987 - val_loss: 1.8784 - val_acc: 0.6857\n",
      "Epoch 4255/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2999 - acc: 0.8905 - val_loss: 1.6052 - val_acc: 0.7054\n",
      "Epoch 4256/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3091 - acc: 0.8872 - val_loss: 1.7007 - val_acc: 0.6375\n",
      "Epoch 4257/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2943 - acc: 0.8935 - val_loss: 1.9887 - val_acc: 0.6813\n",
      "Epoch 4258/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3072 - acc: 0.8899 - val_loss: 1.7387 - val_acc: 0.6598\n",
      "Epoch 4259/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3087 - acc: 0.8899 - val_loss: 1.4650 - val_acc: 0.7054\n",
      "Epoch 4260/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2879 - acc: 0.8962 - val_loss: 1.6282 - val_acc: 0.6884\n",
      "Epoch 4261/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3048 - acc: 0.8917 - val_loss: 1.8454 - val_acc: 0.6804\n",
      "Epoch 4262/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2934 - acc: 0.8951 - val_loss: 2.0012 - val_acc: 0.6563\n",
      "Epoch 4263/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3056 - acc: 0.8941 - val_loss: 1.6909 - val_acc: 0.6696\n",
      "Epoch 4264/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2928 - acc: 0.8941 - val_loss: 1.8949 - val_acc: 0.6509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4265/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2982 - acc: 0.8951 - val_loss: 1.6418 - val_acc: 0.6911\n",
      "Epoch 4266/5000\n",
      "15008/15008 [==============================] - 1s 82us/step - loss: 0.3032 - acc: 0.8938 - val_loss: 2.1348 - val_acc: 0.6277\n",
      "Epoch 4267/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3208 - acc: 0.8931 - val_loss: 1.9535 - val_acc: 0.6598\n",
      "Epoch 4268/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3143 - acc: 0.8908 - val_loss: 1.7066 - val_acc: 0.6929\n",
      "Epoch 4269/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2958 - acc: 0.8948 - val_loss: 1.9108 - val_acc: 0.6750\n",
      "Epoch 4270/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3245 - acc: 0.8895 - val_loss: 1.7424 - val_acc: 0.6795\n",
      "Epoch 4271/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3020 - acc: 0.8925 - val_loss: 1.7350 - val_acc: 0.6795\n",
      "Epoch 4272/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2883 - acc: 0.9009 - val_loss: 2.0003 - val_acc: 0.6491\n",
      "Epoch 4273/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3047 - acc: 0.8957 - val_loss: 1.5975 - val_acc: 0.7000\n",
      "Epoch 4274/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2918 - acc: 0.8944 - val_loss: 1.4559 - val_acc: 0.7161\n",
      "Epoch 4275/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2875 - acc: 0.8953 - val_loss: 1.6075 - val_acc: 0.6821\n",
      "Epoch 4276/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3080 - acc: 0.8884 - val_loss: 1.7735 - val_acc: 0.6705\n",
      "Epoch 4277/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3084 - acc: 0.8921 - val_loss: 1.5150 - val_acc: 0.7134\n",
      "Epoch 4278/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2869 - acc: 0.8963 - val_loss: 1.7112 - val_acc: 0.6848\n",
      "Epoch 4279/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2932 - acc: 0.8959 - val_loss: 1.7976 - val_acc: 0.6973\n",
      "Epoch 4280/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3357 - acc: 0.8891 - val_loss: 1.4008 - val_acc: 0.6866\n",
      "Epoch 4281/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3011 - acc: 0.8899 - val_loss: 1.7984 - val_acc: 0.6768\n",
      "Epoch 4282/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3013 - acc: 0.8919 - val_loss: 1.7432 - val_acc: 0.7018\n",
      "Epoch 4283/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2879 - acc: 0.8988 - val_loss: 2.0414 - val_acc: 0.6527\n",
      "Epoch 4284/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2960 - acc: 0.8943 - val_loss: 1.5265 - val_acc: 0.7107\n",
      "Epoch 4285/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2819 - acc: 0.8983 - val_loss: 1.7576 - val_acc: 0.6607\n",
      "Epoch 4286/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3081 - acc: 0.8905 - val_loss: 1.5147 - val_acc: 0.6955\n",
      "Epoch 4287/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2975 - acc: 0.8915 - val_loss: 1.8657 - val_acc: 0.6777\n",
      "Epoch 4288/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2973 - acc: 0.8917 - val_loss: 2.3568 - val_acc: 0.5893\n",
      "Epoch 4289/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3036 - acc: 0.8949 - val_loss: 1.4500 - val_acc: 0.7205\n",
      "Epoch 4290/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2872 - acc: 0.8979 - val_loss: 1.6679 - val_acc: 0.6964\n",
      "Epoch 4291/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3002 - acc: 0.8927 - val_loss: 1.5422 - val_acc: 0.6982\n",
      "Epoch 4292/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3017 - acc: 0.8920 - val_loss: 1.6577 - val_acc: 0.6848\n",
      "Epoch 4293/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3012 - acc: 0.8904 - val_loss: 1.6990 - val_acc: 0.6982\n",
      "Epoch 4294/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3053 - acc: 0.8911 - val_loss: 1.4375 - val_acc: 0.7098\n",
      "Epoch 4295/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2762 - acc: 0.8981 - val_loss: 1.6362 - val_acc: 0.6732\n",
      "Epoch 4296/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3261 - acc: 0.8860 - val_loss: 2.1051 - val_acc: 0.6491\n",
      "Epoch 4297/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2902 - acc: 0.8990 - val_loss: 1.7209 - val_acc: 0.6705\n",
      "Epoch 4298/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2964 - acc: 0.8937 - val_loss: 1.6984 - val_acc: 0.6937\n",
      "Epoch 4299/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3054 - acc: 0.8899 - val_loss: 1.5690 - val_acc: 0.7071\n",
      "Epoch 4300/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2875 - acc: 0.8973 - val_loss: 1.9410 - val_acc: 0.6571\n",
      "Epoch 4301/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3105 - acc: 0.8943 - val_loss: 1.6809 - val_acc: 0.6688\n",
      "Epoch 4302/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2866 - acc: 0.8939 - val_loss: 1.4897 - val_acc: 0.7241\n",
      "Epoch 4303/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2884 - acc: 0.8974 - val_loss: 1.5123 - val_acc: 0.6821\n",
      "Epoch 4304/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3210 - acc: 0.8889 - val_loss: 1.4404 - val_acc: 0.7223\n",
      "Epoch 4305/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2886 - acc: 0.8959 - val_loss: 1.9890 - val_acc: 0.6759\n",
      "Epoch 4306/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3050 - acc: 0.8958 - val_loss: 1.6330 - val_acc: 0.6839\n",
      "Epoch 4307/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2952 - acc: 0.8947 - val_loss: 1.8910 - val_acc: 0.6696\n",
      "Epoch 4308/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2959 - acc: 0.8941 - val_loss: 1.3921 - val_acc: 0.7321\n",
      "Epoch 4309/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2884 - acc: 0.8963 - val_loss: 1.8515 - val_acc: 0.6714\n",
      "Epoch 4310/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2985 - acc: 0.8963 - val_loss: 2.1762 - val_acc: 0.6768\n",
      "Epoch 4311/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3207 - acc: 0.8925 - val_loss: 1.7365 - val_acc: 0.6884\n",
      "Epoch 4312/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2841 - acc: 0.8991 - val_loss: 1.7365 - val_acc: 0.6973\n",
      "Epoch 4313/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3004 - acc: 0.8929 - val_loss: 2.1541 - val_acc: 0.6402\n",
      "Epoch 4314/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3131 - acc: 0.8905 - val_loss: 1.6393 - val_acc: 0.6866\n",
      "Epoch 4315/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2813 - acc: 0.8989 - val_loss: 1.8273 - val_acc: 0.6598\n",
      "Epoch 4316/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3181 - acc: 0.8929 - val_loss: 1.7185 - val_acc: 0.6813\n",
      "Epoch 4317/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2947 - acc: 0.8915 - val_loss: 1.9657 - val_acc: 0.5938\n",
      "Epoch 4318/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3093 - acc: 0.8857 - val_loss: 1.6849 - val_acc: 0.6964\n",
      "Epoch 4319/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2855 - acc: 0.8980 - val_loss: 2.0323 - val_acc: 0.6688\n",
      "Epoch 4320/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.3050 - acc: 0.8891 - val_loss: 2.1520 - val_acc: 0.6357\n",
      "Epoch 4321/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3186 - acc: 0.8898 - val_loss: 1.4761 - val_acc: 0.7071\n",
      "Epoch 4322/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2740 - acc: 0.9003 - val_loss: 1.8637 - val_acc: 0.6768\n",
      "Epoch 4323/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3050 - acc: 0.8925 - val_loss: 1.5959 - val_acc: 0.7062\n",
      "Epoch 4324/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2997 - acc: 0.8923 - val_loss: 1.6386 - val_acc: 0.6768\n",
      "Epoch 4325/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2865 - acc: 0.8959 - val_loss: 1.8196 - val_acc: 0.6509\n",
      "Epoch 4326/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3057 - acc: 0.8895 - val_loss: 1.6654 - val_acc: 0.7080\n",
      "Epoch 4327/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2958 - acc: 0.8932 - val_loss: 1.5605 - val_acc: 0.7036\n",
      "Epoch 4328/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2873 - acc: 0.8957 - val_loss: 1.9556 - val_acc: 0.6554\n",
      "Epoch 4329/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2974 - acc: 0.8982 - val_loss: 2.0419 - val_acc: 0.6339\n",
      "Epoch 4330/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2960 - acc: 0.8961 - val_loss: 1.4406 - val_acc: 0.7214\n",
      "Epoch 4331/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2971 - acc: 0.8925 - val_loss: 1.7197 - val_acc: 0.6687\n",
      "Epoch 4332/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2990 - acc: 0.8993 - val_loss: 1.6907 - val_acc: 0.6795\n",
      "Epoch 4333/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2987 - acc: 0.8907 - val_loss: 1.9408 - val_acc: 0.6652\n",
      "Epoch 4334/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3075 - acc: 0.8944 - val_loss: 1.6406 - val_acc: 0.6893\n",
      "Epoch 4335/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2997 - acc: 0.8889 - val_loss: 1.5446 - val_acc: 0.6955\n",
      "Epoch 4336/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2833 - acc: 0.8963 - val_loss: 1.6807 - val_acc: 0.6625\n",
      "Epoch 4337/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2964 - acc: 0.8943 - val_loss: 1.5154 - val_acc: 0.7116\n",
      "Epoch 4338/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2851 - acc: 0.8963 - val_loss: 1.3759 - val_acc: 0.7286\n",
      "Epoch 4339/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2852 - acc: 0.8958 - val_loss: 1.4859 - val_acc: 0.7134\n",
      "Epoch 4340/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2805 - acc: 0.8977 - val_loss: 2.1245 - val_acc: 0.6313\n",
      "Epoch 4341/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3334 - acc: 0.8874 - val_loss: 1.7038 - val_acc: 0.6670\n",
      "Epoch 4342/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2997 - acc: 0.8948 - val_loss: 1.7126 - val_acc: 0.6562\n",
      "Epoch 4343/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3148 - acc: 0.8869 - val_loss: 1.5463 - val_acc: 0.7125\n",
      "Epoch 4344/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2791 - acc: 0.8969 - val_loss: 1.6486 - val_acc: 0.6732\n",
      "Epoch 4345/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.2880 - acc: 0.8948 - val_loss: 1.6804 - val_acc: 0.7000\n",
      "Epoch 4346/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2932 - acc: 0.8971 - val_loss: 1.7974 - val_acc: 0.6839\n",
      "Epoch 4347/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2904 - acc: 0.8955 - val_loss: 1.5759 - val_acc: 0.6696\n",
      "Epoch 4348/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3128 - acc: 0.8909 - val_loss: 1.9417 - val_acc: 0.6848\n",
      "Epoch 4349/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3019 - acc: 0.8945 - val_loss: 1.7636 - val_acc: 0.6464\n",
      "Epoch 4350/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3102 - acc: 0.8951 - val_loss: 1.7393 - val_acc: 0.7063\n",
      "Epoch 4351/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2901 - acc: 0.8979 - val_loss: 1.6863 - val_acc: 0.6696\n",
      "Epoch 4352/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2881 - acc: 0.8917 - val_loss: 1.8518 - val_acc: 0.6732\n",
      "Epoch 4353/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3006 - acc: 0.8941 - val_loss: 1.3483 - val_acc: 0.7170\n",
      "Epoch 4354/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2794 - acc: 0.8951 - val_loss: 1.4430 - val_acc: 0.7196\n",
      "Epoch 4355/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2946 - acc: 0.8935 - val_loss: 1.8725 - val_acc: 0.6589\n",
      "Epoch 4356/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2808 - acc: 0.8984 - val_loss: 1.6878 - val_acc: 0.7000\n",
      "Epoch 4357/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2896 - acc: 0.8972 - val_loss: 1.5433 - val_acc: 0.7027\n",
      "Epoch 4358/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2829 - acc: 0.8993 - val_loss: 1.9274 - val_acc: 0.6705\n",
      "Epoch 4359/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3149 - acc: 0.8932 - val_loss: 1.5673 - val_acc: 0.6884\n",
      "Epoch 4360/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2970 - acc: 0.8917 - val_loss: 1.7041 - val_acc: 0.6884\n",
      "Epoch 4361/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2779 - acc: 0.8987 - val_loss: 1.5459 - val_acc: 0.7295\n",
      "Epoch 4362/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2872 - acc: 0.8997 - val_loss: 1.5790 - val_acc: 0.6741\n",
      "Epoch 4363/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3113 - acc: 0.8889 - val_loss: 1.6739 - val_acc: 0.7054\n",
      "Epoch 4364/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2860 - acc: 0.8965 - val_loss: 1.8674 - val_acc: 0.6911\n",
      "Epoch 4365/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2948 - acc: 0.8926 - val_loss: 2.0494 - val_acc: 0.6411\n",
      "Epoch 4366/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3273 - acc: 0.8899 - val_loss: 1.6860 - val_acc: 0.6795\n",
      "Epoch 4367/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2838 - acc: 0.8983 - val_loss: 1.7310 - val_acc: 0.6964\n",
      "Epoch 4368/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2753 - acc: 0.9013 - val_loss: 1.9178 - val_acc: 0.6661\n",
      "Epoch 4369/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3145 - acc: 0.8914 - val_loss: 1.7157 - val_acc: 0.6911\n",
      "Epoch 4370/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2820 - acc: 0.8972 - val_loss: 1.7221 - val_acc: 0.6875\n",
      "Epoch 4371/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.3005 - acc: 0.8899 - val_loss: 1.5553 - val_acc: 0.6991\n",
      "Epoch 4372/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3025 - acc: 0.8909 - val_loss: 1.3755 - val_acc: 0.7348\n",
      "Epoch 4373/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2879 - acc: 0.8936 - val_loss: 1.7817 - val_acc: 0.6821\n",
      "Epoch 4374/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2896 - acc: 0.8958 - val_loss: 1.6431 - val_acc: 0.6911\n",
      "Epoch 4375/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3021 - acc: 0.8920 - val_loss: 1.6677 - val_acc: 0.6857\n",
      "Epoch 4376/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2724 - acc: 0.9019 - val_loss: 1.7522 - val_acc: 0.6625\n",
      "Epoch 4377/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2998 - acc: 0.8953 - val_loss: 1.6087 - val_acc: 0.6964\n",
      "Epoch 4378/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3022 - acc: 0.8936 - val_loss: 1.4645 - val_acc: 0.7196\n",
      "Epoch 4379/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2845 - acc: 0.8947 - val_loss: 1.6412 - val_acc: 0.7071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4380/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2960 - acc: 0.8947 - val_loss: 2.1849 - val_acc: 0.6429\n",
      "Epoch 4381/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3199 - acc: 0.8891 - val_loss: 1.5105 - val_acc: 0.7286\n",
      "Epoch 4382/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2861 - acc: 0.8956 - val_loss: 2.3330 - val_acc: 0.5884\n",
      "Epoch 4383/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3214 - acc: 0.8937 - val_loss: 1.8381 - val_acc: 0.6304\n",
      "Epoch 4384/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3179 - acc: 0.8912 - val_loss: 1.5180 - val_acc: 0.6804\n",
      "Epoch 4385/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2972 - acc: 0.8955 - val_loss: 1.9292 - val_acc: 0.6580\n",
      "Epoch 4386/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3053 - acc: 0.8939 - val_loss: 2.0875 - val_acc: 0.6304\n",
      "Epoch 4387/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3161 - acc: 0.8905 - val_loss: 1.8862 - val_acc: 0.6625\n",
      "Epoch 4388/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3011 - acc: 0.8915 - val_loss: 1.5159 - val_acc: 0.7250\n",
      "Epoch 4389/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2836 - acc: 0.8963 - val_loss: 1.6356 - val_acc: 0.6571\n",
      "Epoch 4390/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3063 - acc: 0.8919 - val_loss: 1.8192 - val_acc: 0.6938\n",
      "Epoch 4391/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2820 - acc: 0.8975 - val_loss: 1.5819 - val_acc: 0.7187\n",
      "Epoch 4392/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2907 - acc: 0.8959 - val_loss: 1.5536 - val_acc: 0.6973\n",
      "Epoch 4393/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2828 - acc: 0.8989 - val_loss: 1.6661 - val_acc: 0.6964\n",
      "Epoch 4394/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2955 - acc: 0.8969 - val_loss: 1.9834 - val_acc: 0.6625\n",
      "Epoch 4395/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2950 - acc: 0.8976 - val_loss: 1.5822 - val_acc: 0.7071\n",
      "Epoch 4396/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2973 - acc: 0.8923 - val_loss: 1.4748 - val_acc: 0.7295\n",
      "Epoch 4397/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2937 - acc: 0.8937 - val_loss: 1.6155 - val_acc: 0.6625\n",
      "Epoch 4398/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3106 - acc: 0.8945 - val_loss: 1.4553 - val_acc: 0.6848\n",
      "Epoch 4399/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2932 - acc: 0.8951 - val_loss: 1.5291 - val_acc: 0.7036\n",
      "Epoch 4400/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2848 - acc: 0.8981 - val_loss: 1.5353 - val_acc: 0.6866\n",
      "Epoch 4401/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2958 - acc: 0.8946 - val_loss: 1.6978 - val_acc: 0.6839\n",
      "Epoch 4402/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2920 - acc: 0.8965 - val_loss: 1.3976 - val_acc: 0.7152\n",
      "Epoch 4403/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3039 - acc: 0.8891 - val_loss: 1.4606 - val_acc: 0.7107\n",
      "Epoch 4404/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2731 - acc: 0.9008 - val_loss: 2.0867 - val_acc: 0.6304\n",
      "Epoch 4405/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3108 - acc: 0.8955 - val_loss: 1.4502 - val_acc: 0.6920\n",
      "Epoch 4406/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2956 - acc: 0.8931 - val_loss: 1.5606 - val_acc: 0.7205\n",
      "Epoch 4407/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2970 - acc: 0.8965 - val_loss: 1.6715 - val_acc: 0.6973\n",
      "Epoch 4408/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2959 - acc: 0.8928 - val_loss: 1.6656 - val_acc: 0.7027\n",
      "Epoch 4409/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2858 - acc: 0.8993 - val_loss: 1.7980 - val_acc: 0.6750\n",
      "Epoch 4410/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3062 - acc: 0.8921 - val_loss: 1.7366 - val_acc: 0.6866\n",
      "Epoch 4411/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3026 - acc: 0.8927 - val_loss: 1.4937 - val_acc: 0.7205\n",
      "Epoch 4412/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2929 - acc: 0.8930 - val_loss: 1.4739 - val_acc: 0.7116\n",
      "Epoch 4413/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2779 - acc: 0.9021 - val_loss: 1.6064 - val_acc: 0.6893\n",
      "Epoch 4414/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2821 - acc: 0.8957 - val_loss: 1.4009 - val_acc: 0.7018\n",
      "Epoch 4415/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2802 - acc: 0.8957 - val_loss: 1.6852 - val_acc: 0.7027\n",
      "Epoch 4416/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3078 - acc: 0.8906 - val_loss: 1.5661 - val_acc: 0.6857\n",
      "Epoch 4417/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2959 - acc: 0.8922 - val_loss: 1.5985 - val_acc: 0.7000\n",
      "Epoch 4418/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2809 - acc: 0.8981 - val_loss: 1.5045 - val_acc: 0.7116\n",
      "Epoch 4419/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2791 - acc: 0.8997 - val_loss: 1.8435 - val_acc: 0.6652\n",
      "Epoch 4420/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3038 - acc: 0.8944 - val_loss: 1.7551 - val_acc: 0.6848\n",
      "Epoch 4421/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2937 - acc: 0.8957 - val_loss: 2.1196 - val_acc: 0.6402\n",
      "Epoch 4422/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3344 - acc: 0.8930 - val_loss: 1.4713 - val_acc: 0.7286\n",
      "Epoch 4423/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2846 - acc: 0.8985 - val_loss: 1.4695 - val_acc: 0.7143\n",
      "Epoch 4424/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2781 - acc: 0.8993 - val_loss: 1.4517 - val_acc: 0.7161\n",
      "Epoch 4425/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2806 - acc: 0.8967 - val_loss: 1.8414 - val_acc: 0.6973\n",
      "Epoch 4426/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2893 - acc: 0.8965 - val_loss: 1.7735 - val_acc: 0.6991\n",
      "Epoch 4427/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2843 - acc: 0.8981 - val_loss: 1.4677 - val_acc: 0.7295\n",
      "Epoch 4428/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2979 - acc: 0.8921 - val_loss: 1.9911 - val_acc: 0.6429\n",
      "Epoch 4429/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2997 - acc: 0.8907 - val_loss: 1.6122 - val_acc: 0.6938\n",
      "Epoch 4430/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2751 - acc: 0.9019 - val_loss: 1.7438 - val_acc: 0.6857\n",
      "Epoch 4431/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2887 - acc: 0.8973 - val_loss: 1.8802 - val_acc: 0.6562\n",
      "Epoch 4432/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2969 - acc: 0.8962 - val_loss: 1.4905 - val_acc: 0.7036\n",
      "Epoch 4433/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2962 - acc: 0.8945 - val_loss: 1.6753 - val_acc: 0.6661\n",
      "Epoch 4434/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2813 - acc: 0.8978 - val_loss: 1.9252 - val_acc: 0.6402\n",
      "Epoch 4435/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3036 - acc: 0.8937 - val_loss: 1.6878 - val_acc: 0.6848\n",
      "Epoch 4436/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2758 - acc: 0.9005 - val_loss: 1.6602 - val_acc: 0.6813\n",
      "Epoch 4437/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3198 - acc: 0.8868 - val_loss: 1.4207 - val_acc: 0.7170\n",
      "Epoch 4438/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2824 - acc: 0.9001 - val_loss: 2.0552 - val_acc: 0.6661\n",
      "Epoch 4439/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3060 - acc: 0.8925 - val_loss: 1.8391 - val_acc: 0.6500\n",
      "Epoch 4440/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2964 - acc: 0.8977 - val_loss: 1.4595 - val_acc: 0.7241\n",
      "Epoch 4441/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2831 - acc: 0.8962 - val_loss: 1.4016 - val_acc: 0.7268\n",
      "Epoch 4442/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2803 - acc: 0.9004 - val_loss: 1.8785 - val_acc: 0.6902\n",
      "Epoch 4443/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2903 - acc: 0.8978 - val_loss: 1.6498 - val_acc: 0.6902\n",
      "Epoch 4444/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2982 - acc: 0.8947 - val_loss: 1.4807 - val_acc: 0.7205\n",
      "Epoch 4445/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2798 - acc: 0.9019 - val_loss: 1.5343 - val_acc: 0.7196\n",
      "Epoch 4446/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2921 - acc: 0.8982 - val_loss: 1.4975 - val_acc: 0.6696\n",
      "Epoch 4447/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3041 - acc: 0.8897 - val_loss: 1.6516 - val_acc: 0.7018\n",
      "Epoch 4448/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2799 - acc: 0.8977 - val_loss: 1.9152 - val_acc: 0.6705\n",
      "Epoch 4449/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3015 - acc: 0.8917 - val_loss: 1.7266 - val_acc: 0.6759\n",
      "Epoch 4450/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2792 - acc: 0.8990 - val_loss: 2.2902 - val_acc: 0.6348\n",
      "Epoch 4451/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3208 - acc: 0.8929 - val_loss: 1.4464 - val_acc: 0.7214\n",
      "Epoch 4452/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2797 - acc: 0.8995 - val_loss: 1.7405 - val_acc: 0.6804\n",
      "Epoch 4453/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2797 - acc: 0.8984 - val_loss: 1.7836 - val_acc: 0.6973\n",
      "Epoch 4454/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2932 - acc: 0.8971 - val_loss: 1.3906 - val_acc: 0.7205\n",
      "Epoch 4455/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2838 - acc: 0.8975 - val_loss: 2.1785 - val_acc: 0.6545\n",
      "Epoch 4456/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2997 - acc: 0.8988 - val_loss: 2.0708 - val_acc: 0.6482\n",
      "Epoch 4457/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3081 - acc: 0.8973 - val_loss: 1.9983 - val_acc: 0.6527\n",
      "Epoch 4458/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.3075 - acc: 0.8981 - val_loss: 1.5599 - val_acc: 0.7071\n",
      "Epoch 4459/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2864 - acc: 0.8992 - val_loss: 1.8694 - val_acc: 0.6777\n",
      "Epoch 4460/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2954 - acc: 0.8945 - val_loss: 2.1823 - val_acc: 0.6545\n",
      "Epoch 4461/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2889 - acc: 0.8975 - val_loss: 2.2806 - val_acc: 0.6170\n",
      "Epoch 4462/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3130 - acc: 0.8958 - val_loss: 1.8674 - val_acc: 0.6384\n",
      "Epoch 4463/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2990 - acc: 0.8934 - val_loss: 1.8783 - val_acc: 0.6661\n",
      "Epoch 4464/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2853 - acc: 0.8974 - val_loss: 1.5859 - val_acc: 0.7304\n",
      "Epoch 4465/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2781 - acc: 0.9001 - val_loss: 1.4932 - val_acc: 0.7214\n",
      "Epoch 4466/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2909 - acc: 0.8961 - val_loss: 1.6503 - val_acc: 0.6777\n",
      "Epoch 4467/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3253 - acc: 0.8872 - val_loss: 1.7627 - val_acc: 0.6750\n",
      "Epoch 4468/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2808 - acc: 0.8979 - val_loss: 1.8825 - val_acc: 0.6857\n",
      "Epoch 4469/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2878 - acc: 0.8982 - val_loss: 1.4090 - val_acc: 0.7250\n",
      "Epoch 4470/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2807 - acc: 0.8971 - val_loss: 1.5370 - val_acc: 0.7170\n",
      "Epoch 4471/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2862 - acc: 0.8959 - val_loss: 1.7196 - val_acc: 0.6777\n",
      "Epoch 4472/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2972 - acc: 0.8941 - val_loss: 1.7811 - val_acc: 0.6920\n",
      "Epoch 4473/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2812 - acc: 0.8953 - val_loss: 1.7136 - val_acc: 0.7179\n",
      "Epoch 4474/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2869 - acc: 0.8986 - val_loss: 1.6395 - val_acc: 0.7143\n",
      "Epoch 4475/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3269 - acc: 0.8911 - val_loss: 1.7280 - val_acc: 0.6607\n",
      "Epoch 4476/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2871 - acc: 0.8981 - val_loss: 1.9681 - val_acc: 0.6554\n",
      "Epoch 4477/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2969 - acc: 0.8925 - val_loss: 1.9183 - val_acc: 0.6920\n",
      "Epoch 4478/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3019 - acc: 0.8947 - val_loss: 1.6999 - val_acc: 0.6848\n",
      "Epoch 4479/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2914 - acc: 0.8954 - val_loss: 1.3606 - val_acc: 0.7330\n",
      "Epoch 4480/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2750 - acc: 0.8983 - val_loss: 1.5503 - val_acc: 0.7054\n",
      "Epoch 4481/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2847 - acc: 0.8969 - val_loss: 1.4201 - val_acc: 0.7170\n",
      "Epoch 4482/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2849 - acc: 0.8962 - val_loss: 1.4272 - val_acc: 0.7152\n",
      "Epoch 4483/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2719 - acc: 0.9002 - val_loss: 1.5178 - val_acc: 0.7098\n",
      "Epoch 4484/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2809 - acc: 0.9005 - val_loss: 1.5609 - val_acc: 0.6866\n",
      "Epoch 4485/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2847 - acc: 0.8955 - val_loss: 1.3651 - val_acc: 0.7080\n",
      "Epoch 4486/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2759 - acc: 0.9002 - val_loss: 1.3580 - val_acc: 0.7143\n",
      "Epoch 4487/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2912 - acc: 0.8939 - val_loss: 1.6268 - val_acc: 0.6973\n",
      "Epoch 4488/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2867 - acc: 0.8965 - val_loss: 1.6146 - val_acc: 0.7205\n",
      "Epoch 4489/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2827 - acc: 0.8989 - val_loss: 1.8530 - val_acc: 0.6920\n",
      "Epoch 4490/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2942 - acc: 0.8981 - val_loss: 1.5952 - val_acc: 0.7089\n",
      "Epoch 4491/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2913 - acc: 0.8907 - val_loss: 1.3576 - val_acc: 0.7295\n",
      "Epoch 4492/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2757 - acc: 0.8989 - val_loss: 1.5098 - val_acc: 0.7223\n",
      "Epoch 4493/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2771 - acc: 0.9011 - val_loss: 1.3907 - val_acc: 0.7143\n",
      "Epoch 4494/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2846 - acc: 0.8967 - val_loss: 1.9183 - val_acc: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4495/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3087 - acc: 0.8972 - val_loss: 1.9373 - val_acc: 0.6616\n",
      "Epoch 4496/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2962 - acc: 0.8971 - val_loss: 1.4220 - val_acc: 0.7339\n",
      "Epoch 4497/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2760 - acc: 0.8995 - val_loss: 1.4471 - val_acc: 0.7196\n",
      "Epoch 4498/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2890 - acc: 0.8944 - val_loss: 1.4613 - val_acc: 0.7321\n",
      "Epoch 4499/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2626 - acc: 0.9053 - val_loss: 2.1026 - val_acc: 0.6429\n",
      "Epoch 4500/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3035 - acc: 0.8931 - val_loss: 1.4874 - val_acc: 0.7259\n",
      "Epoch 4501/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2790 - acc: 0.9013 - val_loss: 1.8924 - val_acc: 0.6812\n",
      "Epoch 4502/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2835 - acc: 0.8999 - val_loss: 1.5665 - val_acc: 0.6884\n",
      "Epoch 4503/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2995 - acc: 0.8963 - val_loss: 1.7085 - val_acc: 0.6616\n",
      "Epoch 4504/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3088 - acc: 0.8915 - val_loss: 1.6795 - val_acc: 0.7080\n",
      "Epoch 4505/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2805 - acc: 0.8998 - val_loss: 1.6895 - val_acc: 0.6839\n",
      "Epoch 4506/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2950 - acc: 0.8937 - val_loss: 1.9131 - val_acc: 0.6366\n",
      "Epoch 4507/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3206 - acc: 0.8879 - val_loss: 1.3411 - val_acc: 0.7187\n",
      "Epoch 4508/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2767 - acc: 0.8999 - val_loss: 1.6718 - val_acc: 0.7080\n",
      "Epoch 4509/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2995 - acc: 0.8920 - val_loss: 1.5462 - val_acc: 0.7080\n",
      "Epoch 4510/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2799 - acc: 0.9005 - val_loss: 1.3715 - val_acc: 0.7313\n",
      "Epoch 4511/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2778 - acc: 0.8985 - val_loss: 1.9048 - val_acc: 0.6634\n",
      "Epoch 4512/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2753 - acc: 0.9018 - val_loss: 1.6932 - val_acc: 0.7089\n",
      "Epoch 4513/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2802 - acc: 0.8971 - val_loss: 1.7948 - val_acc: 0.6937\n",
      "Epoch 4514/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2790 - acc: 0.9012 - val_loss: 1.7372 - val_acc: 0.6482\n",
      "Epoch 4515/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3159 - acc: 0.8906 - val_loss: 1.9325 - val_acc: 0.6688\n",
      "Epoch 4516/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3027 - acc: 0.8986 - val_loss: 1.4376 - val_acc: 0.6982\n",
      "Epoch 4517/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2901 - acc: 0.8957 - val_loss: 1.6466 - val_acc: 0.6839\n",
      "Epoch 4518/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2847 - acc: 0.8987 - val_loss: 1.6283 - val_acc: 0.6946\n",
      "Epoch 4519/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2831 - acc: 0.8984 - val_loss: 1.4601 - val_acc: 0.6946\n",
      "Epoch 4520/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3004 - acc: 0.8913 - val_loss: 1.9303 - val_acc: 0.6750\n",
      "Epoch 4521/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2943 - acc: 0.8942 - val_loss: 1.8529 - val_acc: 0.6563\n",
      "Epoch 4522/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2930 - acc: 0.8971 - val_loss: 1.6073 - val_acc: 0.6982\n",
      "Epoch 4523/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2981 - acc: 0.8931 - val_loss: 1.9304 - val_acc: 0.6509\n",
      "Epoch 4524/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3202 - acc: 0.8893 - val_loss: 2.2542 - val_acc: 0.6125\n",
      "Epoch 4525/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3189 - acc: 0.8935 - val_loss: 1.7839 - val_acc: 0.6625\n",
      "Epoch 4526/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2869 - acc: 0.8972 - val_loss: 1.4253 - val_acc: 0.7027\n",
      "Epoch 4527/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2743 - acc: 0.9012 - val_loss: 1.5697 - val_acc: 0.6741\n",
      "Epoch 4528/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2844 - acc: 0.8976 - val_loss: 1.9051 - val_acc: 0.6438\n",
      "Epoch 4529/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2937 - acc: 0.8987 - val_loss: 1.5341 - val_acc: 0.7009\n",
      "Epoch 4530/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2841 - acc: 0.8928 - val_loss: 2.0908 - val_acc: 0.6598\n",
      "Epoch 4531/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2952 - acc: 0.8962 - val_loss: 1.5897 - val_acc: 0.6911\n",
      "Epoch 4532/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2845 - acc: 0.8999 - val_loss: 1.7558 - val_acc: 0.6741\n",
      "Epoch 4533/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2951 - acc: 0.8928 - val_loss: 1.5694 - val_acc: 0.7250\n",
      "Epoch 4534/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2766 - acc: 0.8989 - val_loss: 1.5409 - val_acc: 0.7116\n",
      "Epoch 4535/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2883 - acc: 0.8967 - val_loss: 2.2844 - val_acc: 0.6152\n",
      "Epoch 4536/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2912 - acc: 0.8971 - val_loss: 1.7642 - val_acc: 0.6893\n",
      "Epoch 4537/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2900 - acc: 0.8953 - val_loss: 1.7400 - val_acc: 0.6616\n",
      "Epoch 4538/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3024 - acc: 0.8910 - val_loss: 1.6557 - val_acc: 0.6946\n",
      "Epoch 4539/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2822 - acc: 0.8965 - val_loss: 1.8363 - val_acc: 0.6911\n",
      "Epoch 4540/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2870 - acc: 0.8991 - val_loss: 1.6758 - val_acc: 0.7000\n",
      "Epoch 4541/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2990 - acc: 0.8935 - val_loss: 1.5629 - val_acc: 0.7134\n",
      "Epoch 4542/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2778 - acc: 0.9015 - val_loss: 1.6601 - val_acc: 0.6714\n",
      "Epoch 4543/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2928 - acc: 0.8943 - val_loss: 1.4904 - val_acc: 0.7027\n",
      "Epoch 4544/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2927 - acc: 0.8920 - val_loss: 1.4242 - val_acc: 0.6955\n",
      "Epoch 4545/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2828 - acc: 0.8974 - val_loss: 1.7914 - val_acc: 0.6795\n",
      "Epoch 4546/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2806 - acc: 0.9008 - val_loss: 1.7179 - val_acc: 0.6330\n",
      "Epoch 4547/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3131 - acc: 0.8893 - val_loss: 1.6130 - val_acc: 0.7134\n",
      "Epoch 4548/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2818 - acc: 0.8983 - val_loss: 1.6532 - val_acc: 0.6866\n",
      "Epoch 4549/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2726 - acc: 0.9022 - val_loss: 1.6840 - val_acc: 0.6804\n",
      "Epoch 4550/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2952 - acc: 0.8916 - val_loss: 1.8057 - val_acc: 0.6964\n",
      "Epoch 4551/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2849 - acc: 0.8983 - val_loss: 2.1673 - val_acc: 0.6527\n",
      "Epoch 4552/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3006 - acc: 0.8972 - val_loss: 1.4659 - val_acc: 0.7214\n",
      "Epoch 4553/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2741 - acc: 0.9019 - val_loss: 1.6114 - val_acc: 0.6920\n",
      "Epoch 4554/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2824 - acc: 0.8996 - val_loss: 1.6019 - val_acc: 0.7027\n",
      "Epoch 4555/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2891 - acc: 0.8987 - val_loss: 1.5848 - val_acc: 0.7000\n",
      "Epoch 4556/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2932 - acc: 0.8944 - val_loss: 2.1760 - val_acc: 0.6259\n",
      "Epoch 4557/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.3315 - acc: 0.8912 - val_loss: 1.6181 - val_acc: 0.6804\n",
      "Epoch 4558/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2710 - acc: 0.9007 - val_loss: 1.8062 - val_acc: 0.6830\n",
      "Epoch 4559/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3053 - acc: 0.8889 - val_loss: 1.9626 - val_acc: 0.6598\n",
      "Epoch 4560/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2822 - acc: 0.9001 - val_loss: 1.6649 - val_acc: 0.7143\n",
      "Epoch 4561/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2819 - acc: 0.8976 - val_loss: 1.5180 - val_acc: 0.7107\n",
      "Epoch 4562/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2797 - acc: 0.8981 - val_loss: 2.2767 - val_acc: 0.6330\n",
      "Epoch 4563/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3188 - acc: 0.8931 - val_loss: 1.5071 - val_acc: 0.7214\n",
      "Epoch 4564/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2791 - acc: 0.8983 - val_loss: 2.1626 - val_acc: 0.6250\n",
      "Epoch 4565/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2974 - acc: 0.8953 - val_loss: 1.5923 - val_acc: 0.7071\n",
      "Epoch 4566/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2860 - acc: 0.8979 - val_loss: 1.4189 - val_acc: 0.7286\n",
      "Epoch 4567/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2765 - acc: 0.8994 - val_loss: 1.4952 - val_acc: 0.7116\n",
      "Epoch 4568/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2658 - acc: 0.9051 - val_loss: 1.6647 - val_acc: 0.6991\n",
      "Epoch 4569/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2835 - acc: 0.8958 - val_loss: 1.8209 - val_acc: 0.6598\n",
      "Epoch 4570/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2935 - acc: 0.8961 - val_loss: 1.5716 - val_acc: 0.7098\n",
      "Epoch 4571/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2850 - acc: 0.8997 - val_loss: 1.4368 - val_acc: 0.7321\n",
      "Epoch 4572/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2806 - acc: 0.8979 - val_loss: 1.5350 - val_acc: 0.7071\n",
      "Epoch 4573/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2734 - acc: 0.9017 - val_loss: 2.2172 - val_acc: 0.6313\n",
      "Epoch 4574/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3039 - acc: 0.8927 - val_loss: 1.8271 - val_acc: 0.6661\n",
      "Epoch 4575/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2899 - acc: 0.8984 - val_loss: 1.8746 - val_acc: 0.6813\n",
      "Epoch 4576/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2907 - acc: 0.8942 - val_loss: 1.6404 - val_acc: 0.7134\n",
      "Epoch 4577/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2768 - acc: 0.8995 - val_loss: 1.9050 - val_acc: 0.6821\n",
      "Epoch 4578/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2893 - acc: 0.8963 - val_loss: 1.6998 - val_acc: 0.6786\n",
      "Epoch 4579/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2821 - acc: 0.8969 - val_loss: 1.7205 - val_acc: 0.6937\n",
      "Epoch 4580/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2899 - acc: 0.8958 - val_loss: 1.9288 - val_acc: 0.6884\n",
      "Epoch 4581/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2965 - acc: 0.8952 - val_loss: 2.0188 - val_acc: 0.6188\n",
      "Epoch 4582/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2825 - acc: 0.9003 - val_loss: 1.7482 - val_acc: 0.6955\n",
      "Epoch 4583/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2924 - acc: 0.8983 - val_loss: 1.6973 - val_acc: 0.7125\n",
      "Epoch 4584/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2779 - acc: 0.9007 - val_loss: 1.4844 - val_acc: 0.7321\n",
      "Epoch 4585/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2797 - acc: 0.8982 - val_loss: 2.0063 - val_acc: 0.6536\n",
      "Epoch 4586/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3039 - acc: 0.8925 - val_loss: 1.6437 - val_acc: 0.7027\n",
      "Epoch 4587/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2839 - acc: 0.8985 - val_loss: 2.2691 - val_acc: 0.6438\n",
      "Epoch 4588/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2930 - acc: 0.8982 - val_loss: 1.6968 - val_acc: 0.6768\n",
      "Epoch 4589/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2866 - acc: 0.8993 - val_loss: 1.8996 - val_acc: 0.6652\n",
      "Epoch 4590/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2774 - acc: 0.8995 - val_loss: 1.4224 - val_acc: 0.7036\n",
      "Epoch 4591/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2820 - acc: 0.8983 - val_loss: 1.4998 - val_acc: 0.7429\n",
      "Epoch 4592/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2721 - acc: 0.8996 - val_loss: 1.9273 - val_acc: 0.6741\n",
      "Epoch 4593/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3019 - acc: 0.8955 - val_loss: 1.8666 - val_acc: 0.6821\n",
      "Epoch 4594/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2939 - acc: 0.8927 - val_loss: 1.3684 - val_acc: 0.7187\n",
      "Epoch 4595/5000\n",
      "15008/15008 [==============================] - 1s 94us/step - loss: 0.2588 - acc: 0.9078 - val_loss: 1.5161 - val_acc: 0.7143\n",
      "Epoch 4596/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2783 - acc: 0.8988 - val_loss: 2.1319 - val_acc: 0.6464\n",
      "Epoch 4597/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3137 - acc: 0.8922 - val_loss: 1.7255 - val_acc: 0.7045\n",
      "Epoch 4598/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2750 - acc: 0.9026 - val_loss: 1.6556 - val_acc: 0.7196\n",
      "Epoch 4599/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2772 - acc: 0.8987 - val_loss: 1.5819 - val_acc: 0.6973\n",
      "Epoch 4600/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2827 - acc: 0.8987 - val_loss: 2.0147 - val_acc: 0.6214\n",
      "Epoch 4601/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3128 - acc: 0.8925 - val_loss: 1.4488 - val_acc: 0.7018\n",
      "Epoch 4602/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2770 - acc: 0.9001 - val_loss: 1.8550 - val_acc: 0.6786\n",
      "Epoch 4603/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2898 - acc: 0.8999 - val_loss: 1.6043 - val_acc: 0.6991\n",
      "Epoch 4604/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2856 - acc: 0.8970 - val_loss: 1.9902 - val_acc: 0.6473\n",
      "Epoch 4605/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2890 - acc: 0.8981 - val_loss: 1.8003 - val_acc: 0.6643\n",
      "Epoch 4606/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2862 - acc: 0.8981 - val_loss: 1.7556 - val_acc: 0.6723\n",
      "Epoch 4607/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2834 - acc: 0.8985 - val_loss: 1.6306 - val_acc: 0.7080\n",
      "Epoch 4608/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2763 - acc: 0.8974 - val_loss: 1.7095 - val_acc: 0.7179\n",
      "Epoch 4609/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2709 - acc: 0.9021 - val_loss: 1.6438 - val_acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4610/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3067 - acc: 0.8940 - val_loss: 1.5708 - val_acc: 0.6607\n",
      "Epoch 4611/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2854 - acc: 0.8978 - val_loss: 1.9219 - val_acc: 0.6571\n",
      "Epoch 4612/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2956 - acc: 0.8992 - val_loss: 1.6789 - val_acc: 0.7089\n",
      "Epoch 4613/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2789 - acc: 0.9029 - val_loss: 1.6263 - val_acc: 0.7071\n",
      "Epoch 4614/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2939 - acc: 0.8923 - val_loss: 1.6168 - val_acc: 0.6866\n",
      "Epoch 4615/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2807 - acc: 0.8982 - val_loss: 2.2727 - val_acc: 0.6277\n",
      "Epoch 4616/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3273 - acc: 0.8929 - val_loss: 1.9079 - val_acc: 0.6777\n",
      "Epoch 4617/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3023 - acc: 0.8913 - val_loss: 2.0278 - val_acc: 0.6562\n",
      "Epoch 4618/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2968 - acc: 0.8969 - val_loss: 1.3811 - val_acc: 0.7384\n",
      "Epoch 4619/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2816 - acc: 0.8982 - val_loss: 1.5513 - val_acc: 0.7116\n",
      "Epoch 4620/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2777 - acc: 0.8992 - val_loss: 2.0717 - val_acc: 0.6482\n",
      "Epoch 4621/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2841 - acc: 0.8988 - val_loss: 1.8666 - val_acc: 0.6839\n",
      "Epoch 4622/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2812 - acc: 0.8986 - val_loss: 1.5619 - val_acc: 0.7134\n",
      "Epoch 4623/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2800 - acc: 0.8986 - val_loss: 1.5459 - val_acc: 0.7152\n",
      "Epoch 4624/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2802 - acc: 0.8975 - val_loss: 1.5935 - val_acc: 0.7036\n",
      "Epoch 4625/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2731 - acc: 0.8993 - val_loss: 1.9113 - val_acc: 0.6205\n",
      "Epoch 4626/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2946 - acc: 0.8935 - val_loss: 1.4691 - val_acc: 0.7045\n",
      "Epoch 4627/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2738 - acc: 0.9021 - val_loss: 1.6857 - val_acc: 0.6911\n",
      "Epoch 4628/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2935 - acc: 0.8947 - val_loss: 1.5437 - val_acc: 0.6884\n",
      "Epoch 4629/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2944 - acc: 0.8962 - val_loss: 2.0343 - val_acc: 0.6464\n",
      "Epoch 4630/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2903 - acc: 0.8991 - val_loss: 1.5807 - val_acc: 0.7214\n",
      "Epoch 4631/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2791 - acc: 0.9027 - val_loss: 1.7073 - val_acc: 0.6902\n",
      "Epoch 4632/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2881 - acc: 0.8946 - val_loss: 1.7320 - val_acc: 0.6902\n",
      "Epoch 4633/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2860 - acc: 0.8959 - val_loss: 1.9501 - val_acc: 0.6688\n",
      "Epoch 4634/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2862 - acc: 0.8985 - val_loss: 2.0809 - val_acc: 0.6777\n",
      "Epoch 4635/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2906 - acc: 0.8981 - val_loss: 1.5453 - val_acc: 0.6964\n",
      "Epoch 4636/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3086 - acc: 0.8907 - val_loss: 1.9384 - val_acc: 0.6384\n",
      "Epoch 4637/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2997 - acc: 0.8967 - val_loss: 1.7646 - val_acc: 0.6804\n",
      "Epoch 4638/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2790 - acc: 0.8983 - val_loss: 1.8057 - val_acc: 0.6982\n",
      "Epoch 4639/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2991 - acc: 0.8961 - val_loss: 1.7430 - val_acc: 0.7009\n",
      "Epoch 4640/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2851 - acc: 0.8954 - val_loss: 1.4733 - val_acc: 0.7161\n",
      "Epoch 4641/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2779 - acc: 0.8999 - val_loss: 2.0990 - val_acc: 0.6786\n",
      "Epoch 4642/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2906 - acc: 0.8953 - val_loss: 2.4642 - val_acc: 0.6286\n",
      "Epoch 4643/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2878 - acc: 0.9000 - val_loss: 1.5166 - val_acc: 0.7134\n",
      "Epoch 4644/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2833 - acc: 0.8986 - val_loss: 1.4665 - val_acc: 0.7286\n",
      "Epoch 4645/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2782 - acc: 0.8996 - val_loss: 1.6349 - val_acc: 0.7009\n",
      "Epoch 4646/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2799 - acc: 0.9000 - val_loss: 1.7717 - val_acc: 0.6687\n",
      "Epoch 4647/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2850 - acc: 0.9005 - val_loss: 1.6222 - val_acc: 0.7009\n",
      "Epoch 4648/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2733 - acc: 0.9005 - val_loss: 1.5008 - val_acc: 0.7080\n",
      "Epoch 4649/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2814 - acc: 0.9007 - val_loss: 1.5554 - val_acc: 0.7259\n",
      "Epoch 4650/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2844 - acc: 0.8970 - val_loss: 2.2441 - val_acc: 0.6491\n",
      "Epoch 4651/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2808 - acc: 0.9020 - val_loss: 2.0232 - val_acc: 0.6696\n",
      "Epoch 4652/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3232 - acc: 0.8914 - val_loss: 1.5077 - val_acc: 0.7000\n",
      "Epoch 4653/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2553 - acc: 0.9062 - val_loss: 1.6325 - val_acc: 0.7125\n",
      "Epoch 4654/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2846 - acc: 0.8980 - val_loss: 1.4821 - val_acc: 0.7179\n",
      "Epoch 4655/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2778 - acc: 0.9001 - val_loss: 1.5315 - val_acc: 0.7036\n",
      "Epoch 4656/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2690 - acc: 0.9029 - val_loss: 1.6966 - val_acc: 0.6964\n",
      "Epoch 4657/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2896 - acc: 0.8993 - val_loss: 1.6242 - val_acc: 0.7098\n",
      "Epoch 4658/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2732 - acc: 0.8986 - val_loss: 1.6393 - val_acc: 0.6920\n",
      "Epoch 4659/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2690 - acc: 0.9020 - val_loss: 2.1476 - val_acc: 0.6571\n",
      "Epoch 4660/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3025 - acc: 0.8973 - val_loss: 1.6502 - val_acc: 0.7116\n",
      "Epoch 4661/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2730 - acc: 0.9010 - val_loss: 3.1267 - val_acc: 0.5661\n",
      "Epoch 4662/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3167 - acc: 0.8974 - val_loss: 1.5576 - val_acc: 0.7116\n",
      "Epoch 4663/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2694 - acc: 0.9029 - val_loss: 1.8241 - val_acc: 0.6670\n",
      "Epoch 4664/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2828 - acc: 0.8976 - val_loss: 1.5152 - val_acc: 0.7000\n",
      "Epoch 4665/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2726 - acc: 0.9001 - val_loss: 1.3706 - val_acc: 0.7366\n",
      "Epoch 4666/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2811 - acc: 0.8966 - val_loss: 1.4777 - val_acc: 0.7161\n",
      "Epoch 4667/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2656 - acc: 0.9022 - val_loss: 1.6455 - val_acc: 0.6821\n",
      "Epoch 4668/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2820 - acc: 0.8992 - val_loss: 2.0627 - val_acc: 0.6500\n",
      "Epoch 4669/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.3033 - acc: 0.8941 - val_loss: 1.7889 - val_acc: 0.7027\n",
      "Epoch 4670/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2831 - acc: 0.8984 - val_loss: 1.8672 - val_acc: 0.6580\n",
      "Epoch 4671/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2742 - acc: 0.9033 - val_loss: 1.6329 - val_acc: 0.6884\n",
      "Epoch 4672/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2879 - acc: 0.9003 - val_loss: 1.8094 - val_acc: 0.6759\n",
      "Epoch 4673/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2923 - acc: 0.8939 - val_loss: 1.9146 - val_acc: 0.6634\n",
      "Epoch 4674/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2800 - acc: 0.9003 - val_loss: 1.6241 - val_acc: 0.6964\n",
      "Epoch 4675/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2943 - acc: 0.8936 - val_loss: 1.6318 - val_acc: 0.7027\n",
      "Epoch 4676/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2748 - acc: 0.8994 - val_loss: 1.7468 - val_acc: 0.7098\n",
      "Epoch 4677/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2819 - acc: 0.9001 - val_loss: 1.6656 - val_acc: 0.6991\n",
      "Epoch 4678/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2767 - acc: 0.9010 - val_loss: 1.5392 - val_acc: 0.6946\n",
      "Epoch 4679/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2745 - acc: 0.9036 - val_loss: 1.6775 - val_acc: 0.6830\n",
      "Epoch 4680/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2814 - acc: 0.8984 - val_loss: 1.5345 - val_acc: 0.7232\n",
      "Epoch 4681/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2924 - acc: 0.8937 - val_loss: 1.4396 - val_acc: 0.7179\n",
      "Epoch 4682/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2690 - acc: 0.9015 - val_loss: 1.8227 - val_acc: 0.6491\n",
      "Epoch 4683/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2906 - acc: 0.8979 - val_loss: 1.6125 - val_acc: 0.6821\n",
      "Epoch 4684/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2773 - acc: 0.9019 - val_loss: 1.9651 - val_acc: 0.6741\n",
      "Epoch 4685/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2820 - acc: 0.8995 - val_loss: 1.7859 - val_acc: 0.6813\n",
      "Epoch 4686/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2707 - acc: 0.9024 - val_loss: 1.5330 - val_acc: 0.7063\n",
      "Epoch 4687/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2767 - acc: 0.9016 - val_loss: 1.7846 - val_acc: 0.6911\n",
      "Epoch 4688/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2872 - acc: 0.8968 - val_loss: 1.7838 - val_acc: 0.7000\n",
      "Epoch 4689/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2945 - acc: 0.8976 - val_loss: 1.6068 - val_acc: 0.7045\n",
      "Epoch 4690/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2718 - acc: 0.9012 - val_loss: 1.7614 - val_acc: 0.6688\n",
      "Epoch 4691/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2814 - acc: 0.8975 - val_loss: 1.4530 - val_acc: 0.7161\n",
      "Epoch 4692/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2855 - acc: 0.8965 - val_loss: 1.8521 - val_acc: 0.6723\n",
      "Epoch 4693/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2907 - acc: 0.8990 - val_loss: 1.9913 - val_acc: 0.6839\n",
      "Epoch 4694/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2801 - acc: 0.9008 - val_loss: 1.8644 - val_acc: 0.6696\n",
      "Epoch 4695/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2978 - acc: 0.8949 - val_loss: 1.8050 - val_acc: 0.6464\n",
      "Epoch 4696/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2851 - acc: 0.8985 - val_loss: 1.5511 - val_acc: 0.7089\n",
      "Epoch 4697/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2788 - acc: 0.8987 - val_loss: 1.4856 - val_acc: 0.7161\n",
      "Epoch 4698/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2677 - acc: 0.9011 - val_loss: 1.5684 - val_acc: 0.7268\n",
      "Epoch 4699/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2628 - acc: 0.9015 - val_loss: 1.6805 - val_acc: 0.7071\n",
      "Epoch 4700/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2929 - acc: 0.8964 - val_loss: 1.5622 - val_acc: 0.7188\n",
      "Epoch 4701/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2868 - acc: 0.8987 - val_loss: 2.0661 - val_acc: 0.6804\n",
      "Epoch 4702/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2706 - acc: 0.9024 - val_loss: 1.5062 - val_acc: 0.6955\n",
      "Epoch 4703/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2943 - acc: 0.8948 - val_loss: 1.4761 - val_acc: 0.7125\n",
      "Epoch 4704/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2673 - acc: 0.9029 - val_loss: 1.4899 - val_acc: 0.7205\n",
      "Epoch 4705/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2747 - acc: 0.8999 - val_loss: 1.6698 - val_acc: 0.7223\n",
      "Epoch 4706/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2760 - acc: 0.9022 - val_loss: 2.0123 - val_acc: 0.6268\n",
      "Epoch 4707/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.3113 - acc: 0.8918 - val_loss: 1.9100 - val_acc: 0.6750\n",
      "Epoch 4708/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2878 - acc: 0.8981 - val_loss: 1.7736 - val_acc: 0.6920\n",
      "Epoch 4709/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2933 - acc: 0.8957 - val_loss: 1.3567 - val_acc: 0.6920\n",
      "Epoch 4710/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3001 - acc: 0.8924 - val_loss: 1.4625 - val_acc: 0.7223\n",
      "Epoch 4711/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2747 - acc: 0.8974 - val_loss: 1.8334 - val_acc: 0.6920\n",
      "Epoch 4712/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3123 - acc: 0.8895 - val_loss: 1.8785 - val_acc: 0.6679\n",
      "Epoch 4713/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2957 - acc: 0.8956 - val_loss: 1.5540 - val_acc: 0.7080\n",
      "Epoch 4714/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2659 - acc: 0.9023 - val_loss: 1.4727 - val_acc: 0.7196\n",
      "Epoch 4715/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2797 - acc: 0.8972 - val_loss: 1.4855 - val_acc: 0.7152\n",
      "Epoch 4716/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2685 - acc: 0.9016 - val_loss: 1.5376 - val_acc: 0.6973\n",
      "Epoch 4717/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2725 - acc: 0.9019 - val_loss: 1.6735 - val_acc: 0.7036\n",
      "Epoch 4718/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2733 - acc: 0.8973 - val_loss: 1.4857 - val_acc: 0.7196\n",
      "Epoch 4719/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2960 - acc: 0.8926 - val_loss: 1.4863 - val_acc: 0.6884\n",
      "Epoch 4720/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2833 - acc: 0.9006 - val_loss: 1.6499 - val_acc: 0.7009\n",
      "Epoch 4721/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2740 - acc: 0.9013 - val_loss: 1.4508 - val_acc: 0.7143\n",
      "Epoch 4722/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2821 - acc: 0.8977 - val_loss: 1.7150 - val_acc: 0.6920\n",
      "Epoch 4723/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2617 - acc: 0.9051 - val_loss: 1.5116 - val_acc: 0.7232\n",
      "Epoch 4724/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2785 - acc: 0.8997 - val_loss: 1.4747 - val_acc: 0.6830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4725/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2753 - acc: 0.9005 - val_loss: 1.5288 - val_acc: 0.7134\n",
      "Epoch 4726/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2757 - acc: 0.8980 - val_loss: 1.6413 - val_acc: 0.7107\n",
      "Epoch 4727/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2861 - acc: 0.8959 - val_loss: 1.6381 - val_acc: 0.7089\n",
      "Epoch 4728/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2844 - acc: 0.8959 - val_loss: 1.4688 - val_acc: 0.7188\n",
      "Epoch 4729/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2625 - acc: 0.9038 - val_loss: 1.6146 - val_acc: 0.6893\n",
      "Epoch 4730/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2934 - acc: 0.8960 - val_loss: 1.5022 - val_acc: 0.7045\n",
      "Epoch 4731/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2764 - acc: 0.9024 - val_loss: 1.4159 - val_acc: 0.7134\n",
      "Epoch 4732/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2735 - acc: 0.9012 - val_loss: 1.5298 - val_acc: 0.7080\n",
      "Epoch 4733/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2929 - acc: 0.8984 - val_loss: 2.0064 - val_acc: 0.6607\n",
      "Epoch 4734/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2895 - acc: 0.8970 - val_loss: 1.6117 - val_acc: 0.7107\n",
      "Epoch 4735/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2576 - acc: 0.9070 - val_loss: 1.5383 - val_acc: 0.7170\n",
      "Epoch 4736/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2778 - acc: 0.8982 - val_loss: 1.6259 - val_acc: 0.6929\n",
      "Epoch 4737/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2785 - acc: 0.9018 - val_loss: 1.7726 - val_acc: 0.6830\n",
      "Epoch 4738/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2989 - acc: 0.8935 - val_loss: 1.8251 - val_acc: 0.6821\n",
      "Epoch 4739/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2973 - acc: 0.8999 - val_loss: 1.5152 - val_acc: 0.7312\n",
      "Epoch 4740/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2664 - acc: 0.8999 - val_loss: 1.6369 - val_acc: 0.7063\n",
      "Epoch 4741/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2681 - acc: 0.9030 - val_loss: 1.7947 - val_acc: 0.6884\n",
      "Epoch 4742/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2832 - acc: 0.8976 - val_loss: 1.6947 - val_acc: 0.7170\n",
      "Epoch 4743/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2621 - acc: 0.9058 - val_loss: 2.1246 - val_acc: 0.6830\n",
      "Epoch 4744/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2906 - acc: 0.8951 - val_loss: 1.5584 - val_acc: 0.7152\n",
      "Epoch 4745/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2868 - acc: 0.8948 - val_loss: 1.7059 - val_acc: 0.7063\n",
      "Epoch 4746/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2626 - acc: 0.9052 - val_loss: 1.6090 - val_acc: 0.6946\n",
      "Epoch 4747/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2801 - acc: 0.8990 - val_loss: 1.6636 - val_acc: 0.6848\n",
      "Epoch 4748/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2830 - acc: 0.8973 - val_loss: 1.4419 - val_acc: 0.7250\n",
      "Epoch 4749/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2685 - acc: 0.8995 - val_loss: 1.4892 - val_acc: 0.7232\n",
      "Epoch 4750/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2901 - acc: 0.8983 - val_loss: 1.6296 - val_acc: 0.7116\n",
      "Epoch 4751/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2586 - acc: 0.9052 - val_loss: 1.9583 - val_acc: 0.6875\n",
      "Epoch 4752/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2758 - acc: 0.9005 - val_loss: 2.0288 - val_acc: 0.6339\n",
      "Epoch 4753/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2923 - acc: 0.8954 - val_loss: 1.5246 - val_acc: 0.7223\n",
      "Epoch 4754/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2714 - acc: 0.8989 - val_loss: 1.7087 - val_acc: 0.7000\n",
      "Epoch 4755/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2809 - acc: 0.9018 - val_loss: 1.5547 - val_acc: 0.7080\n",
      "Epoch 4756/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2733 - acc: 0.8999 - val_loss: 1.6155 - val_acc: 0.7125\n",
      "Epoch 4757/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2849 - acc: 0.8973 - val_loss: 1.5143 - val_acc: 0.7304\n",
      "Epoch 4758/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2775 - acc: 0.8997 - val_loss: 1.6135 - val_acc: 0.7232\n",
      "Epoch 4759/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2905 - acc: 0.8995 - val_loss: 1.7330 - val_acc: 0.6643\n",
      "Epoch 4760/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2805 - acc: 0.8987 - val_loss: 1.6550 - val_acc: 0.7045\n",
      "Epoch 4761/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2825 - acc: 0.8993 - val_loss: 1.6410 - val_acc: 0.7063\n",
      "Epoch 4762/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2569 - acc: 0.9064 - val_loss: 1.5084 - val_acc: 0.7188\n",
      "Epoch 4763/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2889 - acc: 0.8950 - val_loss: 2.1124 - val_acc: 0.6500\n",
      "Epoch 4764/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2935 - acc: 0.8960 - val_loss: 1.6531 - val_acc: 0.6875\n",
      "Epoch 4765/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2738 - acc: 0.8980 - val_loss: 1.4412 - val_acc: 0.7312\n",
      "Epoch 4766/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2642 - acc: 0.9051 - val_loss: 1.6651 - val_acc: 0.6866\n",
      "Epoch 4767/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2836 - acc: 0.8996 - val_loss: 1.9229 - val_acc: 0.7000\n",
      "Epoch 4768/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2690 - acc: 0.9043 - val_loss: 1.7400 - val_acc: 0.6973\n",
      "Epoch 4769/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2891 - acc: 0.8985 - val_loss: 1.8681 - val_acc: 0.6911\n",
      "Epoch 4770/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2886 - acc: 0.8983 - val_loss: 1.8286 - val_acc: 0.6938\n",
      "Epoch 4771/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2886 - acc: 0.8998 - val_loss: 1.7448 - val_acc: 0.6911\n",
      "Epoch 4772/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2806 - acc: 0.9007 - val_loss: 1.6380 - val_acc: 0.6982\n",
      "Epoch 4773/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2727 - acc: 0.8993 - val_loss: 2.2314 - val_acc: 0.6366\n",
      "Epoch 4774/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2930 - acc: 0.8985 - val_loss: 1.4819 - val_acc: 0.7205\n",
      "Epoch 4775/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2729 - acc: 0.9010 - val_loss: 2.1190 - val_acc: 0.6545\n",
      "Epoch 4776/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2927 - acc: 0.8989 - val_loss: 1.5333 - val_acc: 0.6955\n",
      "Epoch 4777/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2696 - acc: 0.9047 - val_loss: 1.8562 - val_acc: 0.6714\n",
      "Epoch 4778/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.3000 - acc: 0.8965 - val_loss: 1.7826 - val_acc: 0.6982\n",
      "Epoch 4779/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2833 - acc: 0.8977 - val_loss: 1.7482 - val_acc: 0.6518\n",
      "Epoch 4780/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2871 - acc: 0.8992 - val_loss: 2.0488 - val_acc: 0.6786\n",
      "Epoch 4781/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2947 - acc: 0.8991 - val_loss: 1.5796 - val_acc: 0.6973\n",
      "Epoch 4782/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2854 - acc: 0.8949 - val_loss: 1.7688 - val_acc: 0.6679\n",
      "Epoch 4783/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2718 - acc: 0.9024 - val_loss: 1.6600 - val_acc: 0.6598\n",
      "Epoch 4784/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2989 - acc: 0.9001 - val_loss: 2.2828 - val_acc: 0.6125\n",
      "Epoch 4785/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2859 - acc: 0.9021 - val_loss: 1.7373 - val_acc: 0.7045\n",
      "Epoch 4786/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2724 - acc: 0.9016 - val_loss: 1.5476 - val_acc: 0.7143\n",
      "Epoch 4787/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2898 - acc: 0.8965 - val_loss: 1.6370 - val_acc: 0.7036\n",
      "Epoch 4788/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2937 - acc: 0.8921 - val_loss: 1.4471 - val_acc: 0.6982\n",
      "Epoch 4789/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2714 - acc: 0.9013 - val_loss: 1.5327 - val_acc: 0.7062\n",
      "Epoch 4790/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2748 - acc: 0.9006 - val_loss: 1.5112 - val_acc: 0.7080\n",
      "Epoch 4791/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2736 - acc: 0.9011 - val_loss: 1.7355 - val_acc: 0.6964\n",
      "Epoch 4792/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2830 - acc: 0.9018 - val_loss: 1.6958 - val_acc: 0.6920\n",
      "Epoch 4793/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2741 - acc: 0.9003 - val_loss: 1.9848 - val_acc: 0.6571\n",
      "Epoch 4794/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2907 - acc: 0.8937 - val_loss: 1.7141 - val_acc: 0.7036\n",
      "Epoch 4795/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2687 - acc: 0.9025 - val_loss: 1.5779 - val_acc: 0.7045\n",
      "Epoch 4796/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2932 - acc: 0.8951 - val_loss: 1.5172 - val_acc: 0.7214\n",
      "Epoch 4797/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2716 - acc: 0.9021 - val_loss: 1.3953 - val_acc: 0.7393\n",
      "Epoch 4798/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2663 - acc: 0.9037 - val_loss: 1.6705 - val_acc: 0.7187\n",
      "Epoch 4799/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2796 - acc: 0.8967 - val_loss: 1.8426 - val_acc: 0.6937\n",
      "Epoch 4800/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2814 - acc: 0.9024 - val_loss: 2.0321 - val_acc: 0.6500\n",
      "Epoch 4801/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2977 - acc: 0.8980 - val_loss: 1.5257 - val_acc: 0.7196\n",
      "Epoch 4802/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2683 - acc: 0.9037 - val_loss: 1.7651 - val_acc: 0.7045\n",
      "Epoch 4803/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2804 - acc: 0.9004 - val_loss: 1.5748 - val_acc: 0.7098\n",
      "Epoch 4804/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2766 - acc: 0.9005 - val_loss: 1.6776 - val_acc: 0.7045\n",
      "Epoch 4805/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2661 - acc: 0.9038 - val_loss: 1.4600 - val_acc: 0.7161\n",
      "Epoch 4806/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2696 - acc: 0.9010 - val_loss: 1.8246 - val_acc: 0.6982\n",
      "Epoch 4807/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2585 - acc: 0.9090 - val_loss: 1.9340 - val_acc: 0.6634\n",
      "Epoch 4808/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.3046 - acc: 0.8974 - val_loss: 1.8670 - val_acc: 0.6991\n",
      "Epoch 4809/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2675 - acc: 0.9054 - val_loss: 1.7150 - val_acc: 0.7000\n",
      "Epoch 4810/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2782 - acc: 0.8981 - val_loss: 2.0348 - val_acc: 0.6634\n",
      "Epoch 4811/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2858 - acc: 0.8999 - val_loss: 1.5539 - val_acc: 0.6938\n",
      "Epoch 4812/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2813 - acc: 0.8969 - val_loss: 1.7121 - val_acc: 0.7018\n",
      "Epoch 4813/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2717 - acc: 0.9035 - val_loss: 2.0842 - val_acc: 0.6509\n",
      "Epoch 4814/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2903 - acc: 0.9013 - val_loss: 1.7546 - val_acc: 0.6982\n",
      "Epoch 4815/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2745 - acc: 0.9041 - val_loss: 1.7207 - val_acc: 0.6848\n",
      "Epoch 4816/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2712 - acc: 0.9037 - val_loss: 1.9210 - val_acc: 0.6437\n",
      "Epoch 4817/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.3064 - acc: 0.8907 - val_loss: 1.5806 - val_acc: 0.6982\n",
      "Epoch 4818/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2787 - acc: 0.9003 - val_loss: 2.1257 - val_acc: 0.6223\n",
      "Epoch 4819/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2923 - acc: 0.8955 - val_loss: 1.8721 - val_acc: 0.6848\n",
      "Epoch 4820/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2675 - acc: 0.9051 - val_loss: 1.6096 - val_acc: 0.7188\n",
      "Epoch 4821/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2682 - acc: 0.9047 - val_loss: 1.7258 - val_acc: 0.6848\n",
      "Epoch 4822/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3027 - acc: 0.8910 - val_loss: 1.6257 - val_acc: 0.7152\n",
      "Epoch 4823/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2737 - acc: 0.9021 - val_loss: 1.5777 - val_acc: 0.7027\n",
      "Epoch 4824/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2880 - acc: 0.8958 - val_loss: 1.8811 - val_acc: 0.6616\n",
      "Epoch 4825/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2773 - acc: 0.9027 - val_loss: 1.7634 - val_acc: 0.6750\n",
      "Epoch 4826/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2908 - acc: 0.8988 - val_loss: 1.7785 - val_acc: 0.6955\n",
      "Epoch 4827/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2541 - acc: 0.9069 - val_loss: 2.1451 - val_acc: 0.6634\n",
      "Epoch 4828/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2881 - acc: 0.8983 - val_loss: 2.1106 - val_acc: 0.6446\n",
      "Epoch 4829/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2995 - acc: 0.8959 - val_loss: 1.5159 - val_acc: 0.7205\n",
      "Epoch 4830/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2707 - acc: 0.9023 - val_loss: 1.7507 - val_acc: 0.6946\n",
      "Epoch 4831/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2680 - acc: 0.9023 - val_loss: 1.9609 - val_acc: 0.6750\n",
      "Epoch 4832/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2990 - acc: 0.8988 - val_loss: 1.5414 - val_acc: 0.6964\n",
      "Epoch 4833/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2834 - acc: 0.8998 - val_loss: 2.3501 - val_acc: 0.6125\n",
      "Epoch 4834/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2753 - acc: 0.9024 - val_loss: 1.6989 - val_acc: 0.6982\n",
      "Epoch 4835/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2830 - acc: 0.9000 - val_loss: 1.5030 - val_acc: 0.7241\n",
      "Epoch 4836/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2650 - acc: 0.9010 - val_loss: 1.5634 - val_acc: 0.7107\n",
      "Epoch 4837/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2803 - acc: 0.8991 - val_loss: 1.5705 - val_acc: 0.6839\n",
      "Epoch 4838/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2914 - acc: 0.8950 - val_loss: 1.5528 - val_acc: 0.7134\n",
      "Epoch 4839/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2640 - acc: 0.9013 - val_loss: 1.8972 - val_acc: 0.7036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4840/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2844 - acc: 0.9002 - val_loss: 1.7802 - val_acc: 0.7080\n",
      "Epoch 4841/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2708 - acc: 0.9029 - val_loss: 1.9462 - val_acc: 0.6580\n",
      "Epoch 4842/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2808 - acc: 0.8983 - val_loss: 1.6806 - val_acc: 0.7000\n",
      "Epoch 4843/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2824 - acc: 0.8976 - val_loss: 1.6008 - val_acc: 0.7375\n",
      "Epoch 4844/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2734 - acc: 0.9022 - val_loss: 1.7709 - val_acc: 0.6750\n",
      "Epoch 4845/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2680 - acc: 0.9034 - val_loss: 1.7425 - val_acc: 0.6866\n",
      "Epoch 4846/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2895 - acc: 0.8975 - val_loss: 1.8046 - val_acc: 0.6902\n",
      "Epoch 4847/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2807 - acc: 0.9004 - val_loss: 1.7808 - val_acc: 0.7009\n",
      "Epoch 4848/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2839 - acc: 0.8982 - val_loss: 1.5625 - val_acc: 0.7188\n",
      "Epoch 4849/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2717 - acc: 0.9029 - val_loss: 1.5303 - val_acc: 0.7286\n",
      "Epoch 4850/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2627 - acc: 0.9027 - val_loss: 1.8285 - val_acc: 0.7009\n",
      "Epoch 4851/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2795 - acc: 0.8995 - val_loss: 1.7894 - val_acc: 0.6839\n",
      "Epoch 4852/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2859 - acc: 0.8986 - val_loss: 1.8140 - val_acc: 0.6875\n",
      "Epoch 4853/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2926 - acc: 0.8988 - val_loss: 1.5543 - val_acc: 0.7116\n",
      "Epoch 4854/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2555 - acc: 0.9084 - val_loss: 1.6328 - val_acc: 0.7187\n",
      "Epoch 4855/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2834 - acc: 0.8995 - val_loss: 1.7367 - val_acc: 0.6955\n",
      "Epoch 4856/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2725 - acc: 0.9055 - val_loss: 1.5805 - val_acc: 0.7054\n",
      "Epoch 4857/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2703 - acc: 0.9033 - val_loss: 1.9109 - val_acc: 0.6509\n",
      "Epoch 4858/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2896 - acc: 0.8961 - val_loss: 1.4162 - val_acc: 0.7312\n",
      "Epoch 4859/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2691 - acc: 0.9005 - val_loss: 1.7265 - val_acc: 0.7018\n",
      "Epoch 4860/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2669 - acc: 0.9027 - val_loss: 1.5443 - val_acc: 0.7161\n",
      "Epoch 4861/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2747 - acc: 0.9017 - val_loss: 1.4293 - val_acc: 0.7134\n",
      "Epoch 4862/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2811 - acc: 0.8976 - val_loss: 1.8661 - val_acc: 0.6643\n",
      "Epoch 4863/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2766 - acc: 0.9037 - val_loss: 1.8468 - val_acc: 0.6973\n",
      "Epoch 4864/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2782 - acc: 0.9015 - val_loss: 1.9288 - val_acc: 0.6437\n",
      "Epoch 4865/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2803 - acc: 0.9009 - val_loss: 1.7500 - val_acc: 0.6955\n",
      "Epoch 4866/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2671 - acc: 0.9034 - val_loss: 1.7968 - val_acc: 0.6402\n",
      "Epoch 4867/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2761 - acc: 0.9011 - val_loss: 1.4146 - val_acc: 0.7375\n",
      "Epoch 4868/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2648 - acc: 0.9026 - val_loss: 1.7541 - val_acc: 0.6759\n",
      "Epoch 4869/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2909 - acc: 0.8971 - val_loss: 1.8267 - val_acc: 0.6580\n",
      "Epoch 4870/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2710 - acc: 0.9032 - val_loss: 1.6340 - val_acc: 0.7054\n",
      "Epoch 4871/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2752 - acc: 0.8985 - val_loss: 1.5503 - val_acc: 0.6875\n",
      "Epoch 4872/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2867 - acc: 0.8993 - val_loss: 1.8309 - val_acc: 0.6813\n",
      "Epoch 4873/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2747 - acc: 0.9027 - val_loss: 1.4391 - val_acc: 0.7250\n",
      "Epoch 4874/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2736 - acc: 0.9022 - val_loss: 1.6878 - val_acc: 0.6777\n",
      "Epoch 4875/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2859 - acc: 0.8980 - val_loss: 1.5549 - val_acc: 0.7250\n",
      "Epoch 4876/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2673 - acc: 0.9030 - val_loss: 1.6861 - val_acc: 0.7036\n",
      "Epoch 4877/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2657 - acc: 0.9057 - val_loss: 1.5227 - val_acc: 0.6982\n",
      "Epoch 4878/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2802 - acc: 0.9001 - val_loss: 1.5771 - val_acc: 0.7304\n",
      "Epoch 4879/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2684 - acc: 0.9029 - val_loss: 1.9301 - val_acc: 0.6411\n",
      "Epoch 4880/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2894 - acc: 0.8963 - val_loss: 2.0457 - val_acc: 0.6875\n",
      "Epoch 4881/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2927 - acc: 0.8967 - val_loss: 1.6783 - val_acc: 0.7045\n",
      "Epoch 4882/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2732 - acc: 0.9033 - val_loss: 1.8294 - val_acc: 0.6661\n",
      "Epoch 4883/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2751 - acc: 0.8997 - val_loss: 2.1471 - val_acc: 0.6277\n",
      "Epoch 4884/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2920 - acc: 0.8990 - val_loss: 1.8019 - val_acc: 0.6955\n",
      "Epoch 4885/5000\n",
      "15008/15008 [==============================] - 1s 88us/step - loss: 0.2658 - acc: 0.9033 - val_loss: 1.7986 - val_acc: 0.6821\n",
      "Epoch 4886/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2621 - acc: 0.9073 - val_loss: 1.6757 - val_acc: 0.7071\n",
      "Epoch 4887/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2668 - acc: 0.9025 - val_loss: 2.0366 - val_acc: 0.6518\n",
      "Epoch 4888/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2728 - acc: 0.9015 - val_loss: 1.4407 - val_acc: 0.6938\n",
      "Epoch 4889/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2847 - acc: 0.8964 - val_loss: 1.6561 - val_acc: 0.6884\n",
      "Epoch 4890/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2633 - acc: 0.9041 - val_loss: 1.7905 - val_acc: 0.6839\n",
      "Epoch 4891/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2763 - acc: 0.9014 - val_loss: 1.4924 - val_acc: 0.7357\n",
      "Epoch 4892/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2691 - acc: 0.9017 - val_loss: 1.5968 - val_acc: 0.7214\n",
      "Epoch 4893/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2643 - acc: 0.9007 - val_loss: 1.6899 - val_acc: 0.6946\n",
      "Epoch 4894/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2885 - acc: 0.8993 - val_loss: 1.8992 - val_acc: 0.6929\n",
      "Epoch 4895/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2790 - acc: 0.9023 - val_loss: 1.9990 - val_acc: 0.6821\n",
      "Epoch 4896/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2922 - acc: 0.8988 - val_loss: 1.7327 - val_acc: 0.6768\n",
      "Epoch 4897/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2829 - acc: 0.9002 - val_loss: 1.7025 - val_acc: 0.6902\n",
      "Epoch 4898/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2902 - acc: 0.9029 - val_loss: 1.6566 - val_acc: 0.6946\n",
      "Epoch 4899/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2903 - acc: 0.8973 - val_loss: 1.4641 - val_acc: 0.7232\n",
      "Epoch 4900/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2583 - acc: 0.9057 - val_loss: 1.8990 - val_acc: 0.6741\n",
      "Epoch 4901/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2783 - acc: 0.9028 - val_loss: 1.9423 - val_acc: 0.6437\n",
      "Epoch 4902/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2993 - acc: 0.8967 - val_loss: 2.1861 - val_acc: 0.6607\n",
      "Epoch 4903/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2700 - acc: 0.9052 - val_loss: 1.8586 - val_acc: 0.6625\n",
      "Epoch 4904/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2809 - acc: 0.9007 - val_loss: 1.4127 - val_acc: 0.7125\n",
      "Epoch 4905/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2641 - acc: 0.9037 - val_loss: 1.6352 - val_acc: 0.7107\n",
      "Epoch 4906/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2777 - acc: 0.8989 - val_loss: 1.6344 - val_acc: 0.6857\n",
      "Epoch 4907/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2984 - acc: 0.8938 - val_loss: 1.7317 - val_acc: 0.6884\n",
      "Epoch 4908/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2661 - acc: 0.9005 - val_loss: 1.4997 - val_acc: 0.7205\n",
      "Epoch 4909/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2615 - acc: 0.9011 - val_loss: 1.7519 - val_acc: 0.6732\n",
      "Epoch 4910/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2737 - acc: 0.8999 - val_loss: 1.4518 - val_acc: 0.7366\n",
      "Epoch 4911/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2523 - acc: 0.9080 - val_loss: 1.9051 - val_acc: 0.6937\n",
      "Epoch 4912/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2617 - acc: 0.9068 - val_loss: 1.7555 - val_acc: 0.7187\n",
      "Epoch 4913/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2796 - acc: 0.9004 - val_loss: 1.8738 - val_acc: 0.6696\n",
      "Epoch 4914/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2743 - acc: 0.9028 - val_loss: 1.7671 - val_acc: 0.6848\n",
      "Epoch 4915/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2593 - acc: 0.9046 - val_loss: 1.7424 - val_acc: 0.7000\n",
      "Epoch 4916/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2932 - acc: 0.8944 - val_loss: 1.6942 - val_acc: 0.6509\n",
      "Epoch 4917/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2804 - acc: 0.8999 - val_loss: 1.9799 - val_acc: 0.6661\n",
      "Epoch 4918/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2826 - acc: 0.9007 - val_loss: 1.7705 - val_acc: 0.6955\n",
      "Epoch 4919/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2812 - acc: 0.9001 - val_loss: 1.8037 - val_acc: 0.6634\n",
      "Epoch 4920/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2658 - acc: 0.9054 - val_loss: 1.5269 - val_acc: 0.7161\n",
      "Epoch 4921/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2632 - acc: 0.9035 - val_loss: 1.8758 - val_acc: 0.6937\n",
      "Epoch 4922/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2699 - acc: 0.9025 - val_loss: 1.7689 - val_acc: 0.6795\n",
      "Epoch 4923/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2875 - acc: 0.9005 - val_loss: 1.6388 - val_acc: 0.7330\n",
      "Epoch 4924/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2731 - acc: 0.9011 - val_loss: 1.8132 - val_acc: 0.7018\n",
      "Epoch 4925/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2823 - acc: 0.8996 - val_loss: 1.8582 - val_acc: 0.6643\n",
      "Epoch 4926/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2601 - acc: 0.9061 - val_loss: 1.6422 - val_acc: 0.7161\n",
      "Epoch 4927/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2723 - acc: 0.9013 - val_loss: 1.6179 - val_acc: 0.7205\n",
      "Epoch 4928/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2624 - acc: 0.9072 - val_loss: 2.0189 - val_acc: 0.6607\n",
      "Epoch 4929/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2907 - acc: 0.9011 - val_loss: 1.6468 - val_acc: 0.7080\n",
      "Epoch 4930/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2739 - acc: 0.9006 - val_loss: 1.7608 - val_acc: 0.6795\n",
      "Epoch 4931/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2736 - acc: 0.9005 - val_loss: 1.5315 - val_acc: 0.7205\n",
      "Epoch 4932/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2715 - acc: 0.9022 - val_loss: 1.6515 - val_acc: 0.7170\n",
      "Epoch 4933/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2683 - acc: 0.9019 - val_loss: 1.9533 - val_acc: 0.6571\n",
      "Epoch 4934/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2888 - acc: 0.8993 - val_loss: 1.9484 - val_acc: 0.6580\n",
      "Epoch 4935/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2888 - acc: 0.8983 - val_loss: 1.5405 - val_acc: 0.7232\n",
      "Epoch 4936/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2701 - acc: 0.9011 - val_loss: 1.5459 - val_acc: 0.7062\n",
      "Epoch 4937/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2752 - acc: 0.9033 - val_loss: 2.3899 - val_acc: 0.6339\n",
      "Epoch 4938/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2858 - acc: 0.9005 - val_loss: 1.6229 - val_acc: 0.7107\n",
      "Epoch 4939/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2681 - acc: 0.9018 - val_loss: 1.9589 - val_acc: 0.6839\n",
      "Epoch 4940/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2855 - acc: 0.8995 - val_loss: 1.5733 - val_acc: 0.6937\n",
      "Epoch 4941/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2620 - acc: 0.9044 - val_loss: 1.7976 - val_acc: 0.6964\n",
      "Epoch 4942/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2763 - acc: 0.9025 - val_loss: 1.7565 - val_acc: 0.6857\n",
      "Epoch 4943/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2670 - acc: 0.9055 - val_loss: 1.8968 - val_acc: 0.6786\n",
      "Epoch 4944/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2788 - acc: 0.9001 - val_loss: 1.6838 - val_acc: 0.6821\n",
      "Epoch 4945/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2791 - acc: 0.8985 - val_loss: 2.0930 - val_acc: 0.6705\n",
      "Epoch 4946/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2815 - acc: 0.8969 - val_loss: 1.6110 - val_acc: 0.7062\n",
      "Epoch 4947/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2628 - acc: 0.9047 - val_loss: 1.6027 - val_acc: 0.7045\n",
      "Epoch 4948/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2653 - acc: 0.9029 - val_loss: 1.5523 - val_acc: 0.7071\n",
      "Epoch 4949/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2604 - acc: 0.9061 - val_loss: 1.6640 - val_acc: 0.7018\n",
      "Epoch 4950/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2848 - acc: 0.8975 - val_loss: 1.4881 - val_acc: 0.7259\n",
      "Epoch 4951/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2766 - acc: 0.8986 - val_loss: 2.3806 - val_acc: 0.6348\n",
      "Epoch 4952/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2814 - acc: 0.9058 - val_loss: 1.5509 - val_acc: 0.7089\n",
      "Epoch 4953/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2686 - acc: 0.9012 - val_loss: 1.4419 - val_acc: 0.7357\n",
      "Epoch 4954/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2563 - acc: 0.9077 - val_loss: 1.7906 - val_acc: 0.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4955/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2723 - acc: 0.9052 - val_loss: 2.4707 - val_acc: 0.6205\n",
      "Epoch 4956/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2895 - acc: 0.9014 - val_loss: 1.6389 - val_acc: 0.7214\n",
      "Epoch 4957/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2569 - acc: 0.9064 - val_loss: 1.6676 - val_acc: 0.7054\n",
      "Epoch 4958/5000\n",
      "15008/15008 [==============================] - 1s 87us/step - loss: 0.2707 - acc: 0.9037 - val_loss: 1.7575 - val_acc: 0.7223\n",
      "Epoch 4959/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2844 - acc: 0.8983 - val_loss: 1.6265 - val_acc: 0.7179\n",
      "Epoch 4960/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2644 - acc: 0.9045 - val_loss: 1.7339 - val_acc: 0.6902\n",
      "Epoch 4961/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.3034 - acc: 0.8929 - val_loss: 1.4019 - val_acc: 0.7330\n",
      "Epoch 4962/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2714 - acc: 0.9040 - val_loss: 1.7185 - val_acc: 0.7134\n",
      "Epoch 4963/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2682 - acc: 0.9038 - val_loss: 1.5579 - val_acc: 0.7286\n",
      "Epoch 4964/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2593 - acc: 0.9069 - val_loss: 1.6914 - val_acc: 0.6759\n",
      "Epoch 4965/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2820 - acc: 0.8985 - val_loss: 1.6919 - val_acc: 0.6991\n",
      "Epoch 4966/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2646 - acc: 0.9027 - val_loss: 1.6394 - val_acc: 0.7045\n",
      "Epoch 4967/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2636 - acc: 0.9051 - val_loss: 1.7393 - val_acc: 0.6955\n",
      "Epoch 4968/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2762 - acc: 0.8994 - val_loss: 1.7691 - val_acc: 0.6920\n",
      "Epoch 4969/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2800 - acc: 0.9010 - val_loss: 1.9940 - val_acc: 0.6821\n",
      "Epoch 4970/5000\n",
      "15008/15008 [==============================] - 1s 91us/step - loss: 0.2704 - acc: 0.9035 - val_loss: 1.6053 - val_acc: 0.7018\n",
      "Epoch 4971/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2700 - acc: 0.9018 - val_loss: 1.4939 - val_acc: 0.7330\n",
      "Epoch 4972/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2705 - acc: 0.9023 - val_loss: 2.2116 - val_acc: 0.6384\n",
      "Epoch 4973/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2851 - acc: 0.9024 - val_loss: 1.9646 - val_acc: 0.6973\n",
      "Epoch 4974/5000\n",
      "15008/15008 [==============================] - 1s 83us/step - loss: 0.2739 - acc: 0.9037 - val_loss: 1.5611 - val_acc: 0.7268\n",
      "Epoch 4975/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2706 - acc: 0.9029 - val_loss: 1.6580 - val_acc: 0.7080\n",
      "Epoch 4976/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2637 - acc: 0.9055 - val_loss: 1.7399 - val_acc: 0.6857\n",
      "Epoch 4977/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2629 - acc: 0.9073 - val_loss: 2.0403 - val_acc: 0.6741\n",
      "Epoch 4978/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2720 - acc: 0.9019 - val_loss: 1.4886 - val_acc: 0.7205\n",
      "Epoch 4979/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2660 - acc: 0.9060 - val_loss: 1.8369 - val_acc: 0.6893\n",
      "Epoch 4980/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2761 - acc: 0.9031 - val_loss: 1.7672 - val_acc: 0.6670\n",
      "Epoch 4981/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2678 - acc: 0.9036 - val_loss: 1.6949 - val_acc: 0.7152\n",
      "Epoch 4982/5000\n",
      "15008/15008 [==============================] - 1s 93us/step - loss: 0.2871 - acc: 0.8983 - val_loss: 1.7748 - val_acc: 0.6920\n",
      "Epoch 4983/5000\n",
      "15008/15008 [==============================] - 1s 89us/step - loss: 0.2791 - acc: 0.9013 - val_loss: 1.6036 - val_acc: 0.7152\n",
      "Epoch 4984/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2637 - acc: 0.9052 - val_loss: 1.7000 - val_acc: 0.7170\n",
      "Epoch 4985/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2798 - acc: 0.9019 - val_loss: 1.7152 - val_acc: 0.6946\n",
      "Epoch 4986/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2773 - acc: 0.9011 - val_loss: 1.7033 - val_acc: 0.6884\n",
      "Epoch 4987/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2739 - acc: 0.9030 - val_loss: 1.7866 - val_acc: 0.6973\n",
      "Epoch 4988/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2826 - acc: 0.8999 - val_loss: 1.8353 - val_acc: 0.6955\n",
      "Epoch 4989/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2658 - acc: 0.9035 - val_loss: 2.0899 - val_acc: 0.6339\n",
      "Epoch 4990/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.3045 - acc: 0.8993 - val_loss: 1.6553 - val_acc: 0.6804\n",
      "Epoch 4991/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2909 - acc: 0.9004 - val_loss: 1.8854 - val_acc: 0.6920\n",
      "Epoch 4992/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2836 - acc: 0.9022 - val_loss: 1.8669 - val_acc: 0.6955\n",
      "Epoch 4993/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2839 - acc: 0.9001 - val_loss: 1.5593 - val_acc: 0.7232\n",
      "Epoch 4994/5000\n",
      "15008/15008 [==============================] - 1s 90us/step - loss: 0.2558 - acc: 0.9082 - val_loss: 1.5972 - val_acc: 0.7018\n",
      "Epoch 4995/5000\n",
      "15008/15008 [==============================] - 1s 92us/step - loss: 0.2767 - acc: 0.9019 - val_loss: 1.4868 - val_acc: 0.7143\n",
      "Epoch 4996/5000\n",
      "15008/15008 [==============================] - 1s 86us/step - loss: 0.2728 - acc: 0.9014 - val_loss: 1.6560 - val_acc: 0.6929\n",
      "Epoch 4997/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2540 - acc: 0.9075 - val_loss: 1.4976 - val_acc: 0.7170\n",
      "Epoch 4998/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2961 - acc: 0.8919 - val_loss: 1.8264 - val_acc: 0.6589\n",
      "Epoch 4999/5000\n",
      "15008/15008 [==============================] - 1s 84us/step - loss: 0.2942 - acc: 0.9005 - val_loss: 1.4675 - val_acc: 0.7018\n",
      "Epoch 5000/5000\n",
      "15008/15008 [==============================] - 1s 85us/step - loss: 0.2795 - acc: 0.8977 - val_loss: 1.7026 - val_acc: 0.7071\n",
      "Execution time = 106 min 52.3833327293396 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history= History()\n",
    "\n",
    "model_resnet.compile(optimizer=keras.optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "training = model_resnet.fit(train_features_1,\n",
    "                    train_lab,\n",
    "                    epochs=5000,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(val_features_1,val_lab), callbacks=[history])\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd8bfa942b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8XGW9+PHPd2ay70mTNm3TNd1b2kJpKQiUvYBaRK+CIqBg5XoRFPR3wQUUVHBBRUWvoIjoveCCCghaQKhsBUrpvqd7uiVt2mzNNjPP748zSWYms5xJZkmm3/frFZg55znnPNNMzvc8uxhjUEoppaJxpDoDSimlhgYNGEoppWzRgKGUUsoWDRhKKaVs0YChlFLKFg0YSimlbNGAoZRSyhYNGEr1g4jsFpELU50PpZJJA4ZSSilbNGAoFUci8hkRqRGRBhF5RkRG+raLiPxIROpEpFFE1onITN++y0Rkk4g0i8h+EflSaj+FUqFpwFAqTkTkfOA+4KNAJbAHeNK3+2LgHGAyUAx8DDjq2/dr4LPGmAJgJvByErOtlG2uVGdAqTTyCeBRY8x7ACJyJ3BMRMYBXUABMBV4xxiz2e+4LmC6iKw1xhwDjiU110rZpCUMpeJnJFapAgBjTAtWKWKUMeZl4GfAQ8BhEXlYRAp9ST8MXAbsEZF/i8jCJOdbKVs0YCgVPweAsd1vRCQPKAP2AxhjfmKMOQ2YgVU19WXf9pXGmCVABfA34I9JzrdStmjAUKr/MkQku/sH60b/KRGZIyJZwHeAt40xu0XkdBFZICIZQCvQDnhEJFNEPiEiRcaYLqAJ8KTsEykVgQYMpfrveaDN7+ds4OvAU8BBYCJwlS9tIfAIVvvEHqyqqh/49n0S2C0iTcBNwDVJyr9SMRFdQEkppZQdWsJQSillS8IChohUicgrIrJZRDaKyK0h0oiI/MQ30GmdiJzqt+86Ednu+7kuUflUSillT8KqpESkEqg0xrwnIgXAKuAKY8wmvzSXAZ/H6lK4AHjQGLNAREqBd4F5gPEde5qvj7pSSqkUSFgJwxhzsHsAkzGmGdgMjApKtgR43FjeAop9geYS4EVjTIMvSLwILE5UXpVSSkWXlJHevpGuc4G3g3aNAvb5va/1bQu3PdS5lwJLAcpyOG1ccXxiYJczjwxPKwDHc8eHTefydpDffgC3Iwu3MweHcXMiszwueUilLHcTOZ1H6XAVkuVuwoiLxpyqpOah+MQuwPr3938dTPBSdGIPRhw05oz1O343VgEVupy5tGYNj3qtxtyxFJ3oGXtHpyufTHdLxHy2ZFeS334w6uc5njseh/FQ2LYXI04ac8YAkN9+EJe3vSddW2YZHa7CnvfdeevWnlFMdtfxkNfo/n2F0ppVQZczL+S+THcLbmcOhW17e/IafF23MweXpy3KpwwU6jyRtrdllJLT1RDTNVKpw1VElrsxYecP9+/kz4iTlvzw96hotm3bdsQYY+umlfCAISL5WN0Mv2CMCf4mS4hDTITtfTca8zDwMMC8kU7z7tL8AeS21/7i+Yw6/g4AT899IGy6kpbtnLP9XhryqiltrYmafqiYUPcCs/b/np3lFzCh/kUg+Z9ryepre67r/9pfZlcTTtPJxRtvo8uRzfOze/e/f80NOE0XAIcKZ/P2xNujXuu5U77D5es+27N9X8mZVB17M2I+X5t0C2dv/3bUz/P03AfI6mpk8YbP0+4qZNksK69nb7un57sDsG70x9hV3jtzenfeum0dfhlTDj8T8ho7h53PhCMvhdy3ctynOFCyoM/23I7DXLTpyxwsmk5lY0NPXoOvW1cwk4rmDVE/p79Q5wF4eeoXOH/LV0Mc0QnE5284GWrKL6K6/p8JO3+4fz9/Hc58sr6+td/XEJE90VNZEhowfIOUngL+1xjzlxBJagH/x9bRWKNla4FFQduXJyaXg4vL04bHkYkRZ6qzYhnk3a4v3XBzqrPQf8ZLaev2iEmqjr4WzwuG3OowbgDy2w/E8VqRhQ4Wql8k1PN1YiSyl5RgzcK52RjzwzDJngGu9fWWOgNoNMYcBJYBF4tIiYiUYM30uSxRee0X403IaS9f91lO2/2LhJw7FiZ538GT1sT6ZZy9/dsBpYtgUw79Na7XFONhRu3/ktXV6LfN3rFOb0fM1xvWvCl6oiEskaWLwSiR4zDOwhrBer6IrPH9XCYiN4nITb40zwM7gRqsUbCfAzDGNAD3Ait9P/f4tqXEWdu/w5LV1/b8kQ1vXM2SNdf31PfGyulpZ+rBpxDfk12w7qowNTRImCf30HrTJvOJvltF03qq65cxe99jfXdGeVLtT8A4q+b+mI9Rg1fCqqSMMa8Tui3CP40B/ivMvkeBRxOQtZgNa9kCQF7HYToyihjR+B4AJa07aMoeHfV4h7cLMHgdmQBMPfgXquv/yYnMYewtOzdh+R6MRhxfRUPeJDozCqMnjud1m9bi8Hb2/A5ORhPrlrFtxAcBq6Sh0oOJfJuNKx3pHRPr6VD83gvRq6Yu2ngbH1h7I+VN6wBwmk4AHN7QJYyEMd6Y2yT8e+8MlMvdyoJdD3LGznA1lPEV/IdUfGIX52/6bxZt+Zrtc9j5/cYm+h93olqNSk/sCLnd5bXb80nrKZMtw91qI5UGjCFhzr7HIvaOyfJ1fcz2dbubuze1BaYla67njB2x9XQa0/B63K7v8D3V5nbWx+2csSroOEhRDFWJYqetKs53+JITO+N7wijO2XaPzZSDuwNEOpo4yNpINGD0h42n9Kqjr7F4wy0Ut4Z+qkuV4c3rbKVzDKDKoujE7rDtM6kUW1tDYmW7mxh39N8h9wUG6dieHqN9xkj7k1m1oeyZcuhpG6m0hDHkdbd7FLbv99vq+2Md5F1VAaYd+FO/jstvP8iirXcxY/8f4pyj6IbG7S6xv/vxR/4VcX9uR52t80Tr+69OThow+iXEH33YIDD4g0Mozn6WEDJ9o4yjjU5NiqAIki5P0Pkdh/p97Kz9/xfHnKjBwKTDOIx01F2cH9sQZTCVMZHDRBJ/welmyepr+z0GJlFVUiMbV8aUi4GKR7frUMHTVnuNOqlpwIgXvyAwof6Ffp0ir+NwvHKTVBPq/smS1ddS0FZr74AwpbFzt9zFnD2PRD3c6e2KJXsDFD3IdE+dEm/FJ3Ym9SY+mNp4lH05XcmbxFsDRpz4j9YdGe0JMMwNs/J4LE+qg0OGu7mnmuP8LV8Z0LmK23b3lN5GHntrwHmLh2T+Mforbt3JuVu/QW7X0YReJz+gjU2pyDRgxMLmA5jtJ7U0qJo6dc/DAe9tffYQn9sRVGo4fffPB5Sv7twMVKRpO/rH3ncjJ8GBorJpNdldxyhr2dazLV3aeFTiaMBIGBt/fH1KGkPvDzbL3Rz7QSFKWNMP/DEOuYms6mj8xpSkg/k7fxxTkEhuVaAajDRgpELYkkX86pCHNW9iQl30tpSKxrWDoiost/NI2H2Xr70xLtcY2/BqXM7TXzkRPmMfSeh6neluZe4++4NJCzqir/mh0psGjJjE/kccstEyCTeDs2ruZ9b+30dNt3DnA8zf9dOE5ydAjFVxLm9nwPvT9vxPmJSDu9H24o232U6bjLJmXqe9MRlKddOAEYORje8iMc7/NHP//8aQeuhVSfVLyIBp/2Zf2bjKZsrB+O9pN0+DO/ipk1NSlmgdikL1dZ9Q/yIesTHbqd8NMfjpOEAaNHpHZQz5HQdpyR6Z6pwEyO46hhdn0mfNtR8INGCoGLTUQ37il4bWEkaMYqqHDiLBT9ZJqJpy2Zrtsv/CNZqWtW5j/o4fMabhVS7YfEfgQjq+QDmu/iW/sRvxCZ4jjq8iuzP60imXbLh1UK/WN/nQs6nOghpKktQhQQNGKgygZJHfvj+mCQ0vX/+f/b6WHZG60VY2re6ZIiS/3a/B1BcoZ9c+znlxXqpzwa4HOXvbvQM4w+B4si9q35fqLCjVhwaMRIkUFAZQsrhg852cu+2bfc4XPI5h0PD/rCH+TXoDTvxu1N2D3QbHrT/QoP09KWWDBowYxXX6BN8NNLvzaM/aGf0xoX4ZH1h7w4DOMfbIy/06znY//iS017j6sYRosl286fZUZ0GpfktYwBCRR0WkTkQ2hNn/Zb+1vjeIiEdESn37dovIet++dxOVx0QKHVZCb71k4xdZvOGWmK+R21GPw9vJaN80Grmd/R8dPCfUGs82hFvFLdHKmrek5LrpzGm09KMiS2QJ4zFgcbidxpjvG2PmGGPmAHcC/zbG+LdWnufbPy+BeUytgTR6Gy8Xbbqdebsfil9+/JzIKI3LecYffSUu5wn2vprvxO1cI47b7aab3iINnlQKEhgwjDGvAtG7q1iuBp5IVF4Gj8jVMpGqdyqPvROwqE33WtPDG/1X0Itfddn+kjMi7i9oq015fbwjUpdlwE7Pq1ENK1iw68HeI4bA4lZKpUrK2zBEJBerJPKU32YDvCAiq0RkaWpylghBN6MY6vWrjr0R4azJHc+R1dXI+Vu+wux9v4nL+ebamNI8lPk7H4yeKIp5e34R8H5MiqcPUWowGwwD9z4AvBFUHXWWMeaAiFQAL4rIFl+JpQ9fQFkKcFplyuOfnzjfxMM8+DrwxLwCW/Xh56ipuCxKwAr/pO3ytAFQ2rrd9jVn73uMxpwxIfeNaXiNg0Wn2j5Xt+HN62M+JpocG2M4lBps3lyxgs6ssoRfZzAEjKsIqo4yxhzw/b9ORP4KzAdCBgxjzMPAwwDzRjoHRX2C3Z5UTk9Hv9d9KPcbCJfpiW1w3owDf+B47jgyPCf6de0eMVbfVB5PXv+F/laXTaxfFuecKJV4Zy5cCIWJn00hpQFDRIqAc4Fr/LblAQ5jTLPv9cXAPSnKYh/xWB6z28wDwc029kslC3f8wHbaEY3vUV8wI2Cbw+tm3JHABumsruN0ZBRHP2ECushWNMW3tDDtwB9PjqlXlEqiRHarfQJYAUwRkVoRuUFEbhKRm/ySfQh4wRjj/4g8HHhdRNYC7wDPGWP+mah8DiYub1vAe/G6Gd64OubzZHY1BbxfsPPHnLLv8RApTZ90PdeO+ap2hC+RxLtLZ3X9soGXoJQaKryepFwmYSUMY8zVNtI8htX91n/bTmB2YnKVTLHfcvM6Aqebnn7gj1TX/5PtFZfjwP4X4qIQ02jndkSfyjrT3RQ1jb/+BhVdO1qpOKvfAsVVCb/MYGolTnvZ7saeBuNQgvvBd69XMKnuOYY3rQt1SEguE627qSViF9IQ+yb5JsTT271Sg01yql81YCRAyYmdjAszYC27K3wvnLLWbUFb+nFrDhcEgr5PIxv7NkDndR7h9F0/Dbvmx/SDfwLAEWpRqFiyOCjXqVBKRaMBI8nEZgyoPvwclTG3XxjyOg7bSlnSWhNy+8jjKylu2xPx2As2/3eM+QqkVVJKxVmSnsEGQ7daFcKMA3+I+Zhztt3LntJzYjgi9I3b6e0Iu8/O8eFouUKpREnOX5cGjKQzjGl4PWFnH2tzpHJh+35gf8znjzYjbomNtTq0SkqpoUmrpFJgsM8KGumGPvXgU2H3QZSSka99RTBJWW1QqZNGksYcacBIsjNrvpvqLEQlEUOGlg6UGnw0YKSlbHdjqrMwQP5fzFhLCSbMa6XUgGgJQ6WKVb6IZQkoe6p1nialhjQNGKqPSN1ey2KYoVYplSxawlCDUGF77YDPEevsukqpKLRKSsVVDHVJBmFi/QtR02nzt1InFw0Yqt/EJGeGTKVUNFrCUPEUw/fJ7tQdOV3H+pkZWLLm+n4fq5QKolVS6mQVbvJDpVRqacBQfaR66o4Prv10Sq+v1NCjJQyVIjqbrFJDjFZJKaWUGkw0YKg+Ul0lpZQanBIWMETkURGpE5ENYfYvEpFGEVnj+7nLb99iEdkqIjUicofdax4ypfHI+klPq6SUUqEksoTxGLA4SprXjDFzfD/3AIiIE3gIuBSYDlwtItPtXLCNrAFkN705tOeRUmlsiLdhGGNeBcIvYB3efKDGGLPTGNMJPAkssXNgK9n9uNzJobB9n+20WiWllAol1SvuLRSRtcAB4EvGmI3AKMD/7lYLLAh3AhFZCiwFyBxRncCsDm0ub6fttLmdRxKYE6VUNM/MfjSm7uWrV6+mcVdHAnNkSWXAeA8Ya4xpEZHLgL8BkwhdtgpbqW6MeRh4GCCrcpJWvsfBqXsfSXUWlDqpGUf4W3NrZgV5nXUB2+bOnQtjFyY6W6nrJWWMaTLGtPhePw9kiMgwrBJFlV/S0VglEKWUOuk15PXWpBzNm2S9SPdxGCIyQsT6lCIy35eXo8BKYJKIjBeRTOAq4JlU5VMplX684kx1FuJiR4WvX9GwyUm5XiK71T4BrACmiEitiNwgIjeJyE2+JB8BNvjaMH4CXGUsbuBmYBmwGfijr21DKaUGbNewC9g48uqe9+tHfTzh13x6zmO203a4CsLu8xIY6A4Wn87Tcx+H3OQMKUhYG4Yx5uoo+38G/CzMvueB5xORL6XUyc6wa9j5zNr/ewDczhy2V1zOpLrnEndJsf9svr3i8rD73pp4O1UNr8cjR/2iI72VUiefoCp/tzN5XfLbMkoC3rdkjaA1s5wDRacBkQfOtmSPYNPI/0ho/iJJu4Dx55KboidSSqW9rSPCDN8yBv+IcbhwdnIy5PPCzAcD3i+fci8vzXiA1qyKgO2dztye120ZvVVO7Zllic1gBGkXMFokJ9VZUEql2NbhH+RA8em20nZkFMflmpsq/4PtFZcO4AyBxZ6j+dP6pEj1oNpUD9yLu9WuuVyf6kwopVJqR8VicrpCTzQhhLrxDnwI1/YRHwBg97ALGXn8HWYc+EPUY9aOvg6PM/SURqvH3EDe9kNsHXEFYjycuucROl2FA87nQKRdwOjwpl2hSSkVRZcjmwxvu98WifA03jc4NOaMjVteTmSV43Fk2kq7u/yCEFut/HW58nll2n09W/eXnhmQqiVrRL/z2F9pFTAEaPekOhdKqWRbNvOnvH/dZ/y2GJqzR4VJ3TdgHC6ay4vTv8/EumVMOPJSn/2rxn6W7K7j7ClbREF7LWdv/3aUHMVeYtlVfiHDm9ayr/R9UdM+M+c3KameSqvHcRHo0ICh1IA1ZleF3B48DmCwCFmtE6Yr68GieSG3n8gazvrR1/D8rIf67GvLKKNm+OV0ufJoyJ/Cq5O+HjE/0o8arrbMYbwy7T5bbSpGnDF11Y2XtAoYDjRgKBUPx/MmhNy+euyNScvDq5Pv5l/T7u/n0dbT94mM3h5F24Z/gKfnPk5d0eye/bvKzgs6zEGXq4CNIz8WsLk5J7C0cix/UpTrWxHjYNGpA/gMg09aBQyvgXUNvcU0t0PXx1CqP3aUX8KbE78UsC1a6WL1mBvimodjeRNpyR5JS2ZF2DR7Ss8JmFupm/E9fb8480dsrvxI3wNF+PvsR1hXdV3I89YM7x089/Tcx+mMMPo6lL1l53Cg+HTWVH2aluyRACyfcg+vV98Z03kGm/QKGIDbCF0Oq2vtc7N11lWl+qMluzLkdjHh61r2lp3Le2M+E3Z/LA75jY3ofgSsLTkDj2QEpFsz9kZem3xXwLY3J34Zt7O3e324HHscWQOq1nl18t1h97mdOawc/3k6M3p7NTXmjuNogdVV1iMZ1Jac0e9rp0paBYxur0z9Fm9N+GKqs6HUkGU1qAY1qtpoY91Xdrat83f5DUoLZVNAlZAXgM2V/8FbE28HwC2ZrKn6VMhj6wtn2cpDNFtHLIlYujmWN9Gax6kf/j7n16wa97n+Zi1l0qqXVHf02085bUXlKc2LUkNfTEvTxKTLmUuG54SttB6HNW2H8ZvC+1jeRPYMOy/cIWHElvctlR9mS+WHo6Z7b8xneqrA0l1afcp8X2m1Uxu+lRqg0MWJcPMcNeROtHXWZl99figvzPhhz+uW7N4xBm9NvI2NIz9Km18DdqT5lpJtX9nZ1JaelepsJEVaBQynry/bzmZdk1oNTXUFM1OdBYvf0/zRvNBrLRzLtXpS1ZacweuTv2brtAfCdGndXzyftsxhABzPGYeR3sqPtsxh1Ax/P0ikwXjQmlnOplAN3Cpu0qpKyuULf3/Y6eTM4e7UZkapfuhyDsa50Izvv4J/tc6rU74R85milQz+OfMnAQ3W0fLk76UZD4S9qoqPtCphZPl6/ZVl9b+42hR2dKhSJ5fuNgOvuOh05lldUCP0korp3EHvG3xLjXZkFFu9l8LmyeE7fnAOIEx3aRUwBMh3GaYWD+RLrU8jKjHq86dH3G+NJxhM37/evPzjlF+wt+zcmI5+a8IXeXPil8Pu754LacWE29lZfrGtczbkTWJ7xWW8N3ap7XzsL5mPQdh3krQzJFJaBQyAFrfwxuH+f6xUTx+s0lfwegfB4vXde3ru42wY2XfBy9Vjwo/S3hHhhu1fjdT9eo+N4HG4cA71hbN62jqCvTbpa7w+6SvWyGux+dnFwaZRV9GeaX9J0hNZw3lm7m9p0dqDAUvkmt6PikidiGwIs/8TIrLO9/OmiMz227dbRNaLyBoReXcg+Vg248csm/Fj2+kbog75VypRBhYw1lRdz3HfrKs7hl/aZ4zA3rJzwh67v3hBn4AVKYDZKsP7gkD3SOz6/Gns85txtTOjkKP5U+2cSQ0SiSxhPAYsjrB/F3CuMeYU4F7g4aD95xlj5hhjQnersKk9s5SOoDnkO515LJ/yzT5pa8ovYdewiwZyOaUiiHybNRGn5I5ub9m5/HvqvRHTdK/itq+kb/XMM3N/GzZnfV+Hz+f+4vls9hu/sHHUVSyf8k3enHQn3qCR2mpoSVjAMMa8CoRewcTa/6Yx5pjv7VvA6Hhcd1i29YVu8+skFapnRmuIEZxWMXfw9O9WqfXi9B/E7VyvTfpq9FAQJcE742+JuD9SsHll6rcCLnEkf0rIdK9X38lL074bNkMS4lWwd8ffzDa/5VGNuGjMHR82vRo6Bku32huAf/i9N8ALImKAXxpjgksfPURkKbAUoLy8HHe79UX+0y4H107yhr2g25XHtuEfYPLhZwOvqgHjpNDhzCfL0xIlVfzas47njmfM0deiXq/LlRdyz4qJX6Ku8BRWj7mBuXt/HfP1m3LGBLw/WHwac/c92vO+3TeldvdcRwH8/iS8vvERbpsLBIWnbYXxtHz58qRcJ+UBQ0TOwwoY/quGnGWMOSAiFcCLIrLFV2LpwxdMHgaorq42V4z38OddTo62934hvY4MNo+4kmmH/hJ4bdM3oAymEaQqcdoyS8lqixwwjN2GWBu8jkz877xH8ybRnD0Sp7eTqmMrrOshbBx5FS1ZIyhq28OYhjd60tcVngJY1U7hA4aN/JruMRW9lQsvzPhhz6C5UKfz/5vYV3omOZ1HqKm4LPq1Qmegn8epSBYtWpSU66Q0YIjIKcCvgEuNMUe7txtjDvj+XycifwXmAyEDRrBTSg1/3gW7WwL/eLZVXtEnYPhPP9BDv88qRrvLFlHaWkNhe23A9sbsKora97Gp8j8CtjdnVfL6ZGsBHoe3ky5nHhOOvESXKx+PM5udFYsR42bTyKtYvOHzYa+7bMaPcXo7OG3PLyk5sTNkT6P1oz4eempuv7QhgwXQnGVN47F72Pk924y42Fp5Zdg8qfSWsm61IjIG+AvwSWPMNr/teSJS0P0auBgI2dMqlLwYQuDe0qBeIwJC+GoslR4OFs5l24groqaLNIDM38ZRV3PYVwLwt3mkFSiO5ltTa3TfomuG9z6dex2ZrB/9SdaPuoY1VZ/u2W7ERUdGUcTrtmeW0ppdyZsTv8y/J38jZJqdFYsD5jlaV3Ud7a7Cns8WaWxIZ0YhT899nNqgtaTVySthJQwReQJYBAwTkVrgbiADwBjzP8BdQBnwc7Gedty+HlHDgb/6trmA/zPG/NPudV1+IdBrwBGplB70RKY9OE4Ox/MmcLA4euc7r7h4dvavOGPHA5S3bA6Z5nDhKbidOWypvJJJdc8H7iuaw3On/NJvqou+1UEAiLCzwt7ANbAm+is9saPnvduVx3FX6LEOwWpLz+wJAH8/5WG8jlRVMmgbxlCUsG+LMabvyKHA/TcCfUYSGWN2ArP7HhG7t+uEhcNjq2PSEkb62+63mlokBvG1PYS+uTVmV/HuuP8CrJLC4YJTGN68LiBN4LxI8anvXFH9/8juOhY9YRQeZ3YcchPjNX0lm6YcHUQ3FKW80TsRHBi8CE/udLIwhkkIjTh6GgWP5U6w6oXVkLe94nIm1T3X895/JtRIjISer6iuYAZvT7gNrzhjWrFN4tQ+5nbm0DIoJymMriOjmDeq7+C4drMdktJuahCA208JvSDGiqA1ioPtLX0fTTlVHM2bzLrR14Yc3KeSzzvAr6nHrwtoW4b9KSW8YQIGCF5HRp9gcaQg2qjl6IPeTgZHCqbbnJFWDTZpGTBKw7RV1hdMx4uDjaOu6rOvw1WA15GJ15HJ65O/xvG8CbQH3Vwag/qyq+QI9TT6evVXYjiDdaPeMuIKXphpf5qY7hv78dxxtlLXVFwesAhQsENFc3znGxtDHqA1TC8mpZItLQNGpt+nqm3tfW3ExbNzHwuYdXNPqbUGcbQeMe+NsT87poqvtqCJ5rZXXM7REE/zGwPWge61Z9h5HM8ZZ3tJz+asSuuFr1NEd2+nbq1Zw0MfKBK2iyrAgZIz+PvsR2jOqbKVD4BnZ/+Kf037nu30SiVSWgYM/55Sf9kVed78rd3dK0PM8++/Tu++svf12a+SY43fLKsNuRPZNCp0YAi1+NCJjDLaM0r499R7aM8osXW9leM/z7KZD/a892/LWDHhdjaMitifIyK7XXW7eR2ZmJT1ZFIqUFoGDIBy35xSO6Is1+p1WF1pT4R4Mgw54EnFxZtR2pP8BdR3hxl9vXzKPQPqFt2cVUm7yxr34HFk9AkuL8z4Ia9N+hp1RbN9PaeUOvmkbcD4wJje7rFNneHTdWQU8864m1k5IfLEbmDNctutrmAmHXEMKEdOsmme60MMdIvkQPHpEfc32mxnCOflafdHfPpvyxxGQ37ota2VOlmkbVl3ZmlvFdPBNqEwM3yfxoMl88PuWz7lm3Q58wFYNe5zjD72JjmdDdRCu+1YAAAgAElEQVRUXIrbmcPl6z4bnwzHaenLoWx/8XxGHX8n5L6a8sWMPL4ycRcX4Z0JtzCxbhknMssTdx2lhrC0LWE4/WouDp7o/3kac8dzIsu6gXRkFLGj4lI2jP4E7ZmlfboG1ueHmOnTT5cjfFfCoT5g0M4KbNHUF8wIuy8Zk0I25Yxh9djPxDS2QqmTyUnxl/HX3YlfMP6l6d+PugTnG5PuSMi1e3r12LR+1MdZPuWb7PKbVM6O7RXWCOngyfQADhXOCXnMrjJ7PZPsCrXmQ/DaJvuL50ddPzuRVo39LP+adn/Krq9UoqR1wLj71N5R3u4EP8CH7WqJ1VMHIjeidz9BR1p3OZyW7JER93d3He62s2Ixjbnj2TKi76yj3Ut8Bnt18t30zIVkY9rvXWXnsb94PuvGfCpsmudnPcRzs34R9Vxgjbw/UHQaa/0m6HvulP/h+Vk/56UZgQsdeRwZvDfW6gbtTsH0F7WlZ0X9nSg1FKVtGwYEDuC7/W0XDy60P02IXa9Ovou8jjqgd/xuS2YF+0vOoCOjmGO5Eyhv3sj0g3+i09cW4m/jyI8y9uirnMgoo5Safi3RGb2yxv45w02HcSxvIpU97QvCigm3s3DnA4D1eYNFChTdunwBdHfZudQVzibT3dyz75Up3+K8rV/rzZfDxcoJtwYc7/YtN9r73goOnc582jNL2TDyKg5EaJ9SSsUmrQNGsBWHhTMqTLiemf1yLK+aY75F7rttH/5+9g5b1PP+eN4Eto/4AGBNTZHT1btybX3BTGqGv59Td/8ypuv6z1ga6uNsHPkxHKaLaQf/ErYrarcOVwFZvpv1nrJzA+bQ2jXs/J6R1sdzrRlRm7NH0eXqvVnvrLg4YhvDa5O+SlHbHk6p/X3I/WvH3ADA2COv9Gxryo19VP3BonmsrbqevaXWmJkdw/u7yI9SKhRbVVIiMlFEsnyvF4nILSJSnNisxceHxvXOK/XkTidrG1I7j4/b13XzzYlfYtvw99PYUwXUO8/QvpKFbPFbr6EpexTPzv4VbX5jA9zOHPYXzw861tLpzKNm+OV0+MYVhLuVd7ryOVQ4h5Xjehfp2TPsvID6/3VV1/eMjD9QfDovT72Pw0Wh2it6r7Jz2IUBexryp7Cr/OKoU1x0l64OFs0FYG/p2aytuj7iMQFE2D3sfNvjJMJVvymlQrPbhvEU4BGRauDXwHjg/xKWqzg6Z0Tg7bIt/rVSMenuDXUis5zNIz/a8/TfnG1N99yWWcp74/6zz6pm/jfBHeWXsGrsTRwutGaBb8oeHfWqoTc7eHvibSGn2QiXvrl7WuqgKNS91vO+kjNZX3VtyMOXT/22rct0ugoBWD32MwGrvcXb65O+RmtmOQ1BJUSlVGh2q6S8xhi3iHwI+LEx5qcisjqRGYsXh8CwbMMR3xrfCe2caWMsRfc64iao6+b24ZdzNH8yDflTera9UX0HZ9Xc73esdf6aikvpzChkX9nZ1BXOovjE7nAZ6nm1ftTHmbU/fIzfPOJKyls2Rs1/b2YC3x4unM3mEVeyq/yisIfYnqE0SWNSPM4sXprxQFKupVQ6sFvC6BKRq4HrgL/7tg2Z5elun9VbLbUrylQhcRGhzaA3YAQ1LosjIFgAIUaSmz7n78iwUzMo7KxYzPGcMT1VYsG2VV7BG5O+CsCOiksAIj55H8ud2Cf/2yqvoMuVF/oAn1Vjb+KVKfeGyebJPe23UoOd3YDxKWAh8G1jzC4RGQ+EbsEchHL9ylHv1CeuJ/Hucqv6pK5gZtg0q8d+hoa8atpt3Oi72yC6lxM95KvbdzvsdRXtvv12P6//e+q3eG72I1GPO1w0l3/M/FnkKcTFwaqxNwHQHLVKrFdt6Zk0hZneu/vfbbfNWWWVUsllq0rKGLMJuAVAREqAAmNM1JFJIvIo8H6gzhjT5y4q1sLdDwKXASeA640x7/n2XQd096v8ljHmt3byGs75lV5ePmgFizVHhTll8a/2OJ47gafnPh4xzZGC6bxWcJet83VmFPL8rJ/T5es+uq7qWraOuKJP1U7frrjWZ+seWd6ZUWjresHXjqa29EyO5k+OOKV3LNozy6L++ymlUsduL6nlIlIoIqXAWuA3IhJ+pZhejwGLI+y/FJjk+1kK/MJ3vVLgbmABMB+42xeo+u0DY3tH7v1mm5PjHQM5W/J0ufJ7pqow4qI9M/yKcQ1B1UT7SxaweswNbB2+JGH5i1ewUEoNfnbrZ4qMMU3AlcBvjDGnARdGOQZjzKtAQ4QkS4DHjeUtoFhEKoFLgBeNMQ3GmGPAi0QOPFE5BCYU9JYq7n4v/YagdC9Fur9kobVBHOwtO1fXU1BKxYXdO4nLdyP/KPDVOF5/FLDP732tb1u47X2IyFKs0gnl5ZFnGf3IeA/fW5eGN09fY7ERJ8+d8j+22ziUUulh+fLlSbmO3bvnPcAy4A1jzEoRmQBsj8P1Q3WLMRG2991ozMPAwwDV1dURGyZGBXXguXVFYqYLSbbunk8drsI+02UopdLfokWLknIdW1VSxpg/GWNOMcb8p+/9TmPMh+Nw/VrAf4Hj0cCBCNsH7JYZgQFiw7Gh35WzIW8ya6o+xbqq61KdFaVUGrPb6D1aRP4qInUiclhEnhIR+30pw3sGuFYsZwCNxpiDWKWZi0WkxNfYfbFv24BNDOr888iWxE99nnAi7Bl2nv2BcUop1Q92G71/g3VzH4nVlvCsb1tEIvIEsAKYIiK1InKDiNwkIjf5kjwP7ARqgEeAzwEYYxqAe4GVvp97fNviojQrsObq1hUu6tridXallEpPdtswyo0x/gHiMRH5QrSDjDFXR9lvgP8Ks+9R4FGb+YvJh8Z5eXSrI2D8wrfXpEd7hlJKJYrdEsYREblGRJy+n2uAo4nMWCKdUmr48UJPn+2eob1KqlJKJZTdgPFprC61h4CDwEewpgsZ0haPDgwaP9yQBu0ZSimVIHZ7Se01xnzQGFNujKkwxlyBNYhvSLtwVGBbRm2rDJkR4EoplWwDmYnvtrjlIkUyHPDteYHtFne/5+JIe4oypJRSg9hAAsbQH8AA5GdAfkZgSeOH67VqSimlgg0kYCRnlZsk+MapgW0ZrW6hpilFmVFKqUEqYsAQkWYRaQrx04w1JiMtZDjg7lMDq6Z+utGVFqPAlVIqXiIGDGNMgTGmMMRPgTEmrWbxK82Cz08PDBqPbHHS2JmiDCml1CCTuOXnhqDqIrh+cmD11F2rXNS2pihDSik1iGjACDI3xEp831/nYnujVk8ppU5uGjBC+P78vlOE/GyTkzY3eNOmqV8ppWKjASOETCf86Iy+QeOOlS4e3ar/ZEqpk5Pe/cJwCHxnXt+gsf6Y/pMppU5OeveLIC+DkDPY3rrCxa0rXHT2nb9QKaXSlgYMG+45LfS05zqFiFLqZKIBw4aiTPjqnL5B47vrrMF9v9jk0NKGUirtacCwqSIHPlndNyo8ssXJlkYHz+510NIFR9vBretqKKXSUFqN1k6004YZDrd5eWF/3zj76iEHrx6yti8o9/Lxao0aSqn0ktAShogsFpGtIlIjIneE2P8jEVnj+9kmIsf99nn89j2TyHzaJQKXj/Hy3RDjNPxt0UF+Sqk0lLAShog4gYeAi4BaYKWIPGOM2dSdxhjzRb/0nwfm+p2izRgzJ1H5G4hsp1U99bua0NOgd3lhXwtU5Sc5Y0oplUCJLGHMB2qMMTuNMZ3Ak8CSCOmvBp5IYH7ial654ZpqD0WZfYd+n3ALP1jvoq4tBRlTSqkESWQbxihgn9/7WmBBqIQiMhYYD7zstzlbRN4F3MD9xpi/hTl2KbAUoLy8PA7Ztu/0csPp5R7uW+PkUFvfaqhvr3Fx9ggvF43yUpSZ1KwppU4iy5cvT8p1EhkwQlXkh5uJ6Srgz8YY/25IY4wxB0RkAvCyiKw3xuzoc0JjHgYeBqiurk7JTE93zvHw7F4HL4VoDH/tkIPXDjm4cYqH8QWG/IwUZFApldYWLVqUlOskskqqFqjyez8aOBAm7VUEVUcZYw74/r8TWE5g+8ag84ExXv57dvjG8F9tdfLVd3W9cKXU0JXIgLESmCQi40UkEyso9OntJCJTgBJghd+2EhHJ8r0eBpwFbAo+drAZmQs3TYs8gu/e1S6+sUrXDFdKDT0JCxjGGDdwM7AM2Az80RizUUTuEZEP+iW9GnjSGONfnTQNeFdE1gKvYLVhDPqAATCt2PDgQjdzy8KPwzjWKTy61cFGXQJWKTWESOB9emirrq42DzzwQKqzEeDWFZGbiT43zUNplsFjYERukjKllEorS5ZE6oAamYisMsbMs5NWpwZJsNtmuSnNMnwuTFXVzzc7+dYaF/etdbGrOcmZU0qpGGjASLCx+XD3qR6mFBt+sCDyCPEfb3DxVp2wvkGrqpRSg48GjCTKcMBVEyI3ij+xw8mvtjp5t1504J9SalDRyQeTbOFww/xyN49vd7CmIXy87p525OMTPRzvhNmlRts4lFIppQEjBZwO+NQUL+/UGVYdEbY0hg8c/7fDChzP74Obp3twiKEqz1p3XCmlkkkDRgrNrzDMrzD8Y5/hn7XRI8DPNllpKnMMd8zRFZuUUsmlbRiDwKVV1tiNqjx7XZwPtknPuuLv1msDuVIqObSEMYh8fobHWrHPwAPr7f1qflfj5K06L1eO8zIyD2pboalTmF6SPuNrlFKDgwaMQSTLCSPzrNcPLnSz6ojw+PboVVXbmxx8d52DuWVeVh+1Co1XjPVw3kgNGkqp+NEqqUFspq+UUJZl78bfHSwA/rbHyffWasu4Uip+tIQxiGU54dOTrWnR69phX4vwtz32g8D+E8I3Vjm5frKHsfmw8ohQ4IJpWl2llOoHDRiD3Owy6+ZemAnVhYZTSt3cs9r+r+1Yp/CjDS6yHIYOr9VA/sMFbu5b62RWiWHJuPCTJCqllD8NGENMWTZ8fa6b4kxrharb3rb3K+wOFvgd8/JBYWqxYUqxljiUUtFpG8YQNCwbXA5rAOD35rs5r7L/pYSfb3ayvkHwGmjtomeBp/2tkEYTGSul4kBLGENclhOuGOdldJ7hcJuwoMJLpgO+vsr+r/ZXWwPbRSYWGHY0C9dUe5g3zLD+mDCzxODQIR9KndQ0YKSJeeUG/yXT75prtXUsKPfydn1sBckdzVZk+H2Nk7/vNRzvtN5fPdHD5CJDaRa8UCvMLDWM1PmtlDppaMBIU2XZ8OMz3IjAkrFeHtnqpLYFukxsxYTuYAHWTLqZDkOnrz3kuX3wxZluKnPhWEfoBaA8Xlh+UDin0pChFaBKDWkaMNKY+O71eRnwhZnW3FMdHvh/7/T/197pDQw4P9rQe64fneEOqLZ6/ZCwos5BbavQ6fVwaZU2iig1lCX0mU9EFovIVhGpEZE7Quy/XkTqRWSN7+dGv33Xich23891icznySTLCV+a5eZLs9zMKvHy4zPcPLjQzU1TPcwvH1gX259vcrCyXtjfajWe/2mXk9pWK4JsPm591X5f4+CRLVrUUGooSlgJQ0ScwEPARUAtsFJEnjHGbApK+gdjzM1Bx5YCdwPzsCrmV/mOPZao/J5MqvKt/984tTdATCsxTCsxXDnOyx0r+/e12N7kYHtT6H17WoTaVljpa0/p9HijTtFe0wh17cKZw7VkotRgkMhHvflAjTFmpzGmE3gSsLtS+SXAi8aYBl+QeBFYnKB8Kj85Lmseqztnu/n4xPhOof79db2B6E+7on/1frrJxR926vQmSg0WiWzDGAXs83tfCywIke7DInIOsA34ojFmX5hjR4W6iIgsBZYClJeXxyHbCqwG7IocQ2Onh3MrDbubhdcPC+sirBIYi3fqHbxT7+D9Yzx0eIT55V7cXtjSKJwfNGnirStcfPNUN8VZcbm0Umln+fLlSblOIgNGqO44wXULzwJPGGM6ROQm4LfA+TaPtTYa8zDwMEB1dbXWXcSRQ+Di0dY/6RTfiPBDJ7yccFtdbo92DHxgxt/3WiWIF/f3BqKSLA9TigJ/lTVNwtwyw9EOqMixtjV3Qb6rt3FfqZPVokWLknKdRAaMWqDK7/1o4IB/AmPMUb+3jwDf9Tt2UdCxy+OeQxWz7q6zd53qobET8jOses0vvBW/r9Jj2/pWQ/2uxsnvaqzXF4z0cnq5l/vXushzGe6c46Egw9rX2mVVq/n31trXApW51uh4pVT/JTJgrAQmich4YD9wFfBx/wQiUmmMOeh7+0Fgs+/1MuA7IlLie38xcGcC86r6oSiz9/W9p7nZ2igIMCLXUNcmbDgmHOsQdjbHtwjwrwMODp6wXre6ha+962JcvmFcgWH5QQeLKr1cMtqLU6DdAz9Y72J+uZdPVOtEi0oNRMIChjHGLSI3Y938ncCjxpiNInIP8K4x5hngFhH5IOAGGoDrfcc2iMi9WEEH4B5jTEOi8qoGrjATTi/vrUYanWc4dVhgtdI/9jn4Z218HvM3HQ88z+4WYXeLFZiWH3Sw/GDg/nfqHZRmwanDrEAyLDsu2VDqpCImjWaYq66uNg888ECqs6EicHutUeENHcKrh4QNx1JTT/SZKR5mlvZ+91u7rLaQXBfUt0F5TkqypVS/LFlitwNqXyKyyhgzz05aHemtksrlsG7G5Tnd06r3VhN98S0n3hinLumvR7Y6qcwxLBrp5YkdvW0mn5vu4eebnHxkvIc/73KydKqHGb4Fp/6vxhqY6EX4zjw3xzrhTzud/Nd0T9QxJUqlAw0YatC4fpKXR7c5+exUD2PzDVlOq7tcqxt+ucVJvsuwpTF+JZKDbRIQLACe22ud/y+7rf8/vCV0JDhwQvjZJmvfl99xcfowL1eO93LnShefrPYwr9zgMbCzSZhUlD6leHVy04ChBo3ZZYa7T3VTGjTeojATvnxK9yBCL8c74K+7HSwa6WV3c2zL1kazx9cOEq2kExwCVh5xsPKIFWR+V+PkvaNeRuXCC/sd3DLDzcTC3rQn3Fa13Ki8yHlpc1s9vpQaLLSjoRpUgoNFKMVZ8KkpXsYXwHkjDbfMcHO63zxY102K7wj1UB7aFDlIbTzm4AXf2JJGvxl/3V64c6WL761zRVyg6q064Y6VLlYd0UEmavDQ5xc15E0shImFXi4c6eXNww7mlhlOHebm1hWD4+v92+1Ofru97/bf1Ti4eJSX/Az46rsuTi3zUpIFl4zubVd5bq+D04YNLAA2d1mrJxZmRk+rVCSD4y9KqTgYkQtXju8taXxuuoeiDMOIXNjbYo0Wb2gXLhrt5e06wSnw0gEHJ9ypeYpfdcTBqiO9hfz3jlqv/3Wgd9vRDuGnGx0caRe6vHBqmeH9Y7xk+/3l1rdBlxdG+qq4tjYKRRmG+nbBIb3tMA8udPccs+aoUJplGJOfwA+o0o4GDJW2/KcXGZMPY/J7VyXsnvLkglGBT+87mqypRx7a5OTgicFRHVTT1BtAXjssvHbYer90qiegUf4789x4DPw8SnUZwG+29Q0iSkWjAUMpP92N03fM9vDvg8KsUoMDuPs960/lO/PcHGqDn2xM/Z9OcA+ur7wbOU+3rnD1CRBNnVbD+p5ma62UKi1xqAhS/61XapA6t7K3hPKDBW6cYs1RNTHDWjM9P8O6yXq8cNvbQ+NP6dYVLs6v7K22+/qqwHw/uNCNMbDpuDCt2LDqiPD7GiffONUqvTyzx8E11X3XMgk1h1ewpk7wGuI663Brl7Uo18ervT3zianEGRrfcqVSLHg98jK/qUWcDvjCTDerjjgYkWMozoT8DENFDmw8JswqMexpFWoapafnVCq9fDB8Hh7a5GBbiLEu33iv91ax9p3Qx48vMEwt8jKv3Go/mVZsaOmCNw8LM0oM3/Oth3LnbDcjcq0eYx0eawnhbu1ua/6vSEHlcJs1S3FeBrxZJ2w67uC32+DmGTpXWKLp1CBKJdEJN+xtEYoyDS1dgsdAQ4d14+weT/KhcR7+unvoDx1fMtbDP/Y5+qwDDzC/3Ms7vtUX/avJvrLSSatbIrathOv9dvYILwsrvNS3C3PK0ue+ZkeypgbRgKHUILGn2ZoUMS8DPMYa3LezWXAAJVkmZLvJrBIv61M0H1e8CQbjWwonz2VodQufrPZQ3y7kuAz/2Oeg3WOvI8J9p7t55YCDyUWGSUWGPS3w7B4HVXlw1ghvn8knvQbWNQinlBocYnVDbnVb0/f7O9BqTZU/2NZg0bmklDrJjC3ofd09o+6w7N4HugcXumntstoJMhywvkGYXWZwiJej7VZX2Wf2OrlztptdzcKTQ2x5W+O3blqrr6vz72r69xl+s82qWnthvxV82j3gMcL2JqtKrrsE8/ohYVujMK7A8PQeJzNLvNwwxcsbh4U/73LyiYkeKnIM4wqsHnQ/2ejiw+M8vG+EYUWdML7AMDK397rP7HGw4rBw33yr911NI2S7YHSUUf1DhQYMpYYQ//r+uX7Tx5dlwwWjDBeMsm6EI3INhZnWnFxeY1WFPba9t6uwYLhxipdHtg6toGKXfztMa4hxNi1d8Ng2B9t9XZbX+hZP2HDMwRff6j32f/3mGrvat8b9U7udPLW791xLxnpYVuvgwlHenjE0HR6rQ8RPN1m32HNGeHn1kIN7T3MP6QGUWiWl1EnihBue3+dgfrm3Z8BeY6dV9TUm37rJZTphdzMcaReq8gzfXefitGHengGGGWLoStKMwkPZ+4Z7ef1w36pCpxgeWODhC2+5KMww3Duv7yj+Yx1WVVhwR4tItEpKKRVXuS74yPjAnkRFmb0rJ3ZPdGhNtWI9SHZX3Xyy2ovXWD3CutW1wX1rnSwZ66W2Vbi8ysv6BuHsEYZfbHawNY4zCw81oYIFWNVih9us101dwuZjwtgCQ4fHqs6qbxf2tVoB+Vvz3D1dhY+2W+f84BgvIvCrLQ4KM+GNww4WVnjpf7iIjZYwlFIJZYy15ntZluHrcz18bZWT4ky4cYqH3c1CfTs8ty89q8YGKstp6Ahq6L96oqfPtPzvff0iSvP6V9elJQyl1KAhArfNclOebb3+tl81TEmW9cB6/kg3BqsapsMD/+8dF2PzDVdP9HD/Wus2NSzLcKSj9+Z572nWE/gX3krf21hwsAD6BAuAg41t/Q4YsUjov7SILAYexFrT+1fGmPuD9t8G3Ii1pnc98GljzB7fPg+w3pd0rzHmg4nMq1IqccZGmXLE5VeDk+UMHJtx72luGjpgXIG1RkiX12pryfbdN786x83f9zo4bZhhbL6hKNNqr6lpEmpbhYIMw1N+41qWjPXwtN8aKp+d6iHTafjpIJjupb+6PMmpKUpYlZSIOIFtwEVALbASuNoYs8kvzXnA28aYEyLyn8AiY8zHfPtajDExzWyjVVJKqVDa3XCozQo6AMc7oPaEUJFtjcgHa+T5ynqhLBt2NAkLK7y8WedgWa2DD43z8Pw+BxMLDE1dwlUTPKw+6giYWbggw9DclZoOAZvuuYTczP4FvMFSJTUfqDHG7PRl6klgCdATMIwxr/ilfwu4JoH5UUqdpLJdvcECrKlHirMCH5ZdDlg43No22TfT8WVVXi6rsjoKLKoM7NFUle9ldpmXH6538ZU5bob7As/bdUKrG57e42RRpZcPjfPy9XedNHUJc0q95Lrgzbq+jeKfn+HudyknJyM5bUCJDBijgH1+72uBBRHS3wD8w+99toi8i1Vddb8x5m/xz6JSSvXf2Py+U8QvqLCCTWWuh0m+3mafmerh1UMOPj7Ri0PgYxOtXmdvHBbaPZDlgOpCa1LL9ceEf+xzcOU4L81dcOEow6ETcN9aF5W5hpumerj7PRc3TvHw+iFhS6MDSdLQ80QGjFCfIGT9l4hcA8wDzvXbPMYYc0BEJgAvi8h6Y8yOEMcuBZYClJeXDzzXSikVB9OKA9djuaY6sEuzQ+DsEYG3xLJsWFRp+pRmRuTCDxe4Ed+Myd1BalapAbwsX748IZ8hWCIDRi1Q5fd+NHAgOJGIXAh8FTjXGNPRvd0Yc8D3/50ishyYC/QJGMaYh4GHwWrDiGP+lVJq0HBGGNayaNGipOQhkSNrVgKTRGS8iGQCVwHP+CcQkbnAL4EPGmPq/LaXiEiW7/Uw4Cz82j6UUkolX8JKGMYYt4jcDCzD6lb7qDFmo4jcA7xrjHkG+D6QD/zJVwfX3X12GvBLEfFiBbX7/XtXKaWUSr6Edjw2xjwPPB+07S6/1xeGOe5NYFYi86aUUio2J+9kL0oppWKiAUMppZQtGjCUUkrZogFDKaWULRowlFJK2aIBQymllC0aMJRSStmiAUMppZQtGjCUUkrZogFDKaWULRowlFJK2aIBQymllC0aMJRSStmiAUMppZQtGjCUUkrZogFDKaWULRowlFJK2aIBQymllC0aMJRSStmS0IAhIotFZKuI1IjIHSH2Z4nIH3z73xaRcX777vRt3yoilyQyn0oppaJLWMAQESfwEHApMB24WkSmByW7AThmjKkGfgR813fsdOAqYAawGPi573xKKaVSJJEljPlAjTFmpzGmE3gSWBKUZgnwW9/rPwMXiIj4tj9pjOkwxuwCanznU0oplSKuBJ57FLDP730tsCBcGmOMW0QagTLf9reCjh0V6iIishRY2v3+iiuuGHDGlVJqiFk1gGPH2k2YyIAhIbYZm2nsHGttNOZh4GEAEQmZRiml0pkxZl4yrpPIKqlaoMrv/WjgQLg0IuICioAGm8cqpZRKokQGjJXAJBEZLyKZWI3YzwSleQa4zvf6I8DLxhjj236VrxfVeGAS8E4C86qUUiqKhFVJ+dokbgaWAU7gUWPMRhG5B3jXGPMM8GvgdyJSg1WyuMp37EYR+SOwCXAD/2WM8di5bCI+i1JKKRDrgV4ppZSKTEd6K6WUskUDhlJKKVsS2a02obQLrVJKxexnxpjP9/fgoVzCqAfaUp0JpZQaQhYN5OAhGzCMMRXAX15844wAAAJrSURBVFOdD6WUGkJCzphh15ANGEoppWLWOZCDh3rAOCvVGVBKqSGkbCAHD/WAUZHqDCil1BAyoGUihmzA8E2Dnp3qfCil1BDSMJCDh2y3WsCb6gwopdQQ89RADtapQZRSStkyZKuklFJKJZcGDKWUUrZowFBKKWWLBgyllFK2aMBQSilliwYMpfpBRL4qIhtFZJ2IrBGRBSLyBRHJTXXelEoU7VarVIxEZCHwQ2CRMaZDRIYBmcCbwDxjzJGUZlCpBNEShlKxqwSOGGM6AHwB4iPASOAVEXkFQEQuFpEVIvKeiPxJRPJ923eLyHdF5B3fT7Vv+3+IyAYRWSsir6bmoykVnpYwlIqR78b/OpALvAT8wRjzbxHZja+E4St1/AW41BjTKiL/DWQZY+7xpXvEGPNtEbkW+Kgx5v0ish5YbIzZLyLFxpjjKfmASoWhJQylYmSMaQFOA5ZiLeT1BxG5PijZGcB04A0RWQNcB4z12/+E3/8X+l6/ATwmIp9hgJPEKZUIQ3kuKaVSxhjjAZYDy30lg+uCkgjwojHm6nCnCH5tjLlJRBYAlwNrRGSOMeZofHOuVP9pCUOpGInIFBGZ5LdpDrAHaAYKfNveAs7ya5/IFZHJfsd8zO//K3xpJhpj3jbG3AUcAaoS+DGUipmWMJSKXT7wUxEpBtxADVb11NXAP0TkoDHmPF811RMikuU77mvANt/rLBF5G+uhrbsU8n1fIBLgX8DapHwapWzSRm+lksy/cTzVeVEqFlolpZRSyhYtYSillLJFSxhKKaVs0YChlFLKFg0YSimlbNGAoZRSyhYNGEoppWz5/08m/aSWSi0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd8bfa94dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plot_accuracy(history.history['acc'], history.history['val_acc'],\"Accuracy\")\n",
    "\n",
    "Plot_Loss(history.history['loss'],history.history['val_loss'], \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_resnet = model_resnet.predict(val_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "for i in range(len(predictions_resnet)):\n",
    "    length = len(predictions_resnet[i])\n",
    "    index = np.argmax(predictions_resnet[i])\n",
    "    predictions_resnet[i][index] = 1\n",
    "    for j in range(length):\n",
    "        if(j != index):\n",
    "            predictions_resnet[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bed       0.84      0.65      0.73       150\n",
      "      bench       0.88      0.31      0.46        70\n",
      "    cabinet       0.56      0.58      0.57       150\n",
      "      chair       0.82      0.85      0.83       150\n",
      "      couch       0.52      0.87      0.65       150\n",
      "       lamp       0.72      0.68      0.70       150\n",
      "      plant       0.95      0.79      0.86       150\n",
      "      table       0.72      0.71      0.72       150\n",
      "\n",
      "avg / total       0.74      0.71      0.71      1120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, predictions_resnet, target_names = dirlist_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN : Region based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/experiencor/basic-yolo-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/philipperemy/yolo-9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

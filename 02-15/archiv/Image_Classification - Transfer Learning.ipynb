{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "#### Neural Network for Suggestive CAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TempUser\\Anaconda3\\envs\\CPU_tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "#from tensorflow.python.framework import dtypes\n",
    "\n",
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "import param\n",
    "import time\n",
    "\n",
    "#from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "#import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#import pydotplus as pydot\n",
    "#import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# KERAS IMPORTS\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import History \n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###From https://gist.github.com/ambodi/408301bc5bc07bc5afa8748513ab9477#file-dataset-py-L74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Data from  https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving raw data to npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to directory\n",
    "path_train = 'raw_data/train/'\n",
    "path_test = 'raw_data/test/'\n",
    "path_val = 'raw_data/val/'\n",
    "\n",
    "img_size= 100\n",
    "\n",
    "#get folder names\n",
    "def directory_scan(path):\n",
    "    dirlist = [ item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item)) ]\n",
    "    return dirlist\n",
    "\n",
    "#build labels and image arrays, resize image to 100*100\n",
    "def read_images_in_folder(dirlist, path):\n",
    "    images=[]\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for i in range(len(dirlist)):\n",
    "        image_stack = []\n",
    "        for img in glob.glob(path+dirlist[i]+'/*jpg'):\n",
    "            count=count+1\n",
    "            labels.append(dirlist[i])\n",
    "            IMG = cv2.imread(img)\n",
    "            #RGB to grey scale\n",
    "            #IMG_2= cv2.cvtColor( IMG, cv2.COLOR_RGB2GRAY )\n",
    "            #resize to 100*100\n",
    "            im_resize = cv2.resize(IMG, (img_size, img_size), 3)\n",
    "            images.append(im_resize)\n",
    "        #np_images = np.array(image_stack)\n",
    "        \n",
    "    return images, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15008 training images\n",
      "There are 1120 testing images\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dirlist_train = directory_scan(path_train)\n",
    "    images_train,labels_train,count_train = read_images_in_folder(dirlist_train, path_train)\n",
    "    dirlist_test = directory_scan(path_test)\n",
    "    images_test,labels_test,count_test = read_images_in_folder(dirlist_test, path_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dirlist_val = directory_scan(path_val)\n",
    "    images_val,labels_val,count_val= read_images_in_folder(dirlist_val, path_val)\n",
    "\n",
    "np.save('npy-color/images_val',images_val)\n",
    "np.save('npy-color/labels_val',labels_val)\n",
    "\n",
    "#save to file\n",
    "np.save('npy-color/images_train',images_train)\n",
    "np.save('npy-color/labels_train',labels_train)\n",
    "np.save('npy-color/images_test',images_test)\n",
    "np.save('npy-color/labels_test',labels_test)\n",
    "\n",
    "print('There are '+ str(count_train) +' training images')\n",
    "print('There are '+ str(count_test) +' testing images')\n",
    "\n",
    "nb_img_train = count_train\n",
    "nb_img_test = count_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NPY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TRAIN\n",
    "#load images\n",
    "train_images = np.load('npy-color/images_train.npy')\n",
    "\n",
    "#load labels\n",
    "train_labels = np.load('npy-color/labels_train.npy')\n",
    "\n",
    "#LOAD TEST\n",
    "#load images\n",
    "test_images = np.load('npy-color/images_test.npy')\n",
    "\n",
    "\n",
    "#load labels\n",
    "test_labels = np.load('npy-color/labels_test.npy')\n",
    "\n",
    "#LOAD VALIDATION\n",
    "#load images\n",
    "val_images = np.load('npy-color/images_val.npy')\n",
    "\n",
    "#load labels\n",
    "val_labels = np.load('npy-color/labels_val.npy')\n",
    "\n",
    "img_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16db6050b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16bc75fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 300\n",
    "print(train_labels[index])\n",
    "plt.imshow(train_images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'raw_data/train/'\n",
    "path_test = 'raw_data/test/'\n",
    "path_val = 'raw_data/val/'\n",
    "img_size= 100\n",
    "\n",
    "#get folder names\n",
    "def directory_scan(path):\n",
    "    dirlist = [ item for item in os.listdir(path) if os.path.isdir(os.path.join(path, item)) ]\n",
    "    return dirlist\n",
    "\n",
    "dirlist_train = directory_scan(path_train)\n",
    "dirlist_test = directory_scan(path_test)\n",
    "dirlist_val = directory_scan(path_val)\n",
    "\n",
    "nb_img_train = len(train_images)\n",
    "nb_img_test = len(test_images)\n",
    "nb_img_val = len(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare and reshape testing/training sets\n",
    "#img_size = 100\n",
    "#nb_img_train = 1400\n",
    "#nb_img_test = 40\n",
    "\n",
    "train_img = train_images.reshape([-1,img_size,img_size,3])\n",
    "train_img = train_img.astype('float32') / 255\n",
    "\n",
    "test_img = test_images.reshape([-1,img_size,img_size,3])\n",
    "test_img = test_img.astype('float32') / 255\n",
    "\n",
    "val_img = val_images.reshape([-1,img_size,img_size,3])\n",
    "val_img = val_img.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL ONE HOT ENCODING\n",
    "#Training\n",
    "k=0\n",
    "for i in range(len(dirlist_train)):\n",
    "    for j in range(len(train_labels)):\n",
    "        if(train_labels[j]==dirlist_train[i]):\n",
    "            train_labels[j]=k\n",
    "    k=k+1\n",
    "\n",
    "#Testing\n",
    "m=0\n",
    "for l in range(len(dirlist_test)):\n",
    "    for h in range(len(test_labels)):\n",
    "        if(test_labels[h]==dirlist_test[l]):\n",
    "            test_labels[h]=m\n",
    "    m=m+1\n",
    "    \n",
    "#Validation\n",
    "g=0\n",
    "for l in range(len(dirlist_val)):\n",
    "    for h in range(len(val_labels)):\n",
    "        if(val_labels[h]==dirlist_val[l]):\n",
    "            val_labels[h]=g\n",
    "    g=g+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab= to_categorical(train_labels)\n",
    "test_lab = to_categorical(test_labels)\n",
    "val_lab = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_accuracy(train_acc, Title):\n",
    "    x = [i for i in range(0, len(train_acc))]\n",
    "    y= train_acc\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(ymax=1)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.xlim(xmin=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(Title)\n",
    "    plt.show()\n",
    "    \n",
    "def Plot_Loss(train_loss, Title):\n",
    "    x = [i for i in range(0, len(train_loss))]\n",
    "    y= train_loss\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(ymax=2)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.xlim(xmin=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(Title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Import VGG pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = applications.VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Preprocess dataset with VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "train_features = conv_base.predict(train_img)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features/', train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape feature maps to match output of VGG model with input of additional layers\n",
    "train_features_reshaped = np.reshape(train_features, (len(train_img), 3 * 3 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save feature maps to file\n",
    "np.save('train_features/train_features', train_features_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads feature map from file\n",
    "train_features_ = np.load('train_features/train_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets feature maps for dataset based on inital VGG model\n",
    "#    >>from training set to feature map\n",
    "start = time.time()\n",
    "\n",
    "val_features = conv_base.predict(val_img)\n",
    "#train_features = np.reshape(train_features, (len(train_img), 3 * 3 * 512))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"Execution time = \" + str(int(((stop-start)-(stop-start)%60)/60)) +\" min \"+str((stop-start)%60)+\" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.reshape(val_features, (len(val_img), 3 * 3 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Additional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=3 * 3 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(dirlist_train), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15008 samples, validate on 1120 samples\n",
      "Epoch 1/50\n",
      "15008/15008 [==============================] - 2s 126us/step - loss: 0.2572 - acc: 0.9152 - val_loss: 0.6641 - val_acc: 0.7911\n",
      "Epoch 2/50\n",
      "15008/15008 [==============================] - 2s 105us/step - loss: 0.2384 - acc: 0.9238 - val_loss: 0.6327 - val_acc: 0.7982\n",
      "Epoch 3/50\n",
      "15008/15008 [==============================] - 2s 104us/step - loss: 0.2291 - acc: 0.9274 - val_loss: 0.7205 - val_acc: 0.7750\n",
      "Epoch 4/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.2179 - acc: 0.9303 - val_loss: 0.7473 - val_acc: 0.7598\n",
      "Epoch 5/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.2119 - acc: 0.9334 - val_loss: 0.6394 - val_acc: 0.7955\n",
      "Epoch 6/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.2037 - acc: 0.9350 - val_loss: 0.6772 - val_acc: 0.7884\n",
      "Epoch 7/50\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 0.1967 - acc: 0.9378 - val_loss: 0.6016 - val_acc: 0.8098\n",
      "Epoch 8/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1940 - acc: 0.9380 - val_loss: 0.6183 - val_acc: 0.8116\n",
      "Epoch 9/50\n",
      "15008/15008 [==============================] - 2s 111us/step - loss: 0.1857 - acc: 0.9417 - val_loss: 0.6628 - val_acc: 0.7812\n",
      "Epoch 10/50\n",
      "15008/15008 [==============================] - 2s 113us/step - loss: 0.1811 - acc: 0.9438 - val_loss: 0.6848 - val_acc: 0.7812\n",
      "Epoch 11/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1728 - acc: 0.9461 - val_loss: 0.6480 - val_acc: 0.8027\n",
      "Epoch 12/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.1654 - acc: 0.9495 - val_loss: 0.8428 - val_acc: 0.7455\n",
      "Epoch 13/50\n",
      "15008/15008 [==============================] - 2s 109us/step - loss: 0.1632 - acc: 0.9513 - val_loss: 0.6947 - val_acc: 0.7938\n",
      "Epoch 14/50\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 0.1597 - acc: 0.9499 - val_loss: 0.6758 - val_acc: 0.7920\n",
      "Epoch 15/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1513 - acc: 0.9546 - val_loss: 0.7062 - val_acc: 0.7911\n",
      "Epoch 16/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1493 - acc: 0.9553 - val_loss: 0.6313 - val_acc: 0.7973\n",
      "Epoch 17/50\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 0.1424 - acc: 0.9579 - val_loss: 0.7333 - val_acc: 0.7821\n",
      "Epoch 18/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1407 - acc: 0.9572 - val_loss: 0.7017 - val_acc: 0.7893\n",
      "Epoch 19/50\n",
      "15008/15008 [==============================] - 2s 115us/step - loss: 0.1388 - acc: 0.9590 - val_loss: 0.6774 - val_acc: 0.7929\n",
      "Epoch 20/50\n",
      "15008/15008 [==============================] - 2s 111us/step - loss: 0.1349 - acc: 0.9599 - val_loss: 0.6818 - val_acc: 0.8018\n",
      "Epoch 21/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.1257 - acc: 0.9640 - val_loss: 0.7413 - val_acc: 0.7911\n",
      "Epoch 22/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.1278 - acc: 0.9613 - val_loss: 0.6468 - val_acc: 0.8116\n",
      "Epoch 23/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.1205 - acc: 0.9644 - val_loss: 0.6551 - val_acc: 0.8054\n",
      "Epoch 24/50\n",
      "15008/15008 [==============================] - 2s 102us/step - loss: 0.1163 - acc: 0.9659 - val_loss: 0.8288 - val_acc: 0.7688\n",
      "Epoch 25/50\n",
      "15008/15008 [==============================] - 2s 104us/step - loss: 0.1134 - acc: 0.9671 - val_loss: 0.8517 - val_acc: 0.7652\n",
      "Epoch 26/50\n",
      "15008/15008 [==============================] - 2s 109us/step - loss: 0.1127 - acc: 0.9690 - val_loss: 0.7679 - val_acc: 0.7821\n",
      "Epoch 27/50\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 0.1088 - acc: 0.9695 - val_loss: 0.7870 - val_acc: 0.7866\n",
      "Epoch 28/50\n",
      "15008/15008 [==============================] - 2s 109us/step - loss: 0.1091 - acc: 0.9688 - val_loss: 0.6675 - val_acc: 0.8036\n",
      "Epoch 29/50\n",
      "15008/15008 [==============================] - 2s 114us/step - loss: 0.1052 - acc: 0.9690 - val_loss: 0.6729 - val_acc: 0.8098\n",
      "Epoch 30/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.1025 - acc: 0.9693 - val_loss: 0.6919 - val_acc: 0.8089\n",
      "Epoch 31/50\n",
      "15008/15008 [==============================] - 2s 104us/step - loss: 0.0999 - acc: 0.9712 - val_loss: 0.6944 - val_acc: 0.8116\n",
      "Epoch 32/50\n",
      "15008/15008 [==============================] - 2s 101us/step - loss: 0.0977 - acc: 0.9709 - val_loss: 0.7972 - val_acc: 0.7786\n",
      "Epoch 33/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0919 - acc: 0.9755 - val_loss: 0.7119 - val_acc: 0.8062\n",
      "Epoch 34/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0919 - acc: 0.9741 - val_loss: 0.8137 - val_acc: 0.7786\n",
      "Epoch 35/50\n",
      "15008/15008 [==============================] - 1s 97us/step - loss: 0.0918 - acc: 0.9735 - val_loss: 0.7418 - val_acc: 0.8054\n",
      "Epoch 36/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0851 - acc: 0.9760 - val_loss: 0.7937 - val_acc: 0.7902\n",
      "Epoch 37/50\n",
      "15008/15008 [==============================] - 1s 99us/step - loss: 0.0861 - acc: 0.9759 - val_loss: 0.7406 - val_acc: 0.8027\n",
      "Epoch 38/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0808 - acc: 0.9772 - val_loss: 0.8101 - val_acc: 0.7804\n",
      "Epoch 39/50\n",
      "15008/15008 [==============================] - 2s 102us/step - loss: 0.0816 - acc: 0.9760 - val_loss: 0.9837 - val_acc: 0.7393\n",
      "Epoch 40/50\n",
      "15008/15008 [==============================] - 2s 108us/step - loss: 0.0824 - acc: 0.9757 - val_loss: 0.7777 - val_acc: 0.7902\n",
      "Epoch 41/50\n",
      "15008/15008 [==============================] - 2s 101us/step - loss: 0.0771 - acc: 0.9781 - val_loss: 0.6909 - val_acc: 0.8143\n",
      "Epoch 42/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0729 - acc: 0.9789 - val_loss: 0.7325 - val_acc: 0.8098\n",
      "Epoch 43/50\n",
      "15008/15008 [==============================] - 1s 97us/step - loss: 0.0747 - acc: 0.9794 - val_loss: 0.8134 - val_acc: 0.7866\n",
      "Epoch 44/50\n",
      "15008/15008 [==============================] - 1s 98us/step - loss: 0.0705 - acc: 0.9803 - val_loss: 0.7523 - val_acc: 0.7982\n",
      "Epoch 45/50\n",
      "15008/15008 [==============================] - 2s 105us/step - loss: 0.0682 - acc: 0.9802 - val_loss: 0.7759 - val_acc: 0.7938\n",
      "Epoch 46/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.0680 - acc: 0.9814 - val_loss: 0.7480 - val_acc: 0.8063\n",
      "Epoch 47/50\n",
      "15008/15008 [==============================] - 2s 105us/step - loss: 0.0665 - acc: 0.9831 - val_loss: 0.7327 - val_acc: 0.8098\n",
      "Epoch 48/50\n",
      "15008/15008 [==============================] - 2s 107us/step - loss: 0.0646 - acc: 0.9831 - val_loss: 0.7362 - val_acc: 0.8161\n",
      "Epoch 49/50\n",
      "15008/15008 [==============================] - 2s 106us/step - loss: 0.0622 - acc: 0.9836 - val_loss: 0.8308 - val_acc: 0.7929\n",
      "Epoch 50/50\n",
      "15008/15008 [==============================] - 2s 112us/step - loss: 0.0650 - acc: 0.9821 - val_loss: 0.7469 - val_acc: 0.8152\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "history = model.fit(train_features_,\n",
    "                    train_lab,\n",
    "                    epochs=50,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(val_features,val_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_features)\n",
    "\n",
    "#one hot encoding\n",
    "for i in range(len(predictions)):\n",
    "    length = len(predictions[i])\n",
    "    index = np.argmax(predictions[i])\n",
    "    predictions[i][index] = 1\n",
    "    for j in range(length):\n",
    "        if(j != index):\n",
    "            predictions[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bed       0.83      0.71      0.77       150\n",
      "      bench       0.85      0.56      0.67        70\n",
      "    cabinet       0.81      0.78      0.79       150\n",
      "      chair       0.86      0.94      0.90       150\n",
      "      couch       0.68      0.88      0.77       150\n",
      "       lamp       0.81      0.79      0.80       150\n",
      "      plant       0.95      0.91      0.93       150\n",
      "      table       0.80      0.83      0.81       150\n",
      "\n",
      "avg / total       0.82      0.82      0.81      1120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, predictions, target_names = dirlist_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN : Region based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/experiencor/basic-yolo-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/philipperemy/yolo-9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
